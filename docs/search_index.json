[["index.html", "R 텍스트마이닝 들어가며 0.1 구성 0.2 오픈 전자책", " R 텍스트마이닝 한국 R 사용자회 2022-08-06 들어가며 “R 텍스트 마이닝” 전자책은 한국 R 사용자회와 제주대학교 제주대학교 언론홍보학과 안도현 교수님이 공동 작업한 저작물로 건강한 R 생태계 확대에 크게 기여할 것으로 기대됩니다. 안도현 교수님의 R로 배우는 텍스트마이닝 콘텐츠를 기반으로 한국 R 사용자회는 텍스트 분석/마이닝 전용 bitTA 패키지를 개발하여 2022년 현재 시점 한국어 텍스트 분석의 새로운 여정을 시작했습니다. “R 텍스트 마이닝”은 한국어 텍스트 분석에 필요한 다양한 도구를 bitTA 패키지에 내장하고 한국어 텍스트 전처리와 정제에 필요한 스크립트와 해석을 전자책에 담아냈습니다. 또한, 대표적인 한글 소설과 “인공지능”과 “코로나19” 뉴스기사로 감성분석과 주제모형을 개발작업을 진행하여 현업 담당자가 실제 업무에 적용할 수 있도록 구성하였으며 shiny 앱을 내장하여 실제 텍스트 마이닝 데이터 사이언스 제품개발에 관심있는 개발자도 전체적인 개발흐름을 따라가며 구현에 도움이 되도록 구성했습니다. R로 배우는 텍스트마이닝 - GitHub 저장소 텍스트 마이닝 bitTA 패키지 텍스트 마이닝 웹앱: bitTA 패키지 내장 R로 배우는 텍스트마이닝 - 전자책 “R 텍스트마이닝” 저작물을 포함한 한국 알(R) 사용자회 저작물은 크리에이티브 커먼즈 저작자표시-비영리-동일조건 변경 허락 (BY-NC-SA) 라이선스를 준용하고 있습니다. The online version of this book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. 관련 문의와 연락이 필요한 경우 한국 알(R) 사용자회 admin@r2bit.com 대표전자우편으로 연락주세요. 후원계좌 디지털 불평등 해소를 위해 제작중인 오픈 통계패키지 개발과 고품질 콘텐츠 제작에 큰 힘이 됩니다. 하나은행 448-910057-06204 사단법인 한국알사용자회 0.1 구성 텍스트 마이닝에 필요한 R의 기초, 분석의 전단계, 그리고 분석 등으로 구성돼 있다. 특히, 기존 텍스트 마이닝 Base R 을 최대한 tidyverse 패러다임을 담아 깔끔한 데이터(tidy data)를 주요 자료구조로 탐색적 텍스트 마이닝을 구현할 수 있도록 했으며 shiny 앱을 bitTA 패키지에 담아 텍스트 분석 생산성을 높이도록 했다. 텍스트 데이터가 엑셀과 같은 직사각형 데이터에 익숙한 분들도 다양한 텍스트 전처리 기법을 담아 텍스트 전처리 업무에 대한 부담을 대폭 낮추면서 어떠한 텍스트 데이터도 후속 시각화와 모형개발에 문제가 없도록 자주 사용되는 기법을 담아내었다. 텍스트 마이닝의 꽃인 감성분서과 주제모형을 별도 패키지와 데이터를 구하지 않고도 bitTA 패키지에 담아 내었으며 한글 감성분석에 꼭 필요한 감성사전을 포함하여 감성분석의 새지평을 열었으며 주제모형도 “인공지능”, “코로나19”를 실제 데이터로 삼아 tidytext 체계를 담아내어 최신 텍스트 마이닝 추세를 반영하여 한국어 텍스트 마이닝에 실전 사례로 확장하시는 독자에게 큰 도움이 되고자 노력을 하였습니다. 0.1.1 설치 및 환경설정 텍스트 마이닝에 꼭 필요한 여러 패키지와 툴에 다양하게 다뤘습니다. KoNLP는 자바의존성을 갖고 2016년 이후 더이상 유지보수가 되고 있지 않아 관련 패키지는 동일한 기능을 갖는 다른 패키지 메카브(Mecab)로 대체하였고, 하지만, KoNLP에서 제공하는 기능은 파이썬에서 제공하는 텍스트 마이닝 연관 패키지를 reticualte를 사용해서 필요한 기능을 더 확장해서 사용할 수 있도록 독자와 사용자의 편의성을 배려하였다. 또한, 딥러닝 관련 사항도 일부 다뤄서 딥러닝 사전학습모형(pretrained model)을 필요한 경우 텍스트 마이닝에 활용할 수 있는 초석도 마련하여 향후 딥러닝 관련 텍스트 마이닝 분야도 함께 성장할 수 있도록 했다. 텍스트 마이닝을 R 언어로 수행하면서 관련된 데이터유형과 구조, 시각화의 기초적인 내용과 R과 RStudio를 이용하는 과정에서 겪을 수 있는 문제해결 방법에 대해 다뤘다. 0.1.2 텍스트 살펴보기 텍스트 마이닝을 본격적으로 진행하기에 앞서 텍스트를 일별할 수 있는 방법에 대해 소개하고 shiny 앱을 통해 텍스트 마이닝 전반에 대해 살펴볼 수 있도록 했다. 특히, bitTA 패키지 내부에 텍스트 마이닝 shiny 앱이 내장되어 있어 필요하면 바로 텍스트 마이닝 작업을 수행하는 것도 가능하도록 했다. 0.1.3 분석 전단계 텍스트마이닝의 전반적인 구조와 자료 수집과 불러오기, 정제(전처리)에 필요한 다양한 도구(stringr, dplyr, tidyr, purrr, regex 등)의 학습 및 정제(전처리)하는 방법에 대해 학습한다. 이 과정을 통해 분석이 불가능한 텍스트를 깔끔한 텍스트(tidy text) 즉 데이터프레임으로 변환시켜 후속 작업을 준비한다. 0.1.4 기술 통계 단어의 빈도를 계산해 텍스트에서 의미를 추론하는 방식을 학습한다. 사전(감정사전)을 이용하는 방법, 상대적인 빈도(tf-idf, 가중로그승산비 등)를 계산하는 방법, 기계학습의 비지도학습(주제모형: topic models)으로 계산하는 방법 등을 학습한다. 0.1.5 분석 I 텍스트 마이닝의 꽃인 감성분석과 주제모형, 공변량 주제모형으로 텍스트 분석 범위를 모형까지 넓혀 실무업무에서 사용되는 다양한 기법을 현업과 밀접한 “인공지능”, “코로나19” 뉴스기사를 통해 생생한 현장의 모습을 담아내고자 했다. 0.1.6 분석 II (예정) 텍스트 마이닝 기계학습의 지도학습 방식에 대해서는 별도 전자책으로 준비하여 공개할 예정이다. 0.2 오픈 전자책 사단법인 한국 알(R) 사용자회는 디지털 불평등 해소와 통계 대중화를 위해 2022년 설립되었습니다. 오픈 통계 패키지 개발을 비롯하여 최근에 데이터 사이언스 관련 교재도 함께 제작하여 발간하는 작업을 수행하고 있습니다. 그 첫번째 결과물로 John Fox 교수님이 개발한 설치형 오픈 통계 패키지 Rcmdr(Fox 2016) (Fox and Bouchet-Valat 2021) (Fox 2005) 를 신종화 님께서 한글화 및 문서화에 10년 넘게 기여해주신 한국알사용자회 저작권을 흔쾌히 허락해 주셔서 설치형 오픈 통계 패키지 - Rcmdr로 세상에 나왔습니다. 두번째 활동을 여기저기 산재되어 있던 시각화 관련 자료를 묶어 데이터 시각화(Data Visualization)를 전자책 형태로 공개하였고, 데이터 분석 관련 저술을 이어 진행하게 되었습니다. 세번째 활동으로 데이터 사이언스가 하나로 구성되지 않은 것을 간파하고 데이터 사이언스를 지탱하는 기본기술을 5가지로 정리한 데이터 과학을 지탱하는 기본기 전자책을 공개했습니다. 네번째 활동으로 데이터 과학이 이제는 R 혹은 파이썬 언어가 중요한 것이 아니라 데이터 과학 문제 해결에 집중한 데이터 과학 프로그래밍 전자책을 공개했습니다. “데이터 과학 프로그래밍” 저작을 위해 “Python for Everybody”와 Python for Informatics: Exploring Information에 기반하여 2015년부터 시작된 모두를 위한 파이썬 한글화 프로젝트가 밑바탕이 되었습니다. 데이터 분석 언어 R에 관한 지식을 신속히 습득하여 독자들이 갖고 있는 문제에 접목시키고자 하시는 분은 한국 알(R) 사용자회에서 번역하여 공개한 R 신병훈련소(Bootcamp) 과정을 추천드립니다. 참고문헌 "],["install-setup.html", "1 . 설치 및 환경설정 1.1 색상 1.2 ggplot 글꼴 1.3 bitTA 1.4 메카브설치 1.5 spacyr 1.6 파이썬 nltk, konlpy", " 1 . 설치 및 환경설정 즐거운 텍스트 마이닝(Text Mining) 작업환경을 구축하기 위해서는 몇가지 환경이 구비되어야만 한다. 먼저 작업할 데이터가 텍스트이기 때문에 텍스트에서 특정 단어 색상을 달리하는 것은 추후 딥러닝 질의응답 인공지능 시스템을 구축할 때 딥러닝 시스템이 질의에 대한 답변을 전체 텍스트의 일부를 색상을 달리하여 시각적으로 표현하게 되면 사용자 편의성이 크게 개선시킬 수 있다. 텍스트 색상을 달리할 경우 크게 두가지 부분이 이슈가 된다. 첫번째는 텍스트 마이닝 콘솔 작업할 때 코드와 R 코드로 작업한 결과물을 출력할 때 색상을 차별화하는 것이고, 다른 하나는 텍스트 마이닝 결과를 데이터 과학 제품으로 출력할 때 색상을 달리하여 웹상으로 표현하는 것이다. 1.1 색상 1.1.1 콘솔 색상 glue 패키지 glue_col() 함수를 사용하게 되면 텍스트에 색상을 입히는 작업을 간단하게 실행에 옮길 수 있다. library(glue) library(crayon) glue_col(&quot;{blue 1 + 2 = {red 1 + 2}}&quot;) ## 1 + 2 = 1 + 2 1.1.2 R마크다운 색상 .Rmd R마크다운 파일 작업결과에 색상을 입히기 위해서는 fansi 패키지가 필요하다. R마크다운 코드 덩어리에 다음 사항을 추가하고 R마크다운 작업을 수행하면 자동으로 해당 색상을 .html, .pdf, shiny 결과물에 반영할 수 있다. ```r knitr::knit_hooks$set(output = function(x, options){ paste0( '', fansi::sgr_to_html(x = htmltools::htmlEscape(x), warn = FALSE), '' ) }) ``` R마크다운 색상 적용에 대한 자세한 사항은 rmarkdown and terminal colors를 참조한다. 1.1.3 데이터프레임 이를 확장하여 콘솔, R마크다운, 그리고 데이터프레임에도 색상을 적용하여 반영시킬 수 있다. 1.2 ggplot 글꼴 단어구름(worldcloud)를 사용해서 텍스트 시각화를 많이 한다. ggwordcloud 패키지는 ggplot에서 텍스트 단어구름을 자연스럽게 구현했다. ggwordcloud에 내장된 전세계 사랑 이라는 단어가 love_words_small 데이터프레임으로 내장되어 있다. 이를 기본 글꼴을 사용해서 단어구름 시각화를 구현해보자. library(ggwordcloud) library(tidyverse) data(&quot;love_words_small&quot;) love_words_small %&gt;% mutate(color = ifelse(word == &quot;사랑&quot;, &quot;blue&quot;, &quot;gray50&quot;)) %&gt;% ggplot(aes(label = word, size = speakers, color = color)) + geom_text_wordcloud() + scale_size_area(max_size = 40) + theme_minimal() + scale_color_manual(values = c(&quot;blue&quot;, &quot;gray50&quot;)) 글꼴을 다양한 방식으로 구현하면 좀더 미려한 워드 클라우드를 뽑아낼 수 있다. 가장 최근에 네이버에서 공개한 마루부리 글꼴을 워드 클라우드에 반영해보자. 마루 부리 글꼴 다운로드 압축을 풀어 해당 글꼴을 운영체제에 설치 sysfonts 패키지를 사용해서 R 글꼴로 등록 showtext 패키지 showtext_auto() 함수로 ggplot에 사용할 수 있도록 설정 다음 워드 클라우드를 통해 마루부리 글꼴이 잘 반영된 것이 확인되지만 다른 언어로 표현된 글꼴을 마루부리 글꼴이 적절히 반영하지 않는 것도 확인된다. library(systemfonts) library(sysfonts) library(showtext) # 글꼴이 설치된 경로 표시 font_paths() ## [1] \"C:\\\\Windows\\\\Fonts\" # 운영체제 등록된 글꼴을 R 글꼴로 등록 sysfonts::font_add(family = &quot;MaruBuri&quot;, regular = &#39;MaruBuri-Regular.ttf&#39;) # 마루부리 글꼴이 설치되었는지 확인 font_files() %&gt;% tibble() %&gt;% filter(str_detect(family, &quot;Maru&quot;)) ## # A tibble: 4 × 6 ## path file family face version ps_name ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 C:/Windows/Fonts MaruBuri-Bo… MaruB… Regu… Versio… MaruBu… ## 2 C:/Windows/Fonts MaruBuri-Li… MaruB… Regu… Versio… MaruBu… ## 3 C:/Windows/Fonts MaruBuri-Re… MaruB… Regu… Versio… MaruBu… ## 4 C:/Windows/Fonts MaruBuri-Se… MaruB… Regu… Versio… MaruBu… # ggplot에서 사용할 수 있도록 설정 showtext::showtext_auto() love_words_small %&gt;% mutate(color = ifelse(word == &quot;사랑&quot;, &quot;blue&quot;, &quot;gray50&quot;)) %&gt;% ggplot(aes(label = word, size = speakers, color = color)) + geom_text_wordcloud(family = &quot;MaruBuri&quot;) + scale_size_area(max_size = 40) + theme_minimal() + scale_color_manual(values = c(&quot;blue&quot;, &quot;gray50&quot;)) 상기 문제를 풀고자 글꼴이 필요한데 구글 폰트에서 웹폰트를 가져와서 이를 워드 클라우드 작성에 활용해보자. sysfonts 패키지 font_add_google() 함수를 사용하면 R에서 바로 사용할 수 있는 글꼴을 바로 설치해주기 때문에 매우 편리하다. 다만, Noto Serif KR 글꼴은 한글과 한자, 영어는 문제가 없어 보이지만 다른 언어를 표현하는데 문제가 있음을 알 수 있다. sysfonts::font_add_google(name = &quot;Noto Serif KR&quot;, family = &quot;noto_serif&quot;) showtext::showtext_auto() love_words_small %&gt;% mutate(color = ifelse(word == &quot;사랑&quot;, &quot;blue&quot;, &quot;gray50&quot;)) %&gt;% ggplot(aes(label = word, size = speakers, color = color)) + geom_text_wordcloud(family = &quot;noto_serif&quot;) + scale_size_area(max_size = 40) + theme_minimal() + scale_color_manual(values = c(&quot;blue&quot;, &quot;gray50&quot;)) R문서/그래프/코딩 글꼴(font)을 바꾸고 싶을 때가 있다. 자세한 사항은 데이터 시각화 - R 문서/그래프/코딩 글꼴(font) 문서를 참조한다. 1.3 bitTA 1.3.1 은전한닢 형태소분석기 은전한닢 형태소분석기(mecab-ko)는 오픈소스 일본어 형태소분석기인 MeCab(메카브)를 한글의 특성을 반영하여 포팅한 오픈소스입니다. 은전한닢은 국립국어원의 21세기 세종계획 말뭉치(Corpus)로 모델을 학습하였습니다. 1.3.2 은전한닢 장점 어찌보면 MeCab의 장점이겠습니다.1 사전, 코퍼스 독립적 범용 디자인 조건부 확률 필드 (CRF)를 기반으로 한 높은 분석 정확도 속도가 빠름 사전 추출 알고리즘/데이터 구조에는 고속 TRIE 구조인 Double-Array 채택 C++로 개발 다양한 스크립트 언어 바인딩 perl/ruby/python/java/C# 1.3.3 은전한닢 설치 1.3.3.1 Linix와 Mac 운영체제 은전한닢 형태소분석기인 mecab-ko와 한글사전인 mecab-ko-dic을 설치해야하는데, Linux와 Mac 운영체제에서의 은전한닢 형태소분석시의 설치는 그리 어렵지 않습니다. mecab-ko-dic 페이지에 설치 방법이 잘 가이드되어 있어, 기술하는 방법으로 소스를 컴파일하여 설치하면 됩니다. 1.3.3.2 Windows 운영체제 Windows 운영체제에서의 mecab-ko와 mecab-ko-dic을 설치하는 것은 쉽지 않았습니다. 그러나 형태소분석기와 사전을 Windows 환경에서 컴파일한 바이너리 버전을 다음 사이트에서 다운로드 받아 “c:” 디렉토리에 설치하면 됩니다. mecab-ko-msvc mecab-ko-dic-msvc 1.3.3.3 bitTA 패키지를 이용한 설치 은전한닢 형태소분석기를 설치하지 않은 상태에서 bitTA 패키지를 로드하면, 다음과 같은 메시지가 출력됩니다. &gt; library(bitTA) To use bitTA, you need to install mecab-ko and mecab-ko-dic. You can install it with install_mecab_ko(). Linix와 Mac, Windows 운영체제와 무관하게 bitTA 패키지의 install_mecab_ko() 함수는 은전한닢 형태소분석기와 한글사전을 설치해줍니다. 그러므로 은전한닢 형태소분석기를 설치하지 않은 상태라면, install_mecab_ko() 함수를 사용하는 것을 추천합니다. 다음과 같이 설치합니다. library(bitTA) install_mecab_ko() 다음은 Winows 운영체제에서의 설치 예시입니다. Winows 운영체제에서는 바이너리 프로그램을 다운로드한 후 정해진 경로에 복사하는 것으로 설치됩니다. &gt; install_mecab_ko() Install mecab-ko-msvc...trying URL &#39;https://github.com/Pusnow/mecab-ko-msvc/releases/download/release-0.9.2-msvc-3/mecab-ko-msvc-x64.zip&#39; Content type &#39;application/octet-stream&#39; length 777244 bytes (759 KB) downloaded 759 KB Install mecab-ko-dic-msvc...trying URL &#39;https://github.com/Pusnow/mecab-ko-dic-msvc/releases/download/mecab-ko-dic-2.0.3-20170922-msvc/mecab-ko-dic-msvc.zip&#39; Content type &#39;application/octet-stream&#39; length 32531949 bytes (31.0 MB) downloaded 31.0 MB Windows 환경에서는 “c:” 디렉토리에 형태소분석기를 설치해야 정상적으로 작동합니다만, 다른 디렉터로리에 설치하려면 다음과 같이 mecabLocation 인수를 사용합니다. 그러나 이 방법은 권장하지 않습니다. install_mecab_ko(mecabLocation = &quot;d:/morpheme/mecab&quot;) 1.3.4 RcppMeCab 패키지 설치 bitTA에서 형태소분석 기능을 사용하기 위해서는 RcppMeCab 패키지고 설치해야 합니다. 만약에 이 패키지가 설치되어 있지 않다면, 형태소분석기를 호출할 때 다음과 같은 에러가 발생합니다. &gt; morpho_mecab(&quot;아버지가 방에 들어가신다.&quot;) Error in morpho_mecab(&quot;아버지가 방에 들어가신다.&quot;) : To use morpho_mecab(), you need to install RcppMeCab package. You can install it with install.packages(&quot;RcppMeCab&quot;). RcppMeCab은 CRAN에 등록된 패키지므로 다음처럼 간단하게 설치합니다. install.packages(&quot;RcppMeCab&quot;) 여기까지 설치되었다면 비로소 형태소분석을 수행할 수 있습니다. library(&quot;bitTA&quot;) morpho_mecab(&quot;아버지가 방에 들어가신다.&quot;, type = &quot;morpheme&quot;) 1.4 메카브설치 빠르면서 성능이 좋다고 알려진 메카드(MeCab) 형태소 분석기를 설치한다. MeCab 설치과정 1.4.1 맥 MeCab 설치과정은 가장먼저 MeCab 설치부터 시작한다. 일본에서 제작했기 때문에 RMeCaB 패키지를 설치하면 일본어 형태소 분석 작업을 바로 시작할 수 있다. 한글을 형태소 분석하기 위해서는 은전한닢(mecab-ko)를 설치한 후에 R에서 사용할 수 있도록 개발중인 bitTA 패키지를 설치하면 된다. 1.4.1.1 MeCab 설치 GitHub Installation of RMeCab 1.07 on M1 Mac #13 에 자세한 사항이 나와 있지만 간략하게 정리하면 다음과 같다. ## xcode 설치되면 생략 ----- $ xcode-select --install ## MeCab 설치 -------------- $ cd ~/Downloads $ curl -fsSL &#39;https://drive.google.com/uc?export=download&amp;id=0B4y35FiV1wh7cENtOXlicTFaRUE&#39; -o mecab-0.996.tar.gz $ tar xf mecab-0.996.tar.gz $ cd mecab-0.996 $ ./configure --with-charset=utf8 $ make $ sudo make install ## MeCab 사전 설치 -------------- $ cd ~/Downloads $ curl -fsSL &#39;https://drive.google.com/uc?export=download&amp;id=0B4y35FiV1wh7MWVlSDBCSXZMTXM&#39; -o mecab-ipadic-2.7.0-20070801.tar.gz $ tar zvxf mecab-ipadic-2.7.0-20070801.tar.gz $ tar xf mecab-ipadic-2.7.0-20070801.tar.gz $ cd mecab-ipadic-2.7.0-20070801 $ ./configure --with-charset=utf-8 $ make $ sudo make install ## MeCab 설치 테스트 -------------- $ mecab すもももももももものうち 1.4.1.2 RMeCab 설치 (생략) RMeCab GitHub 저장소에 설치사항을 정리하여 보면 MeCab와 사전을 설치한 후에 install.packages() 에 RMeCab 패키지 저장소를 달리 지정하여 설치하면 된다. install.packages(&quot;RMeCab&quot;, repos = &quot;https://rmecab.jp/R&quot;, type = &quot;source&quot;) library(RMeCab) res &lt;- RMeCabC(&quot;すもももももももものうち&quot;) unlist (res) ## 名詞 助詞 名詞 助詞 名詞 助詞 名詞 ## &quot;すもも&quot; &quot;も&quot; &quot;もも&quot; &quot;も&quot; &quot;もも&quot; &quot;の&quot; &quot;うち&quot; 1.4.1.3 MeCab-ko 설치 일본어 MeCab 설치과정과 동일하게 한국어 MeCab-ko를 설치한다. Bitbucket eunjeon/mecab-ko 저장소에서 mecab-ko 최신버전을 다운로드 한다. Bitbucket eunjeon/mecab-ko-dic 저장소에서 mecab-ko-dic 사전 최신버전을 다운로드 한다. ## MeCab-ko 설치 ------------ $ cd ~/Downloads $ curl -fsSL &#39;https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz&#39; -o mecab-0.996-ko-0.9.2.tar.gz $ tar xzvf mecab-0.996-ko-0.9.2.tar.gz $ cd mecab-0.996-ko-0.9.2 $ ./configure --with-charset=utf-8 $ make $ sudo make install ## MeCab-ko-dic 사전 설치 ------------ $ cd ~/Downloads $ curl -fsSL &#39;https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz&#39; -o mecab-ko-dic-2.1.1-20180720.tar.gz $ cd mecab-ko-dic-2.1.1-20180720 $ ./configure --with-charset=utf-8 $ make $ sudo make install 1.5 spacyr 표제어 추출에 사용하는 패키지는 spacyr이다. 사용자설명서 1.5.1 미니콘다 설치 (아나콘다 혹은 미니콘다가 이미 컴퓨터에 설치돼 있으면 곧바로 spacyr 패키지 설치로 이동) spacyr은 파이썬 spaCy패키지를 R에서 사용할 수 있도록한 패키지이므로, spacyr을 이용하기 위해서는 컴퓨터에 파이썬과 필요한 패키지가 설치돼 있어야 한다. 파이썬과 자주사용하는 패키지를 한번에 설치할 수 있는 것이 아나콘다와 미니콘다이다. 아나콘다: 파이썬 + 자주 사용하는 패키지 1500여개 설치(3GB 이상 설치공간 필요) 미니콘다: 파이썬 + 필수 패키지 700여개 설치(260MB 설치공간 필요) (미니콘다 설치 안내) 미니콘다 설치 안내 페이지에서 본인의 운영체제에 맞는 파일을 선택해 컴퓨터에 설치한다. 관리자권한으로 설치한다 (다운로드받은 파일에 마우스커서 올려 놓고 오른쪽 버튼 클릭해 ‘관리자권한으로 실행’ 선택) 설치후 Anaconda Prompt를 연다. 윈도화면 왼쪽 아래의 시작버튼을 클릭하면 윈도 시작 메뉴가 열린다. 상단 ’최근에 추가한 앱’에서 Anaconda Prompt(Miniconda 3)을 클릭하면 Anaconda Prompt가 열린다. (Anaconda Powershell Prompt를 이용해도 된다) 프롬프트가 열리면 conda --version을 입력한다. conda 4.9.2처럼 콘다의 버전 정보가 뜨면 설치에 성공. 1.5.2 spacyr 패키지 설치 spacyr 패키지를 설치하고 구동한다. (패키지 설치할 때는 R이나 RStudio를 관리자 권한으로 실행해 설치한다.) install.packages(&quot;spacyr&quot;) library(spacyr) 패키지를 설치하고 구동했으면 spacy_install()을 실행한다. 콘솔에 Proceed여부를 묻는 화면이 나면 2: Yes를 선택해 진행한다. spacy_install() Proceed? 1: No 2: Yes Selection: 2 A new conda environment &quot;spacy_condaenv&quot; will be created and spaCy and language model(s): en_core_web_sm will be installed. Creating spacy_condaenv conda environment for spaCy installation... Collecting package metadata (current_repodata.json): ...working... done Solving environment: ...working... done ... ✔ Download and installation successful You can now load the package via spacy.load(&#39;en_core_web_sm&#39;) Language model &quot;en_core_web_sm&quot; is successfully installed Installation complete. Condaenv: spacy_condaenv; Language model(s): en_core_web_sm spacy_install()은 시스템 파이썬(또는 아나콘다 파이썬)과는 별개로 R환경에서 파이썬을 실행할 수 있는 콘다환경이 생성된다. 1.5.3 reticulate 패키지 설치 파이썬 모듈을 R환경에서 실행할 수 있도록 하는 파이썬-R 인터페이스 패키지다. install.packages(&quot;reticulate&quot;) 1.5.4 spacy_initialize() spacy_initialize()로 R에서 spaCy를 초기화한다. library(spacyr) spacy_initialize(model = &quot;en_core_web_sm&quot;) 1.5.5 파이썬 설정 오류 과거에 파이썬을 설치했었거나 혹은 파이썬에 의존하는 R패키지를 설치했었던 경우 오류가 발생할 수있다. spacy_initialize()를 실행하면 아래와 같은 파이썬 설정 오류가 발생할 수 있다. spacy python option is already set, spacyr will use: condaenv = &quot;spacy_condaenv&quot; ERROR: The requested version of Python (&#39;C:\\Users\\[사용자ID]\\AppData\\Local\\r-miniconda\\envs\\spacy_condaenv\\python.exe&#39;) cannot be used, as another version of Python (&#39;새로 설치한 미니콘다 경로&#39;) has already been initialized . Please restart the R session if you need to attach reticulate to a different version of Python. Error in use_python(python, required = required) : failed to initialize requested version of Python spacy_initialize()함수가 파이썬을 C:\\Users\\[사용자ID]\\AppData\\Local\\r-miniconda\\envs\\spacy_condaenv\\python.exe에서 찾는다는 의미다. 이 곳으로 파이썬 환경을 설정한다. 세 가지 방법이 있다. (이 위치는 사용자별로 파이썬이 설치된 환경에 따라 다르다.) 1.5.5.1 RStudio에서 설정 RStudio의 Tools 메뉴 선택. 드롭다운 메뉴가 열리면, Global Options 선택 Options 창이 뜨면 왼쪽 메뉴 하단의 Python선택 Python interpreter:의 Select버튼 클릭 Python interpreter를 선택할 수 있는 창이 열리면, spacy_initialize()함수가 찾는 파이썬 경로 선택(예: C:\\Users\\[사용자ID]\\AppData\\Local\\r-miniconda\\envs\\spacy_condaenv\\python.exe) 1.5.5.2 Sys.setevn() 함수 이용 Sys.setenv(RETICULATE_PYTHON = &quot;`spacy_initialize()`함수가 찾는 파이썬 경로&quot;) 예: Sys.setenv(RETICULATE_PYTHON = &quot;C:\\Users\\[사용자ID]\\AppData\\Local\\r-miniconda\\envs\\spacy_condaenv\\python.exe&quot;) 1.5.5.3 reticulate::use_python() 함수 이용 reticulate::use_python(&quot;`spacy_initialize()`함수가 찾는 파이썬 경로&quot;) 예: reticulate::use_python(&quot;C:\\Users\\[사용자ID]\\AppData\\Local\\r-miniconda\\envs\\spacy_condaenv\\python.exe&quot;) 1.5.5.4 설정 변경 확인 reticulate::py_config()를 실행하면 파이썬 설정 환경을 확인할 수 있다. python: C:\\Users\\[사용자ID]\\AppData\\Local\\r-miniconda\\envs\\spacy_condaenv\\python.exe ... spacyr 패키지 소품문에 나와 있는 예제를 실행해본다. library(&quot;spacyr&quot;) spacy_initialize(model = &quot;en_core_web_sm&quot;) txt &lt;- c(d1 = &quot;spaCy is great at fast natural language processing.&quot;, d2 = &quot;Mr. Smith spent two years in North Carolina.&quot;) parsedtxt &lt;- spacy_parse(txt, tag = TRUE, entity = FALSE, lemma = FALSE) parsedtxt ## doc_id sentence_id token_id token pos tag ## 1 d1 1 1 spaCy VERB VBN ## 2 d1 1 2 is AUX VBZ ## 3 d1 1 3 great ADJ JJ ## 4 d1 1 4 at ADP IN ## 5 d1 1 5 fast ADJ JJ ## 6 d1 1 6 natural ADJ JJ ## 7 d1 1 7 language NOUN NN ## 8 d1 1 8 processing NOUN NN ## 9 d1 1 9 . PUNCT . ## 10 d2 1 1 Mr. PROPN NNP ## 11 d2 1 2 Smith PROPN NNP ## 12 d2 1 3 spent VERB VBD ## 13 d2 1 4 two NUM CD ## 14 d2 1 5 years NOUN NNS ## 15 d2 1 6 in ADP IN ## 16 d2 1 7 North PROPN NNP ## 17 d2 1 8 Carolina PROPN NNP ## 18 d2 1 9 . PUNCT . 1.6 파이썬 nltk, konlpy reticulate 패키지를 사용해서 파이썬 텍스트 처리 패키지 nltk, konlpy 기능을 활용하여 효과적인 한국어 텍스트 처리도 가능하다. 즉 bitTA 같은 텍스트 패키지를 사용해서 직접 R에서 텍스트 분석을 수행해도 되지만 파이썬에서 이미 구축된 한글 패키지를 파이썬에서 처리하고 결과값을 reticulate 패키지 통해서 R에서 후속 작업을 수행하는 작업흐름도 권장할만하다. 먼저 konlpy는 기본 파이썬 패키지로 설치되지 않아 pip install 명령어로 설치한다. nltk 패키지는 아나콘다를 설치한 경우 자동으로 설치되어 있다. 하지만 nltk.download()을 실행하여 All Packages탭 선택 후에 Punkt와 Stopwords를 추가로 다운로드 받아 설치작업을 마무리한다. ! pip install konlpy import nltk nltk.download() RStudio 혹은 파이썬 IDE에서 설치된 파이썬 한글처리 패키지가 제대로 동작되는지 꼬꼬마를 사용해서 테스트한다. 다음으로 파이썬에서 품사(Pos)를 꼬꼬마를 사용해서 처리한다. 처리한 결과를 파이썬 객체 예를 들어 kkma_pos_res 변수에 저장하고 이를 R에서 불러온다. import nltk ## &lt;frozen importlib._bootstrap&gt;:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject from konlpy.tag import Kkma kkma = Kkma() kkma.sentences(&#39;한국어 텍스트 분석을 R로 가능해요? 딥러닝 시대 맞는건가요? 한편으로는 정말 빨리 잘 할 수 있을지 많이 기대됩니다.&#39;) ## ['한국어 텍스트 분석을 R로 가능해요?', '딥 러닝 시대 맞는 건가요?', '한편으로는 정말 빨리 잘 할 수 있을지 많이 기대됩니다.'] kkma_pos_res = kkma.pos(&#39;한편으로는 정말 빨리 잘 할 수 있을지 많이 기대됩니다.&#39;) reticulate를 실행하게 되면 py$... 와 같은 방식으로 ... 객체를 R에서 불러와서 후속작업을 수행할 수 있다. library(reticulate) library(tidyverse) pos_tbl &lt;- py$kkma_pos_res %&gt;% enframe() %&gt;% mutate(text = map_chr(value, 1), pos = map_chr(value, 2)) pos_tbl ## # A tibble: 15 × 4 ## name value text pos ## &lt;int&gt; &lt;list&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 &lt;list [2]&gt; 한편 NNG ## 2 2 &lt;list [2]&gt; 으로 JKM ## 3 3 &lt;list [2]&gt; 는 JX ## 4 4 &lt;list [2]&gt; 정말 MAG ## 5 5 &lt;list [2]&gt; 빨리 MAG ## 6 6 &lt;list [2]&gt; 잘 MAG ## # … with 9 more rows 데이터프레임으로 가져왔기 때문에 NNG 명사만 추출하여 후속 작업을 이어간다. pos_tbl %&gt;% filter(pos == &quot;NNG&quot;) ## # A tibble: 1 × 4 ## name value text pos ## &lt;int&gt; &lt;list&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 &lt;list [2]&gt; 한편 NNG https://taku910.github.io/mecab/ 발췌↩︎ "],["prep.html", "2 . 준비운동 2.1 패키지와 rtools 2.2 경고와 오류 2.3 코딩 스타일 2.4 파일경로 표시 2.5 파일과 객체 2.6 따옴표와 백틱 2.7 기타", " 2 . 준비운동 2.1 패키지와 rtools 패키지(Package)는 R함수, 데이터, 완성된 코드를 정리해 종합한 꾸러미다. 라이브러리(library)라고도 한다. 패키지를 R환경에 올리는 함수가 library()인 이유다. require()도 패키지를 R환경에 올리는 함수이나 library()와는 달리, TRUE와 FALSE값을 산출한다. 패키지::함수와 같은 방식을 이용해도 패키지의 함수를 이용할 수 있다. 아래 예문은 psych패키지의 headTail()함수 이용. 예: psych::headTail(df) sessionInfo()함수를 이용해 R환경에 올려진 패키지와 버전을 확인할 수 있다. 패키지 버전만 확인하고 싶으면 packageVersion(\"패키지명\")을 이용한다. R Studio에서 버전을 확인하는 방법은 다음과 같다. Tools -&gt; Check for Package Updates ... 패키지를 설치할 때는 RStudio를 관리자 권한으로 실행해야 제대로 설치된다. 윈도가 보안정책을 강화했기 때문이다. 2.1.1 rtools 윈도에서 R의 다양한 패키지를 설치하고 사용하는데 필요한 패키지다. (맥이나 리눅스에서는 설치 불필요.) CRAN의 rtools페이지에 접속한다. 설치된 R버전과 일치하는 버전의 rtools를 다운로드받는다. 예를 들어, R이 4.0대 버전이면 rtools40을 설치한다. rtools는 다른 패키지와 달리 install.packages()함수로 설치하지 않고, 설치파일을 컴퓨터에서 직접 실행해 설치한다. 설치할때 경로변경하지 말고 C:/Rtools에 설치한다. 실치과정에서 Add rtools to system PATH가 체크돼 있는지 확인한다. 2.1.2 tidyverse 정돈된 세계를 구성해 R을 한층 뛰어난 프로그래밍 언어로 만들어준 패키지다. (tidy를 직역하면 “깔끔한”이지만, tidyverse의 취지에 맞게 tidy를 “정돈된”으로 번역한다. ) 정돈된 세상에 대해서는 한국통계학회 소식지 2019년 10월호에 실린 “데이터사이언스 운영체계 tidyverse 참조. https://statkclee.github.io/ds-authoring/ds-stat-tidyverse.html tidyverse는 정돈된 세상(tidy + universe)이란 의미의 종합패키지다. “정돈된 세계(tidyverse)”안에서는 코딩작업을 직관적이고도 일관성있게 할 수 있다. 해들리 위컴(Hadley Wicham)이 제시한 “정돈된 데이터 원리(tidy data principle)”에 따라 구성한 세계관이다. tidyverse패키지를 설치하면 ggplot2, dplyr, tidytext, lubridate 등 다수의 유용한 패키지가 함께 설치된다. Hadley Wicham http://hadley.nz/ Tidyverse Overview https://tidyverse.tidyverse.org 먼저 install.packages()함수를 이용해 tidyverse 패키지를 설치하고, library()함수를 이용해 현재의 R환경에 올려 패키지를 사용할 수 있도록 하자. install.packages(&quot;tidyverse&quot;) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✔ ggplot2 3.3.6 ✔ purrr 0.3.4 ## ✔ tibble 3.1.7 ✔ dplyr 1.0.9 ## ✔ tidyr 1.2.0 ✔ stringr 1.4.0 ## ✔ readr 2.1.2 ✔ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 설치하면 -- Attaching packages --라는 메시지가 뜨며 아래 8종의 패키지 목록이 보인다. 이 8종의 패키지는 library(tidyverse)를 통해 tidyverse패키지를 현재의 R환경에 올리면 함께 R환경에 올라가는 부착된 패키지다. 즉, 아래 8개 패키지를 library()로 개별적으로 올리지 않아도 R환경에서 사용할 수있다. ggplot2 시각화 도구모음. 그래픽의 문법에 기초 dplyr 데이터 조작. 일종의 공구상자 tidyr 데이터 정돈. 데이터를 정돈하는 함수 제공 readr 데이터 이입. 행과 열이 있는 데이터(예: csv, tsv, fwf) 이입(import) purr 함수형프로그래밍(functional programming) 도구모음 tibble 데이터프레임의 현대적 양식 sringr문자형 데이터(string) 작업 도구 모음 forcats 데이터유형(type)이 요인(factor)일때 발생하는 문제해결 도구모음. 아래의 패키지는 tidyverse와 함께 설치되지만 library(패키지명)로 따로 R환경에 올려야 사용할 수 있는 패키지다. hms housr와 time 데이터 처리 도구 모음 lubridate date와 times 데이터 처리 도구 모음 httr 웹 API rvest 웹 스크레핑 readxl xls ghrdms xlsx 파일 이입 haven SPSS, SAS, STATA 파일 이입 jasonlite JSON xml2 XML modelr 파이프라인안에서 모델링 broom 모델을 tidydata로 2.1.3 패키지 일괄설치 텍스트마이닝에는 여러 종류의 패키지가 필요하다. 이 모든 것을 매번 설치하고 번번히 실행하는 것이 번거롭다. 게다가 이미 설치된 패키지가 무엇이 있는지 기억하기 어렵기 때문에, 패키지를 반복해서 설치하는 경우가 있다. 설치돼 있는 패키지는 설치를 피하고, 새로운 패키지만 골라서 동시에 설치하는 방법을 알아보자. 명령어 앞뒤로 괄호( )를 치면 할당한 객체의 내용을 콘솔에서 보여준다. names( installed.packages()[,&quot;Package&quot;] ) package_list &lt;- c( &quot;tidyverse&quot;, &quot;epubr&quot;, &quot;showtext&quot;, &quot;tidytext&quot;, &quot;tidymodels&quot;, &quot;tidygraph&quot;, &quot;textclean&quot; &quot;topicmodels&quot;, &quot;stm&quot;) ( package_list_installed &lt;- package_list %in% installed.packages()[,&quot;Package&quot;] ) ( new_pkg &lt;- package_list[!package_list_installed] ) if(length(new_pkg)) install.packages(new_pkg) 설치한 여러 패키지를 동시에 R환경에 올리는 방법은 다음과 같다. lapply함수는 데이터 하부요소에 대해 투입한 함수를 하나 씩 반복적으로 실행해준다. for의 반복문보다 효율적이어서 자주 쓰는 함수다. 아래 코드는 require함수를 설치해야 하는 패키지 목록이 담겨 있는 package_list의 하부요소(개별 패키지)에 대해 하나씩 실행하라는 의미다. character.only = TRUE 대신 ch = T로 입력해도 된다. lapply(package_list, require, character.only = TRUE) ( .packages() ) library대신 require를 쓴 이유는 반복실행 과정에 처리 결과에 대한 TRUE와 FALSE가 필요하기 때문이다. require와 library는 둘다 패키지를 R환경에 올리지만, require가 처리 결과에 대한 TRUE와 FALSE를 산출한다. 위 코드를 각자 자주 사용하는 패키지 목록으로 수정해, 별도의 R파일로 만들어 패키지를 새로 설치하거나 실행할 때 사용하면 편리하다. 매번 번거롭게 library를 반복해서 입력할 필요가 없어진다. 2.2 경고와 오류 Warning(경고)메시지는 참고만 하고 무시해도 되지만, 오류(error)메시지는 거의 대부분 해결해야 다음 단계로 넘어갈 수 있다. 오류가 발생하면 당황하지 말고, 오류메시지를 잘 읽어보자. R 언어로 텍스트 마이닝 뿐만 아니라 다양한 개발 및 분석작업을 진행할 때 흔한 오류로 다음을 꼽을 수 있다. 2.2.1 오자와 탈자 가장 흔한 오류의 원인은 오타다. 영어는 단수와 복수의 구분이 엄격하다. 철자에 ’s’가 정확하게 들어 있는지 확인한다. ’1’과 ’l’의 차이. 사람 눈에 비슷해 보이는 글자에 주의한다. 기계에게는 전혀 다른 내용이다. 예를 들어, readxl패키지를 readx1로 입력하지 않았는지 확인해보자. 2.2.2 괄호의 불일치 괄호의 앞(과 )가 일치해야 한다. 마무리를 하지 않으면 코드 실행이 될수가 없다. R스튜디오 편집창에서 오류 표시를 해 준다. 2.2.3 보이지 않는 코드 PDF문서의 내용을 R스튜디오 편집창에 복사해서 붙일 때, 문서의 코드를 그대로 복사해서 붙였으므로, 오자나 탈자가 없는데도, 코드가 작동하지 않는 경우가 있다. 이런 일이 생기는 이유는 사람의 눈에 동일한 문자라도 기계는 다른 문자코드로 인식할 수 있기 때문에. 특히, PDF문서를 복사해서 편집기에 복사해서 붙이면 사람 눈에 보이지 않는 코드도 함께 복사돼 붙여지는 경우가 있다. 이런 문제가 생기면, 메모장같은 텍스트에디터에 해당 코드를 붙여넣었다 다시 복사해서 R스튜디오 편집창에 붙여 넣으면 해결할 수 있다. 2.2.4 패키지 설치시 겪을 수 있는 오류 패키지설치할 때 아래와 같은 오류가 발생하는 경우가 있다. &gt; library(&quot;tidyverse&quot;) Error: package or namespace load failed for ‘tidyverse’ in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]): namespace ‘scales’ 0.4.0 is already loaded, but &gt;= 0.4.1 is required tidyverse패키지와 함께 설치되는 패키지가 의존하는 scale패키지가 컴퓨터에서 업데이트돼 있지 않아 오류가 발생했다는 의미다. 해당 패키지(이 경우 scale)를 새로 설치하거나 업데이트한다. 새로 설치할 때는 dependencies=TRUE를 명령어에 추가해 모든 의존 패키지도 설치하도록 하자. install.packages(&quot;패키지명&quot;, dependencies=TRUE) 패키지 업데이트를 해보는 방법도 있다. update.packages(&quot;패키지명&quot;) 만일 새로 설치해도 해결이 안되면, 패키지를 R콘솔에서 설치해본다. 이때 R을 관리자권한으로 실행하자. 그래도 안되면, 패키지가 설치돼 있는 하드디스크폴더에서 해당 패키지 폴더를 삭제하고 다시 설치한다. 패키지가 설치된 위치는 다음과 같다. .libPaths()함수를 통해 확인할 수 있다. 패키지가 설치되는 위치는 RStudio를 관리자 권한으로 실행하는 경우 C:/Program Files/R/[R버전]/library다. 관리자권한으로 실행하지 않으면 아래의 경우처럼 문서폴더에 패키지가 설치된다. 내 PC &gt; 문서 &gt; R &gt; win-library 또는 C:\\Users\\[윈도사용자]\\Documents\\R\\win-library 예를 들어: C:\\Users\\myPC\\Documents\\R\\win-library 패키지 설치위치는 .libPaths(\"폴더명\")함수로 패키지 설치위치를 바꿀 수 있다. 예를 들어, C드라이브의 library폴더를 만들어 .libPaths(\"c:/library\")를 입력하면 패키지 설치 경로가 변경된다. 2.2.5 실행할 때 겪는 오류 다음과 같은 에러메시지가 뜨는 경우가 있다. 메시지의 내용을 잘 읽어보자. Error in tibble(text = text_vword) %&gt;% unnest_tokens(output = word, input = text) : 함수 &quot;%&gt;%&quot;를 찾을 수 없습니다 실행하는데 필요한 함수 %&gt;%를 찾을 수 없다는 의미다. %&gt;%는 파이핑에 필요한 함수로서 dplyr에서 magrittr패키지를 통해 제공한다. dplyr는 tidyverse를 올리면 함께 부착된다. 따라서 아래 3가지 중 하나만 하면 해결할 수 있다. library(tidyverse) 또는 library(dplyr) 또는 library(magrittr) 마찬가지로 아래와 같은 에러메시지가 뜨면 어떻게 해야할까? Error in unnest_tokens(., output = word, input = text) : 함수 &quot;unnest_tokens&quot;를 찾을 수 없습니다 오류메시지의 내용을 보니, unnest_tokens함수가 없다고 한다. unnest_tokens가 어디에 속해 있는 함수인지 검색해보자. tidytext패키지의 함수란 정보를 어렵지 않게 찾을 수 있다. 따라서 unnest_tokens함수를 제공하는 패키지 tidytext를 실행한다. 설치가 안돼 있으면 설치부터 한다. install.packages(&quot;tidytext&quot;) library(tidytext) 기타 패키지 설치 관련 정보는 이 문서 참조 2.3 코딩 스타일 일관성 있는 코드작성은 해들리 위컴의 R스타일 가이드를 참조하자. the tidyverse style guide 일관성 있는 R 코드 작성하기: 해들리 위컴의 R 코딩 스타일 가이드 데이터를 할당한 객체이름을 정할 때 객체의 특성(예: 데이터구조)를 명기하는 습관을 들이는 것이 좋다. 예: 벡터: name_v 데이터프레임: name_df 매트릭스: name_m 리스트: name_l 사용자 함수: name_f 토큰: name_tk 2.4 파일경로 표시 2.4.1 슬래시 / 윈도에서는 경로(path) 표시 역슬래시\\를 사용하지만, R환경에서는 슬래시/를 이용해 경로를 표시한다. \\\\처럼 역슬래시를 두번 쓰기도 한다. /만 쓰거나 첫번째로 적는 /는 루트디렉토리를 의미한다. list.files(&quot;/&quot;) #루트디렉토리에 있는 모든 파일과 폴더 표시 list.files(&quot;/data&quot;) #루트디렉토리 아래의 data폴더에 있는 모든 파일과 폴더 표시 list.files(&quot;data&quot;) # 현재 작업디렉토리 아래의 data폴더에 있는 모든 파일과 폴더 표시 2.4.2 퀴즈 list.files(\"data\\images\")를 실행하면 아래와 같은 오류가 발생한다. 이유는? &gt; list.files(&quot;data\\images&quot;) 에러: &quot;&quot;C:\\U&quot;로 시작하는 문자열들 가운데 16진수가 아닌 &quot;\\U&quot;가 사용되었습니다 윈도에서 사용하는 경로표시 \\를 R환경에서 사용했기 때문이다. \\를 /로 수정한다. 2.4.3 마침표 . .. 마침표 하나. 현재 작업디렉토리 마침표 둘.. 현재 작업디렉토리 바로 한단계 위에 있는 폴더 list.files(&quot;.&quot;) #작업디렉토리에 있는 모든 파일과 폴더 표시 list.files(&quot;./images&quot;) #작업디렉토리아래에 있는 images폴더에 있는 모든 파일과 폴더 표시. list.files(\"./images\")의 산출결과는 list.files(\"images\")와 같다. ./images와 images는 둘다 현재 작업디렉토리 아래에 있는 images란 의미. list.files(&quot;..&quot;) # 작업디렉토리 위 폴더에 있는 모든 파일과 폴더 표시 list.files(&quot;../dir&quot;) #작업디렉토의 위 폴더 아래(즉, 현 작업디렉토리와 같은 단계의 위치에 있는 폴더) 아래에 있는 images폴더에 있는 모든 파일과 폴더 표시 2.4.4 물결 ~ 물결표시~는 홈디렉토리를 의미한다. 로그인 사용자가 사용할 수 있는 최상위 디렉토리. 윈도의 경우에서 만일 사용자 ID를 user1이라고 한 경우 C:/Users/user1/Documents폴더가 홈디렉토리가 된다. (윈도환경에서는 경로를 역슬래시를 사용하므로 C:\\Users\\user1\\Documents로 표시된다.) 따라서 list.files(\"~\")과 list.files(\"C:/Users/user1/Documents\")는 같은 결과 산출. list.files(&quot;~&quot;) #홈디렉토리에 있는 모든 파일 표시 list.files(&quot;~/R/win-library&quot;) #홈디렉토리 아래단계에 있는 R/win-library폴더의 모든 파일과 폴더 표시 2.5 파일과 객체 2.5.1 로컬 파일 처리 함수 하드디스크에 있는 로컬 파일을 다루는 함수는 디렉토리 생성을 할 때 dir.create(), 디렉토리 파일과 폴더 목록을 확인할 때 list.files() 함수를 사용한다. dir.create(&quot;C:/newdir&quot;) 디렉토리(폴더) 생성 list.files() 현재 작업디렉토리 파일과 폴더 목록 2.5.2 R 세센 객체 처리 함수 R을 실행하게 되면 세션을 생성하는데 해당 세션 작업공간에 저장된 객체 목록을 살펴볼 때 ls(), 객체를 삭제할 때는 rm() 명령어를 사용한다. ls() 현재 작업공간에 있는 객체 목록 rm(객체명) 작업공간의 객체 제거. 필요없는 객체 제거에 사용. 현재 작업중인 디렉토리를 파악할 때는 getwd(), 작업 디렉토리를 지정할 때는 setwd() 명령어를 사용한다. getwd() 작업디렉토리(working directory) 표시 setwd(&quot;C:/newdir&quot;) &quot;C:/newdir&quot;을 작업디록토리 설정 단일 R객체를 파일로 저장하는 방법은 saveRDS() 반대로 이 파일을 로컬 컴퓨터에서 R 세션으로 불러올 때는 readRDS() 함수를 사용한다. saveRDS(name_df, file = &quot;name_df.rds&quot;) name_df &lt;- readRDS(&quot;name_df.rds&quot;) 복수의 R객체를 파일로 저장하고 불러들일 때는 save()와 load() 함수를 사용한다. save(n1_df, n2_df, file = &quot;name.RData&quot;) n1_df과 n2_df객체를 작업디렉토리에 name.RData란 이름의 파일로 저장 load(&quot;name.RData&quot;) name.RData를 작업공간에 탑재 R환경의 모든 객체를 이미지로 저장할 때는 save.images() 함수와 이를 다시 불러올 때는 load() 함수를 사용한다. save.images(&quot;myData.RData&quot;) 작업공간을 이미지파일로 저장. 파일명 지정하지 않으면 .RData로 저장. load(&quot;myData.RData&quot;) 작업디렉토리 파일을 현 작업공간에 객체로 탑재 fst패키지를 이용하면 대용량 파일을 빠르게 처리할 수있다. fst: http://www.fstpackage.org # Store the data frame to disk write.fst(df, &quot;dataset.fst&quot;) # Retrieve the data frame again df &lt;- read.fst(&quot;dataset.fst&quot;) 2.6 따옴표와 백틱 2.6.1 겹따옴표 홑따옴표 겹따옴표와 홑따옴표는 문자형요소를 지칭하는데, 함께 사용해야 할때가 있다. 인용문안에서 인용해야 할때는 겹따옴표와 홑따옴표를 함께 사용한다. &quot;이렇게 인용문 안에 &#39;다시 인용할때&#39; 겹따옴표와 홑따옴표를 함께 사용&quot; 2.6.2 백틱(backquote/backtick) ` 변수 혹은 객체명이 한글 특수기호 혹은 띄어쓰기가 있는 경우, 키보드 가장 왼쪽 위에 있는 부호 백틱 한쌍을 이용한다. `한글변수(명)` 2.7 기타 2.7.1 폴더명칭 주의사항 R작업에 관련된 폴더이름은 모두 영문으로 지정한다. 한글명 폴더를 이용할 경우 오류발생하는 경우가 드물지 않다. 폴더명을 지정할 때 띄어쓰기를 하지않는다. 오류의 원인이다. 데이터분석 작업을 하는 폴더의 이름은 모두 영문으로 붙여쓴다. R스튜디오는 작업디렉토리 기본설정이 홈디렉토리(~)로 돼 있다. 만일 윈도나 맥 등 운영체제에서 한글을 사용자명으로 지정했을 꼉우, 경로에 한글폴더가 포함된다. 대부분 문제없지만, 간혹 오류 발생의 원인이 된다. 작업디렉토리 기본값을 루트디렉토리 아래 영문폴더(예: C:/data)를 만들어 설정하자. 2.7.2 NA와 NULL의 차이 결측값 NA는 값이 없지만, 값이 들어갈 자리는 있는 상태. NULL은 값과 자리가 모두 없는 상태. 0은 결측값이 아니다. 0은 숫자형 값 \"0\"은 문자형 값 "],["browse.html", "3 . 헬로 월드 3.1 수집 3.2 정제 3.3 분석 3.4 소통 3.5 연습 3.6 과제", " 3 . 헬로 월드 인식론의 틀에서 우리가 사는 세상은 세상 그 자체가 아니라, 우리 마음에 속에 비친 그림자이다. 따라서 세상은 “인식전 세상”과 “인식된 세상”으로 구분할 수 있다. 인식의 과정은 자료-정보-지식-지혜(DIKW: Data, Information, Knowledge, Wisdom) 위계론을 통해 4단계 모형으로 구성할 수 있다. 1단계: 인식전 세계에서 원자료(raw data) 수집 2단계: 원자료 정제해 자료(data)로 1차부호화 3단계: 자료를 분석해 정보(information)로 2차부호화 4단계: 정보를 지식(knowledge)으로 해석하고 지혜(wisdom)로 내면화 이를 순서대로 제시하면 다음과 같다. 이를 소통의 관점에서 재구성하면 해석과 내면화의 과정을 의미공유 과정인 소통(communication)으로 대체할 수 있다. 이를 재정리하면 다음과 같다. 텍스트마이닝에도 이 위계 과정을 적용해, 수집 → 정제 → 분석 → 소통 등의 구조로 이뤄진다. 각 단계별로 자세하게 다루기 이전에 전반적인 구조를 먼저 이해하자. 수집 정제(전처리) 분석 소통 3.1 수집 텍스트마이닝의 첫 단계다. 원자룔 수집한 다음, 정제하고 분석하기 위해 R환경에 탑재하는 단계다. 텍스트데이터가 저장된 데이터구조는 다양하다. 벡터(vector), 데이터프레임(data frame), 리스트(list) 등의 구조에 저장해 사용한다. 가장 기본적인 텍스트데이터 구조는 문자형으로 구성된 문자벡터다. 학이편 1장의 텍스트데이터를 문자벡터에 담아 불러와 보자. 배우며 제때에 실행하면 진실로 즐겁지 않겠는가? 벗이 먼 곳에서부터 온다면 참으로 즐겁지 않겠는가? 남이 알아주지 않아도 성내지 않는다면 참으로 군자답지 않겠는가? text_v &lt;- c(&quot;배우며 제때에 실행하면 진실로 즐겁지 않겠는가? 벗이 먼 곳에서부터 온다면 참으로 즐겁지 않겠는가? 남이 알아주지 않아도 성내지 않는다면 참으로 군자답지 않겠는가?&quot;) text_v ## [1] \"배우며 제때에 실행하면 진실로 즐겁지 않겠는가?\\n 벗이 먼 곳에서부터 온다면 참으로 즐겁지 않겠는가?\\n 남이 알아주지 않아도 성내지 않는다면 참으로 군자답지 않겠는가?\" \\n은 행바꿈을 의미하는 정규표현식이다. 3.2 정제 3.2.1 정돈 텍스트 문자형, 숫자형, 논리형 등의 데이터 유형 중 텍스트데이터유형은 문자형이다. 그런데 데이터 유형이 문자형이어서는 컴퓨터로 분석할 수 없다. 양화(quantification)시켜 컴퓨터가 계산할 수 있도록 바꿔줘야 한다. f 다양한 방법이 있는데, 데이터프레임을 “정돈된 데이터 원리(tidy data principle)”에 따라 만든 정돈텍스트(tidy text)구조부터 시작하자. 정돈텍스트(tidy text)는 데이터프레임을 텍스트마이닝에 적합하도록 만든 데이터구조다. 일반적으로 데이터프레임은 복수의 열(column)로 이뤄져 있는데, 정돈텍스트 구조에서는 열을 단 하나로 고정시켰다. 즉, 행(row) 하나에 토큰(token)이 하나만 할당돼 있다(one-token-per-row). 토큰은 텍스트분석의 기본 단위다. 단어 하나를 토큰으로 이용하기도 하고, 복수의 단어(n-gram)를 묶어 하나의 토큰으로 이용하기도 한다. 3.2.2 토큰화 정돈된 깔끔한 텍스트 분석을 위해서 데이터프레임 구성과 함께 텍스트를 데이터프레임에 깔끔하게 변환시킬 수 있는 tidytext 패키지 unnest_tokens() 함수가 필요하다. 데이터프레임 구성 tibble 토큰화 unnest_tokens 정돈된 세계(tidyverse)에서 텍스트데이터를 정돈텍스트 구조에 담는 함수가 tidytext패키지에서 제공하는 unnest_tokens다. (철자에 주의하자. 복수 s가 붙어 있다.) install.packages(&quot;tidytext&quot;) tidytext의 자세한 사용법은 아래 사용설명서를 참조한다. tidytext https://cran.r-project.org/web/packages/tidytext/ unnest_tokens은 데이트프레임 구조의 데이터를 받아 처리한다. 앞서 만든 문자벡터 text_v를 정돈텍스트로 바꾸기 위해서는 먼저 데이터프레임으로 바꿔야 한다. tibble패키지의 tibble함수를 이용해 text_v의 내용을 데이터프레임에 담아 보자. tibble은 tidyverse에 부착돼 있다. 티블(tibble)은 현대적인 데이터프레임의 형식이다. 문자열을 요인(factor)형식으로 로 바꾸지 않는 등 텍스트분석에 사용하기 좋다. 데이터프레임의 열 이름을 무엇으로 설정했는지에 주의하자. 여기서는 “text”로 설정했다. library(tidyverse) text_df &lt;- tibble(text = text_v) text_df ## # A tibble: 1 × 1 ## text ## &lt;chr&gt; ## 1 \"배우며 제때에 실행하면 진실로 즐겁지 않겠는가?\\n … 이제 생성된 데이터프레임을 tidytext패키지의 unnest_tokens함수를 이용해 정돈텍스트 구조로 바꿔보자. (철자에 주의. 복수 s) 앞서 만든 데이터프레임의 열 이름(“text”)을 input에 투입한 것을 잘 기억하자. library(tidytext) text_df %&gt;% unnest_tokens(output = word, input = text) ## # A tibble: 21 × 1 ## word ## &lt;chr&gt; ## 1 배우며 ## 2 제때에 ## 3 실행하면 ## 4 진실로 ## 5 즐겁지 ## 6 않겠는가 ## # … with 15 more rows input : 입력한 데이터프레임의 열 이름 output : 출력할 정돈텍스트의 열 이름 이처럼 텍스트데이터를 컴퓨터가 분석할 수 있도록 양화할 수 있는 단위로 나누는 것은 토큰화(tkoenization)이라고 한다. 이번에는 토큰을 단어 2개로 묶은 ngram으로 정돈텍스트를 만들어 보자. text_df %&gt;% unnest_tokens(output = word, input = text, token = &quot;ngrams&quot;, n = 2 ) ## # A tibble: 20 × 1 ## word ## &lt;chr&gt; ## 1 배우며 제때에 ## 2 제때에 실행하면 ## 3 실행하면 진실로 ## 4 진실로 즐겁지 ## 5 즐겁지 않겠는가 ## 6 않겠는가 벗이 ## # … with 14 more rows unnest_tokens함수의 자세한 사용법은 ?unnest_tokens의 도움말을 참고한다. 3.3 분석 3.3.1 빈도: count 함수 이제 텍스트마이닝을 해보자. 어떤 문서에서 많이 사용하는 단어가 있다면, 그 문서는 그 단어가 나타내는 의미에 의해 규정된다고 할 수 있다. “사랑”이란 단어를 많이 사용한 문서면 사랑에 대한 문서이고, “학습”이란 단어를 많이 사용했으면 학습에 대한 문서일 가능성이 높다. 문서에 등장하는 단어를 세어 주는 함수가 count다. count함수를 앞서 정돈텍스트 구조에 저장된 텍스트에 많이 등장한 단어가 무멋인지 찾아보자. text_tk &lt;- text_df %&gt;% unnest_tokens(output = word, input = text) text_tk %&gt;% count(word, sort = TRUE) ## # A tibble: 17 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 않겠는가 3 ## 2 즐겁지 2 ## 3 참으로 2 ## 4 곳에서부터 1 ## 5 군자답지 1 ## 6 남이 1 ## # … with 11 more rows “않겠는가”가 3회, “즐겁지”와 “참으로”로가 각각 2회 사용됐다. 즐거움에 관한 문서라고 가늠할 수 있다. 3.4 소통 분석결과를 해석해 의미를 공유하는 과정이다. 분석결과를 요약 통계량 예를 들어 빈도수, 혹은 표형태로 깔끔하게 정리하거나, 도표로 시각화한다. 물론 글로도 분석결과의 의미를 정리한다. 여기서는 먼저 분석결과를 도표에 시각화하는 작업부터 해보자. 3.4.1 시각화 분석결과가 데이터프레임에 저장돼 있으므로 ggplot2패키지로 직접 파이핑(piping)할 수 있다. ggplot2는 파이프(%&gt;%)가 아니라 +로 레이어를 더하는 방식을 이용한다는 점에 주의하자. ggplot2는 tidyverse에 부착돼 있다. 다음 코드는 2회 이상 등장filter(n &gt; 1)한 단어를 등장빈도 순서대로 정렬mutate(word = reorder(word, n))해 막대도포geom_col()로 시각화했다. text_tk %&gt;% count(word, sort = TRUE) %&gt;% filter(n &gt; 1) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(word, n)) + geom_col() + coord_flip() 3.4.2 표 그래프 문법으로 시각화하는 방식이 ggplot이라면 동일한 방식으로 표를 작성하는 표문법을 구현한 것이 gt 패키지다. 이를 통해 가장 언급이 많은 빈도수를 갖는 단어를 표로 깔끔하게 정리해보자. library(gt) # install.packages(&quot;gt&quot;) text_tk %&gt;% count(word, sort = TRUE) %&gt;% filter(n &gt; 1) %&gt;% mutate(word = reorder(word, n)) %&gt;% gt() %&gt;% tab_header( title = &quot;언급 많은 단어&quot; ) %&gt;% cols_label( word = &quot;단어&quot;, n = &quot;언급횟수&quot; ) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #akplliztue .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #akplliztue .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #akplliztue .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #akplliztue .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #akplliztue .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #akplliztue .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #akplliztue .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #akplliztue .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #akplliztue .gt_column_spanner_outer:first-child { padding-left: 0; } #akplliztue .gt_column_spanner_outer:last-child { padding-right: 0; } #akplliztue .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #akplliztue .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #akplliztue .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #akplliztue .gt_from_md > :first-child { margin-top: 0; } #akplliztue .gt_from_md > :last-child { margin-bottom: 0; } #akplliztue .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #akplliztue .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #akplliztue .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #akplliztue .gt_row_group_first td { border-top-width: 2px; } #akplliztue .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #akplliztue .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #akplliztue .gt_first_summary_row.thick { border-top-width: 2px; } #akplliztue .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #akplliztue .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #akplliztue .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #akplliztue .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #akplliztue .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #akplliztue .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #akplliztue .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #akplliztue .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #akplliztue .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #akplliztue .gt_left { text-align: left; } #akplliztue .gt_center { text-align: center; } #akplliztue .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #akplliztue .gt_font_normal { font-weight: normal; } #akplliztue .gt_font_bold { font-weight: bold; } #akplliztue .gt_font_italic { font-style: italic; } #akplliztue .gt_super { font-size: 65%; } #akplliztue .gt_two_val_uncert { display: inline-block; line-height: 1em; text-align: right; font-size: 60%; vertical-align: -0.25em; margin-left: 0.1em; } #akplliztue .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #akplliztue .gt_asterisk { font-size: 100%; vertical-align: 0; } #akplliztue .gt_slash_mark { font-size: 0.7em; line-height: 0.7em; vertical-align: 0.15em; } #akplliztue .gt_fraction_numerator { font-size: 0.6em; line-height: 0.6em; vertical-align: 0.45em; } #akplliztue .gt_fraction_denominator { font-size: 0.6em; line-height: 0.6em; vertical-align: -0.05em; } 언급 많은 단어 단어 언급횟수 않겠는가 3 즐겁지 2 참으로 2 3.5 연습 다음은 소설가가 이상의 오감도다. 오감도에서 자주 사용된 단어의 빈도를 계산하시오. 13인의 아해가 도로로 질주하오. (길은 막다른 골목이 적당하오.) 제1의 아해가 무섭다고 그리오. 제2의 아해도 무섭다고 그리오. 제3의 아해도 무섭다고 그리오. 제4의 아해도 무섭다고 그리오. 제5의 아해도 무섭다고 그리오. 제6의 아해도 무섭다고 그리오. 제7의 아해도 무섭다고 그리오. 제8의 아해도 무섭다고 그리오. 제9의 아해도 무섭다고 그리오. 제10의 아해도 무섭다고 그리오. 제11의 아해가 무섭다고 그리오. 제12의 아해도 무섭다고 그리오. 제13의 아해도 무섭다고 그리오. 13인의 아해는 무서운 아해와 무서워하는 아해와 그렇게뿐이 모였소. (다른 사정은 없는 것이 차라리 나았소) 그중에 1인의 아해가 무서운 아해라도 좋소. 그중에 2인의 아해가 무서운 아해라도 좋소. 그중에 2인의 아해가 무서워하는 아해라도 좋소. 그중에 1인의 아해가 무서워하는 아해라도 좋소. (길은 뚫린 골목이라도 적당하오.) 13인의 아해가 도로로 질주하지 아니하여도 좋소. 작업순서는 다음과 같다. 자료준비 문자벡터에 저장 c 데이터프레임으로 변환 tibble 정제 토큰화 unnest_tokens 분석 count 소통(시각화) ggplot 텍스트 양이 많은 편이니 텍스트 객체를 별도로 마련하자. ogamdo_txt &lt;- &quot;13인의 아해가 도로로 질주하오. (길은 막다른 골목이 적당하오.) 제1의 아해가 무섭다고 그리오. 제2의 아해도 무섭다고 그리오. 제3의 아해도 무섭다고 그리오. 제4의 아해도 무섭다고 그리오. 제5의 아해도 무섭다고 그리오. 제6의 아해도 무섭다고 그리오. 제7의 아해도 무섭다고 그리오. 제8의 아해도 무섭다고 그리오. 제9의 아해도 무섭다고 그리오. 제10의 아해도 무섭다고 그리오. 제11의 아해가 무섭다고 그리오. 제12의 아해도 무섭다고 그리오. 제13의 아해도 무섭다고 그리오. 13인의 아해는 무서운 아해와 무서워하는 아해와 그렇게뿐이 모였소.(다른 사정은 없는 것이 차라리 나았소) 그중에 1인의 아해가 무서운 아해라도 좋소. 그중에 2인의 아해가 무서운 아해라도 좋소. 그중에 2인의 아해가 무서워하는 아해라도 좋소. 그중에 1인의 아해가 무서워하는 아해라도 좋소. (길은 뚫린 골목이라도 적당하오.) 13인의 아해가 도로로 질주하지 아니하여도 좋소.&quot; # 자료준비 txt_df &lt;- tibble(text = ogamdo_txt) # 정제(토큰화) 및 분석 txt_tk &lt;- txt_df %&gt;% unnest_tokens(output = word, input = text) %&gt;% count(word, sort = T) txt_tk ## # A tibble: 45 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 그리오 13 ## 2 무섭다고 13 ## 3 의 13 ## 4 제 13 ## 5 아해도 11 ## 6 아해가 8 ## # … with 39 more rows 이제 분석결과를 ggplot2의 막대도표로 시각화하자. 4회 이상 등장한 단어만 포함시키자. txt_tk %&gt;% filter(n &gt;= 4) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(word, n)) + geom_col() + coord_flip() 3.6 과제 소설이나 신문기사 등 마음에 드는 텍스트를 한 건 골라, 단어빈도를 계산하시오. 분석결과를 표와 막대도표로 제시하시오. "],["view-text.html", "4 . 텍스트 살펴보기 4.1 특정 단어 강조 4.2 단어 위치 4.3 뉴스기사 요약", " 4 . 텍스트 살펴보기 4.1 특정 단어 강조 윤석열 대통령 취임사 텍스트를 취임사_윤석열.txt 파일로 저장한 후에 특정 단어 자유 를 탐색하여 색상을 달리하여 출력해보자. ## 기본 텍스트 패키지 library(tidyverse) library(tidytext) ## 한국 텍스트 처리 패키지 # library(RMeCab) # library(bitTA) ## 글 색상 library(glue) library(crayon) library(fansi) options(crayon.enabled = TRUE) yoon_raw &lt;- read_lines(&quot;data/취임사_윤석열.txt&quot;) yoon_txt &lt;- yoon_raw[yoon_raw !=&quot;&quot;] crayon_words &lt;- function(input_text, word = &quot;자유&quot;) { replaced_text &lt;- str_replace_all(input_text, word, &quot;{red {word}}&quot;) for(i in 1:length(replaced_text)) { crayon_text &lt;- glue::glue_col(deparse(replaced_text[[i]])) print(crayon_text) } } crayon_words(input_text = yoon_txt, &quot;자유&quot;) ## \"존경하고 사랑하는 국민 여러분,\" ## \"750만 재외동포 여러분,\" ## \"그리고 자유를 사랑하는 세계 시민 여러분,\" ## \"저는 이 나라를 자유민주주의와 시장경제 체제를 기반으로 국민이 진정한 주인인 나라로 재건하고, 국제사회에서 책임과 역할을 다하는 나라로 만들어야 하는 시대적 소명을 갖고 오늘 이 자리에 섰습니다.\" ## \"역사적인 자리에 함께해 주신 국민 여러분께 감사드립니다.\" ## \"문재인, 박근혜 전 대통령, 그리고 할리마 야콥 싱가포르 대통령, 포스탱 아르샹쥬 투아데라 중앙아프리카공화국 대통령, 왕치산 중국 국가부주석, 메가와티 수카르노푸트리 인도네시아 전 대통령, 더글러스 엠호프 해리스 미국 부통령 부군, 조지 퓨리 캐나다 상원의장, 하야시 요시마사 일본 외무상을 비롯한 세계 각국의 경축 사절과 내외 귀빈 여러분께도 깊이 감사드립니다.\" ## \"이 자리를 빌려 지난 2년간 코로나 팬데믹을 극복하는 과정에서 큰 고통을 감내해주신 국민 여러분께 경의를 표합니다.\" ## \"그리고 헌신해주신 의료진 여러분께도 감사드립니다.\" ## \"존경하는 국민 여러분,\" ## \"세계 시민 여러분,\" ## \"지금 전 세계는 팬데믹 위기, 교역 질서의 변화와 공급망의 재편, 기후 변화, 식량과 에너지 위기, 분쟁의 평화적 해결의 후퇴 등 어느 한 나라가 독자적으로, 또는 몇몇 나라만 참여해서 해결하기 어려운 난제들에 직면해 있습니다.\" ## \"다양한 위기가 복합적으로 인류 사회에 어두운 그림자를 드리우고 있는 것입니다.\" ## \"또한 우리나라를 비롯한 많은 나라들이 국내적으로 초저성장과 대규모 실업, 양극화의 심화와 다양한 사회적 갈등으로 인해 공동체의 결속력이 흔들리고 와해되고 있습니다.\" ## \"한편, 이러한 문제들을 해결해야 하는 정치는 이른바 민주주의의 위기로 인해 제 기능을 하지 못하고 있습니다.\" ## \"가장 큰 원인으로 지목되는 것이 바로 반지성주의입니다.\" ## \"견해가 다른 사람들이 서로의 입장을 조정하고 타협하기 위해서는 과학과 진실이 전제되어야 합니다.\" ## \"그것이 민주주의를 지탱하는 합리주의와 지성주의입니다.\" ## \"국가 간, 국가 내부의 지나친 집단적 갈등에 의해 진실이 왜곡되고, 각자가 보고 듣고 싶은 사실만을 선택하거나 다수의 힘으로 상대의 의견을 억압하는 반지성주의가 민주주의를 위기에 빠뜨리고 민주주의에 대한 믿음을 해치고 있습니다.\" ## \"이러한 상황이 우리가 처해있는 문제의 해결을 더 어렵게 만들고 있습니다.\" ## \"그러나 우리는 할 수 있습니다.\" ## \"역사를 돌이켜 보면 우리 국민은 많은 위기에 처했지만 그럴 때마다 국민 모두 힘을 합쳐 지혜롭게, 또 용기있게 극복해 왔습니다.\" ## \"저는 이 순간 이러한 위기를 극복하는 책임을 부여받게 된 것을 감사한 마음으로 받아들이고, 우리 위대한 국민과 함께 당당하게 헤쳐 나갈 수 있다고 확신합니다.\" ## \"또 세계 시민과 힘을 합쳐 국내외적인 위기와 난제들을 해결해 나갈 수 있다고 믿습니다.\" ## \"존경하는 국민 여러분,\" ## \"세계 시민 여러분,\" ## \"저는 이 어려움을 해결해 나가기 위해서 우리가 보편적 가치를 공유하는 것이 매우 중요하다고 생각합니다.\" ## \"그것은 바로 ‘자유’입니다.\" ## \"우리는 자유의 가치를 제대로, 그리고 정확하게 인식해야 합니다.\" ## \"자유의 가치를 재발견해야 합니다.\" ## \"인류 역사를 돌이켜보면 자유로운 정치적 권리, 자유로운 시장이 숨 쉬고 있던 곳은 언제나 번영과 풍요가 꽃 피었습니다.\" ## \"번영과 풍요, 경제적 성장은 바로 자유의 확대입니다.\" ## \"자유는 보편적 가치입니다.\" ## \"우리 사회 모든 구성원이 자유 시민이 되어야 하는 것입니다.\" ## \"어떤 개인의 자유가 침해되는 것이 방치된다면 우리 공동체 구성원 모두의 자유마저 위협받게 됩니다.\" ## \"자유는 결코 승자독식이 아닙니다.\" ## \"자유 시민이 되기 위해서는 일정한 수준의 경제적 기초, 그리고 공정한 교육과 문화의 접근 기회가 보장되어야 합니다.\" ## \"이런 것 없이 자유 시민이라고 할 수 없습니다.\" ## \"어떤 사람의 자유가 유린되거나 자유 시민이 되는데 필요한 조건을 충족하지 못한다면 모든 자유 시민은 연대해서 도와야 합니다.\" ## \"그리고 개별 국가뿐 아니라 국제적으로도 기아와 빈곤, 공권력과 군사력에 의한 불법 행위로 개인의 자유가 침해되고 자유 시민으로서의 존엄한 삶이 유지되지 않는다면 모든 세계 시민이 자유 시민으로서 연대하여 도와야 하는 것입니다.\" ## \"모두가 자유 시민이 되기 위해서는 공정한 규칙을 지켜야 하고, 연대와 박애의 정신을 가져야 합니다.\" ## \"존경하는 국민 여러분,\" ## \"국내 문제로 눈을 돌려 제가 중요하게 생각하는 방향에 대해 말씀드리겠습니다.\" ## \"우리나라는 지나친 양극화와 사회 갈등이 자유와 민주주의를 위협할 뿐 아니라 사회 발전의 발목을 잡고 있습니다.\" ## \"저는 이 문제를 도약과 빠른 성장을 이룩하지 않고는 해결하기 어렵다고 생각합니다.\" ## \"빠른 성장 과정에서 많은 국민이 새로운 기회를 찾을 수 있고, 사회 이동성을 제고함으로써 양극화와 갈등의 근원을 제거할 수 있습니다.\" ## \"도약과 빠른 성장은 오로지 과학과 기술, 그리고 혁신에 의해서만 이뤄낼 수 있는 것입니다.\" ## \"과학과 기술, 그리고 혁신은 우리의 자유민주주의를 지키고 우리의 자유를 확대하며 우리의 존엄한 삶을 지속 가능하게 할 것입니다.\" ## \"과학과 기술, 그리고 혁신은 우리나라 혼자만의 노력으로는 달성하기 어렵습니다.\" ## \"자유와 창의를 존중함으로써 과학 기술의 진보와 혁신을 이뤄낸 많은 나라들과 협력하고 연대해야만 합니다.\" ## \"존경하는 국민 여러분,\" ## \"세계 시민 여러분,\" ## \"자유민주주의는 평화를 만들어내고, 평화는 자유를 지켜줍니다.\" ## \"그리고 평화는 자유와 인권의 가치를 존중하는 국제사회와의 연대에 의해 보장이 됩니다.\" ## \"일시적으로 전쟁을 회피하는 취약한 평화가 아니라 자유와 번영을 꽃피우는 지속 가능한 평화를 추구해야 합니다.\" ## \"전 세계 어떤 곳도 자유와 평화에 대한 위협에서 자유롭지 못합니다.\" ## \"지금 한반도와 동북아의 평화도 마찬가지입니다.\" ## \"저는 한반도뿐 아니라 아시아와 세계의 평화를 위협하는 북한의 핵 개발에 대해서도 그 평화적 해결을 위해 대화의 문을 열어놓겠습니다.\" ## \"그리고 북한이 핵 개발을 중단하고 실질적인 비핵화로 전환한다면 국제사회와 협력하여 북한 경제와 북한 주민의 삶의 질을 획기적으로 개선할 수 있는 담대한 계획을 준비하겠습니다.\" ## \"북한의 비핵화는 한반도에 지속 가능한 평화를 가져올 뿐 아니라 아시아와 전 세계의 평화와 번영에도 크게 기여할 것입니다.\" ## \"사랑하고 존경하는 국민 여러분,\" ## \"지금 우리는 세계 10위권의 경제 대국 그룹에 들어가 있습니다.\" ## \"그러므로 우리는 자유와 인권의 가치에 기반한 보편적 국제 규범을 적극 지지하고 수호하는데 글로벌 리더 국가로서의 자세를 가져야 합니다.\" ## \"우리나라뿐 아니라 세계 시민 모두의 자유와 인권을 지키고 확대하는데 더욱 주도적인 역할을 해야 합니다.\" ## \"국제사회도 대한민국에 더욱 큰 역할을 기대하고 있음이 분명합니다.\" ## \"지금 우리나라는 국내 문제와 국제 문제를 분리할 수 없습니다.\" ## \"국제사회가 우리에게 기대하는 역할을 주도적으로 수행할 때 국내 문제도 올바른 해결 방향을\" ## \"찾을 수 있는 것입니다.\" ## \"저는 자유, 인권, 공정, 연대의 가치를 기반으로 국민이 진정한 주인인 나라, 국제사회에서 책임을 다하고 존경받는 나라를 위대한 국민 여러분과 함께 반드시 만들어 나가겠습니다.\" ## \"감사합니다.\" 4.2 단어 위치 윤석열 대통령 취임사에서 가장 빈도수가 높은 명사 5개를 찾아낸다. 이를 위해서 먼저 텍스트를 각행별로 텍스트를 데이터프레임으로 변환시킨다. 그리고 나서 메카브(MeCab) 형태소 분석기를 사용해서 연설문 형태소 분석을 수행하고 명사만 추출한 후에 가장 빈도수가 높은 단어 3개를 뽑아낸다. library(RcppMeCab) library(tidytext) yoon_tbl &lt;- yoon_txt %&gt;% enframe(name = &quot;행&quot;, value = &quot;text&quot;) %&gt;% filter(text != &quot;&quot;) youn_noun &lt;- yoon_tbl %&gt;% unnest_tokens( output = 분석_텍스트, input = text, token = RcppMeCab::pos) %&gt;% separate(분석_텍스트, c(&quot;명사&quot;, &quot;형태소&quot;), sep = &quot;/&quot;) %&gt;% filter(형태소 == &quot;nng&quot;) %&gt;% count(명사, sort = TRUE, name = &quot;빈도수&quot;) youn_top_three &lt;- youn_noun %&gt;% slice_max(빈도수, n = 3) %&gt;% pull(명사) youn_top_three ## [1] \"자유\" \"국민\" \"시민\" 윤 대통령 취임사에서 가장 많이 언급된 명사 3개(자유, 국민, 시민)가 취임사 어느 부분에 위치하는지 시각화를 한다. 이를 위해서 ggpage 패키지를 활용하여 ggpage_build() 함수와 ggpage_plot() 함수를 사용하여 깔끔하게 시각화한다. library(ggpage) yoon_tbl %&gt;% ggpage_build(wtl = TRUE, lpp = 30, x_space_pages =10, y_space_pages = 0, nrow = 1) %&gt;% unnest_tokens( output = 분석_텍스트, input = word, token = RcppMeCab::pos, drop = FALSE) %&gt;% separate(분석_텍스트, c(&quot;명사&quot;, &quot;형태소&quot;), sep = &quot;/&quot;) %&gt;% mutate(highlight = case_when(명사 %in% c(&quot;자유&quot;) ~ &quot;자유&quot;, 명사 %in% c(&quot;국민&quot;) ~ &quot;국민&quot;, 명사 %in% c(&quot;시민&quot;) ~ &quot;시민&quot;, TRUE ~ &quot;기타&quot;)) %&gt;% mutate(highlight = factor(highlight, levels=c(&quot;자유&quot;, &quot;국민&quot;, &quot;시민&quot;, &quot;기타&quot;))) %&gt;% ggpage_plot(aes(fill = highlight), paper.show = TRUE, page.number = &quot;top&quot;, paper.limits = 3) + scale_fill_manual(values = c(&quot;blue&quot;, &quot;red&quot;, &quot;green&quot;, &quot;darkgray&quot;)) + labs(title = &quot;2022 윤석열 대통령 취임사&quot;, fill = NULL) + theme_void(base_family = &quot;NanumGothic&quot;) 4.3 뉴스기사 요약 Ainize Teachable-NLP를 사용한 kobart 문서요약 텍스트/신문기사를 사용하여 한겨례 신문 7월 물가 ‘두 달 연속 6%’…전기·가스·수도요금 16% 올라 기사의 일부를 발췌하여 기사를 요약해보자. 허깅페이스 사전학습모형을 가져와서 신문기사 요약에 적용한다. 사전학습모형이 크기 때문에 cache_dir를 별도 NAS 저장소에 저장시켜두고 이를 불러와서 텍스트 요약 작업을 수행한다. from transformers import pipeline ## &lt;frozen importlib._bootstrap&gt;:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject from transformers import AutoModelForSeq2SeqLM, AutoTokenizer summary_tokenizer = AutoTokenizer.from_pretrained(&quot;ainize/kobart-news&quot;, cache_dir=&quot;z:/dataset/hf/&quot;) summary_model = AutoModelForSeq2SeqLM.from_pretrained(&quot;ainize/kobart-news&quot;, cache_dir=&quot;z:/dataset/hf/&quot;) ## You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2. summarizer = pipeline(&quot;summarization&quot;, model = summary_model, tokenizer = summary_tokenizer) news_article = &#39;&#39;&#39; 지난달 소비자물가가 6.3% 치솟았다. 지난 6월(6.0%)보다 상승폭이 더 커졌다. 최근 글로벌 경기 둔화 가능성이 확대되면서 국제유가가 하락해 국내 석유류 가격 상승세도 약간 둔화하는 모습을 보였지만, 지난달 공공요금 인상으로 인해 전기·가스·수도 가격이 부쩍 올랐다. 통계청이 2일 발표한 ‘7월 소비자물가동향’을 보면, 지난달 소비자물가지수는 108.74(2020년=100)로 1년 전보다 6.3% 올랐다. 지난 6월에 이어 물가상승률이 두 달 연속 6%대를 나타낸 것은 외환위기 중이었던 1998년 10월∼11월 이후 23년 8개월 만이다. 여전히 전체 물가상승을 견인하는 품목은 석유류를 비롯한 공업제품이었다. 전체 6.3% 물가 상승분 가운데 1.59%포인트를 석유류가 차지했다. 다만 최근 국제유가가 내림세를 띠면서 국내 석유류 가격 상승률도 지난 6월 39.6%에서 7월 35.1%로 약간 둔화하는 모습을 나타냈다. &#39;&#39;&#39; hani_news = summarizer(news_article, max_length=100, min_length=30, do_sample=False) library(reticulate) py$hani_news %&gt;% unlist() %&gt;% as.character(.) ## [1] \"통계청이 2일 발표한 ‘7월 소비자물가동향’을 보면, 지난달 소비자물가지수는 108.74(2020년=100)로 1년 전보다 6.3% 올랐는데, 지난 6월에 이어 물가상승률이 두 달 연속 6%대를 나타낸 것은 외환위기 중이었던 1998년 10월∼11월 이후 23년 8개월 만이다.\" "],["텍스트-데이터-탐색.html", "5 . 텍스트 데이터 탐색 5.1 Text Data Explorer 5.2 Text Data Explorer 기능 5.3 R 명령어 실행", " 5 . 텍스트 데이터 탐색 bitTA 패키지는 한국어 텍스트 분석에 특화된 기능을 다수 포함하고 있다. 특히 GUI 인터페이스를 지원하는 shiny와 텍스트 마이닝이 결합할 경우 텍스트에 담긴 숨은 의미를 찾아내는 것은 물론 쉽고 빠르게 결과를 도출하는데 도움이 된다. 데이터 분석을 위해서 데이터를 탐색(EDA, Exploratory Data Analysis)하는 것처럼 텍스트 데이터 분석 역시 데이터의 탐색이 필요합니다. 이 작업을 통해 분석가는 주관적인 판단으로 데이터의 품질을 정성적으로 느끼거나, 데이터를 분석하기 위한 실마리를 찾습니다. 데이터 품질의 인식은 데이터 정제의 패턴 룰을 발견하고, 필터링 룰을 도출하여 분석 대상 문서를 선별하는 작업의 기초가 됩니다. 5.1 Text Data Explorer bitTA에는 buzz라는 데이터를 제공하는데 다음과 같은 변수를 가지고 있습니다. 변수 CONTENT를 대상으로 데이터를 탐색하여 합니다. names(bitTA::buzz) ## [1] \"KEYWORD\" \"SRC\" \"SECTION\" ## [4] \"CRAWL_DT\" \"PUBLISH_DT\" \"URL\" ## [7] \"TITLE\" \"CONTENT\" \"DOC_KEY\" ## [10] \"PUBLISH_ID\" \"CLICK_CNT\" \"LIKE_CNT\" ## [13] \"SEARCH_KEYWORD\" bitTA는 텍스트 데이터를 탐색할 수 있는 Text Data Explorer를 제공합니다. Text Data Explorer는 Shiny 앱으로 explore_docs() 함수로 호출합니다. library(bitTA) data(buzz) explore_docs() 5.2 Text Data Explorer 기능 5.2.1 데이터 구조 파악하기 Text Data Explorer가 실행되면 다음과 같은, 분석할 데이터를 선택하고, 선택한 데이터의 변수 구조를 알수 있는 데이터 탭이 보여집니다. 현재는 데이터 프레임 객체가 buzz 하나라서 자동으로 해당 데이터가 선택되었습니다. 분석할 데이터 프레임 객체를 선택하면, 오른쪽 테이블에서는 데이터 프레임 객체의 변수 이름과 데이터 유형이 출력됩니다. 그리과 좌측에는 텍스트 데이터를 편집했을 경우, 데이터를 저장하는 기능의 위젯이 위치합니다. 5.2.2 데이터 탐색과 정제하기 텍스트 데이터를 탐색할 때 특히 하나의 문서가 아니라 여러 개의 문서들로 구성되었을 때, 문서를 하나씩 패치하여 읽어나가거나 정규표현식을 만족하는 문서만 추려서 하나씩 패치하면서 살펴봅니다. 경우에 따라서는 문서 내에서 오기된 단어들을 수정하기도 합니다. 이러한 작업을 콘솔에서 수행하기에는 여간 성가신 것이 아닙니다. 또한 긴 문장으로 구성된 텍스트는 콘솔에서는 일부만 표현되기도 합니다. Text Data Explorer는 텍스트 데이터를 탐색하고 정제할 수 있는 간단한 기능을 제공합니다. 이 기능은 검색/대체 탭을 사용합니다. 5.2.2.1 탐색을 위한 변수 선택 데이터를 탐색하는 방법을 정의하는 과정입니다. 데이터 프레임 객체에서 대상 변수를 선택하는 과정입니다. 다음과 같은 변수를 선택합니다. 아이디 변수 패치하면서 출력되는 문서를 식별하거나, 보조적 정보를 표현활 변수들을 선택합니다. 문자열 변수 탐색할, 텍스트 분석의 대상이 데는 문서를 담은 변수를 선택합니다. 조건 변수 탐색할 문서들 중에서 특정 조건에 해당하는 하위 집합을 정의할 변수를 선택합니다. 범주형/문자형 데이터가 대상이됩니다. 조건변수값에서 해당 변수에서 탐색한 수준(levels)을 선택합니다. 다음 그림은 buzz의 CONTENTS 변수를 탐색하는데, 그 대상은 KEYWORD가 맞벌이인 데이터를 대상으로 합니다. 화면에는 CONTENTS와 함께 TITLE, SECTION, KEYWORD를 표시할 것입니다. 5.2.2.2 검색 패턴 검색에 검색하려는 정규표현식을 입력한 후 검색 버튼을 누르면 다음처럼 정규표현식을 만족하는 문서들이 출력됩니다. 1000개의 문서들 중에서 19개의 문서에 검색 패턴인 기저귀가 포함되었으며, 그 중 첫번째 문서라는 것을 알려줍니다. 패턴과 매치되는 텍스트는 붉은색으로 표시됩니다. 5.2.2.3 패치 오른쪽 방향 버튼을 누르면 두번째 문서화면으로 이동(패치)됩니다. 반대의 경우도 가능합니다. 5.2.2.4 대체 검색한 패턴을 다른 문자열로 대체할 수도 있습니다. 텍스트 데이터의 정제를 위한 기능입니다. 다음은 전업주부라는 복합명사(compound noun)가 전업 주부로 분리되어 있는 사례를 보정하는 예시입니다. 만약에 대체 기능으로 텍스트 데이터를 수정(정제)하였다면, “데이터 구조 파악하기”에서 소개한 저장 기능을 이용해서 변경된 데이터를 저장해야 합니다. 5.2.3 형태소분석을 이용한 데이터 탐색 형태소분석 탭의 기능은 장문의 본문을 일일이 읽기 어려울 경우에 유용한 기능입니다. 원문과 함께 명사만 추출한 정보를 제공해서 명사를 탐색하면서 문장의 전체 맥락을 속성으로 이해할 수 있습니다. 몇몇 개의 문서를 탐색하는 경우에는 문제가 발생하지 않지만, 하루 종일 수백, 수천개의 문서를 탐색하면서 텍스트 분석의 실마리를 찾아야하는 경우에 천사와 같은 기능을 제공해줄 것입니다. 5.2.4 공동발생분석을 이용한 데이터 탐색 공동발생분석 탭의 기능은 검색 키워드와 공동으로 발현(collocation)하는 명사들을 추출해줍니다. 관심있는 키워드와 함께 이야기하는 주제를 파악하기 쉽습니다. 입력 위젯의 Span은 공동발현하는 명사가 키워드를 중심으로 몇번 째 거리에서 출현하는가를 정의합니다. 즉, 아래 그림의 예제는 기저귀와 2번째 거리 이내의 명사를 리스트업합니다. 5.2.5 n-grams를 이용한 데이터 탐색 N-Gram 탭의 기능은 검색 키워드로 검색된 개별 문서에 대해서 명사를 n-gram으로 토큰화합니다. 이 기능 역시 문서의 주제를 찾는데 유용합니다. 5.3 R 명령어 실행 bitTA 패키지에서 실행한 Shiny 앱은 모달(Modal) 창을 띄웁니다. 이것은 앱이 종료되기 전는에 R 콘솔을 사용할 수 없다는 것을 의미합니다. 그래서 Text Data Explorer는 간단한 R 스크립트를 수행할 수 있는 기능을 구현했습니다. R Command 탭은 간단한 R 스크립트를 수행할 수 있습니다. 다음 예제는 Text Data Explorer를 사용중에 get_spacing()으로 문서의 띄어쓰기를 보정하는 예시입니다. "],["text-to-tibble.html", "6 . 텍스트 → 데이터프레임 6.1 정규표현식 6.2 문자열 기본 작업 6.3 tibble + stringr", " 6 . 텍스트 → 데이터프레임 텍스트 원데이터(Raw data)를 R에서 처리하는 방식은 오래전부터 많은 함수와 도구가 개발되어 현재도 사용가능하다. 하지만 tidyverse 생태계를 구성하고 있는 stringr은 문자열(string)을 다루기 위해서 개발된 전용 패키지로 티블(tibble) 데이터프레임과 결합할 경우 텍스트 전처리 작업에 생산성을 높일 수 있다. stringr 패키지는 stringi 패키지를 기반으로 하여 str_* 방식으로 문자열 처리 함수 API 잘 문서화 되어 되어 한번 기억하면 기억을 되새기며 코딩하기 적합하다. 텍스트 처리 tibble과 stringr 조합 6.1 정규표현식 텍스트 데이터는 신문기사, 책, 블로그, 채팅로그 등 다양한 형태로 나타난다. 이런 텍스트를 다루는데 별도 언어가 필요한데 이것이 정규표현식(regular expression)이다. 강력한 정규표현식을 사용하기 위해서 기본적으로 R에서 문자열을 불러오고 패턴을 매칭하고, 문자열 일부를 떼어내고 등등 다양한 기능을 수행하는 팩키지가 있다. 물론 R Base의 기본 기능함수도 있지만, stringi, stringr, rebus 팩키지를 조합하여 사용하는 것이 생산성을 최대한 끌어올릴 수 있다. 텍스트/문자열 툴체인 stringr: RStudio 개발환경에서 str_ + 탭완성 기능을 조합하여 일관성을 갖고 가장 많이 사용되는 문자열 처리 기능을 함수로 제공하는 팩키지 stringi: 속도 및 확장성, 기능에서 R 환경에서 타의 추종을 불허하는 기본 문자열 처리 R 팩키지 rebus: Regular Expression Builder, Um, Something: 사람이 읽고 사용하기 쉬운 형태로 바꾼 정규식 구현 팩키지 텍스트/문자열 처리작업을 수행할 때 tidyverse 팩키지와 마찬가지로 복잡하고 난잡한 부분을 가능하면 숨기고 나서 가장 많이 활용되는 기능만 뽑아서 가장 생산성 높게 사용하는 툴체인으로 활용가능하다. 정규표현식의 보다 자세한 사항은 데이터 과학 프로그래밍 정규표현식을 참고한다. 6.2 문자열 기본 작업 기본적으로 작업하는 기본 문자열 작업은 문자열을 찾고, 문자열을 쪼개고, 문자열을 치환하는 작업이다. stringr에서 문자열 매칭에 문자열을 탐지하고, 탐지된 문자열을 뽑아내고, 매칭된 문자열 갯수를 찾아내는 기본작업을 지원한다. 이런 기본기 작업에 활용되는 다음과 같다. str_detect() str_subset() str_count() 문자열을 특정 패터에 맞춰 쪼개고 나면 str_split() 함수를 통해 반환되는 객체는 리스트 자료형이 된다. 왜냐하면 고정된 길이를 갖지 않을 수 있기 때문에 반환되는 리스트를 lapply() 함수와 연동하여 길이를 바로 구하는 것도 많이 작어되는 기본 작업 패턴이다. 그리고 str_replace(), str_replace_all() 함수를 활용하여 문자열을 치환하는 것도 많이 사용되는 패턴이다. library(stringr) library(magrittr) # 1. 문자열 매칭하기 ------------------------- hangul &lt;- c(&quot;자동차&quot;, &quot;한국&quot;, &quot;한국산 자동차와 손수레&quot;) str_detect(string = hangul, pattern = &quot;자동차&quot;) ## [1] TRUE FALSE TRUE str_detect(string = hangul, pattern = fixed(&quot;자동차&quot;)) ## [1] TRUE FALSE TRUE str_subset(string = hangul, pattern = fixed(&quot;자동차&quot;)) ## [1] \"자동차\" \"한국산 자동차와 손수레\" str_count(string = hangul, pattern = fixed(&quot;자동차&quot;)) ## [1] 1 0 1 # 2. 문자열 쪼개기 ------------------------- hangul &lt;- c(&quot;한국산 자동차와 손수레 그리고 오토바이&quot;) str_split(hangul, pattern = &quot; 그리고 |와&quot;) ## [[1]] ## [1] \"한국산 자동차\" \" 손수레\" \"오토바이\" str_split(hangul, pattern = &quot; 그리고 |와&quot;, n=2) ## [[1]] ## [1] \"한국산 자동차\" \" 손수레 그리고 오토바이\" hanguls &lt;- c(&quot;한국산 자동차와 손수레 그리고 오토바이&quot;, &quot;독일산 자동차 그리고 아우토반&quot;) str_split(hanguls, pattern = &quot; 그리고 |와&quot;, simplify = TRUE) ## [,1] [,2] [,3] ## [1,] \"한국산 자동차\" \" 손수레\" \"오토바이\" ## [2,] \"독일산 자동차\" \"아우토반\" \"\" hanguls_split &lt;- str_split(hanguls, pattern = &quot; 그리고 |와&quot;) purrr::map_int(hanguls_split, length) ## [1] 3 2 # 3. 매칭된 문자열 치환 -------------------- str_replace_all(hanguls, pattern = &quot;와&quot;, replacement = &quot; 그리고&quot;) ## [1] \"한국산 자동차 그리고 손수레 그리고 오토바이\" ## [2] \"독일산 자동차 그리고 아우토반\" 6.3 tibble + stringr 짧은 텍스트는 문자열과 stringr 정규표현식을 조합하여 처리해도 되지만, 데이터프레임 dplyr 기본기를 익힌 경우 텍스트 문자열을 데이터프레임으로 변환시킨 후에 이를 stringr 함수와 결합하여 텍스트를 처리하게 되면 좀더 구조화되고 가독성 높은 코드를 작성할 수 있게 된다. 이상의 오감도 텍스트를 가져와서 텍스트 분석을 시작할 경우 파일에 읽어와도 되지만, 문자열로 읽어 ogamdo_txt 변수에 저장해 두고 이를 몇단계를 거쳐 데이터프레임으로 변환시킨다. library(dplyr) ogamdo_txt &lt;- &quot;13인의 아해가 도로로 질주하오. (길은 막다른 골목이 적당하오.) 제1의 아해가 무섭다고 그리오. 제2의 아해도 무섭다고 그리오. 제3의 아해도 무섭다고 그리오. 제4의 아해도 무섭다고 그리오. 제5의 아해도 무섭다고 그리오. 제6의 아해도 무섭다고 그리오. 제7의 아해도 무섭다고 그리오. 제8의 아해도 무섭다고 그리오. 제9의 아해도 무섭다고 그리오. 제10의 아해도 무섭다고 그리오. 제11의 아해가 무섭다고 그리오. 제12의 아해도 무섭다고 그리오. 제13의 아해도 무섭다고 그리오. 13인의 아해는 무서운 아해와 무서워하는 아해와 그렇게뿐이 모였소.(다른 사정은 없는 것이 차라리 나았소) 그중에 1인의 아해가 무서운 아해라도 좋소. 그중에 2인의 아해가 무서운 아해라도 좋소. 그중에 2인의 아해가 무서워하는 아해라도 좋소. 그중에 1인의 아해가 무서워하는 아해라도 좋소. (길은 뚫린 골목이라도 적당하오.) 13인의 아해가 도로로 질주하지 아니하여도 좋소.&quot; ogamdo_tbl &lt;- ogamdo_txt %&gt;% str_split(pattern = &quot;\\n&quot;) %&gt;% # 문장단위로 행을 구분 tibble::enframe(value = &quot;text&quot;) %&gt;% # 데이터프레임 변환 tidyr::unnest(text) %&gt;% # 행기준으로 펼치기 mutate(행 = row_number()) %&gt;% # 행번호 붙이기 select(행, text) ogamdo_tbl ## # A tibble: 25 × 2 ## 행 text ## &lt;int&gt; &lt;chr&gt; ## 1 1 \"13인의 아해가 도로로 질주하오.\" ## 2 2 \"(길은 막다른 골목이 적당하오.)\" ## 3 3 \"\" ## 4 4 \"제1의 아해가 무섭다고 그리오.\" ## 5 5 \"제2의 아해도 무섭다고 그리오.\" ## 6 6 \"제3의 아해도 무섭다고 그리오.\" ## # … with 19 more rows 이상 오감도는 총 25 줄로 구성되어 있는데 숫자가 들어간 행이 있고 그렇지 않은 행이 있는데 몇 문장에 숫자가 포함되어 있는지 str_detect() 함수와 숫자패턴을 탐지하는 정규표현식 \\\\d 를 조합하면 부울 참/거짓으로 변수를 생성시킬 수 있고 이를 count() 개수 함수를 사용하면 빈도를 쉽게 구할 수 있다. ogamdo_tbl %&gt;% mutate(숫자여부 = str_detect(text, &quot;\\\\d&quot;)) %&gt;% count(숫자여부) ## # A tibble: 2 × 2 ## 숫자여부 n ## &lt;lgl&gt; &lt;int&gt; ## 1 FALSE 5 ## 2 TRUE 20 str_squish() 함수를 통해 연속된 공백을 제거하고 하나의 공백으로 처리할 수 있기 때문에 텍스트가 깔끔하지 못한 경우 큰 도움이 되는 함수로 꼭 기억해두자. 만약 면자열 시작과 끝에 공백이 있는 경우 str_trim()을 사용하면 큰 힘 들이지 않고 모든 문자열에 대해 공백을 제거할 수 있다. 전체적인 텍스트 정제작업이 마무리된 경우 dplyr::pull() 함수를 사용해서 해당 변수의 모든 텍스트를 추출하고 str_c() 함수로 각 행별로 구분되었던 문자열을 collapse = \" \" 인자를 넣어 하나의 텍스트로 묶어 작업을 마무리 한다. ogamdo_tbl %&gt;% mutate(text = str_squish(text) %&gt;% str_trim(.)) %&gt;% pull(text) %&gt;% str_c(., collapse = &quot; &quot;) ## [1] \"13인의 아해가 도로로 질주하오. (길은 막다른 골목이 적당하오.) 제1의 아해가 무섭다고 그리오. 제2의 아해도 무섭다고 그리오. 제3의 아해도 무섭다고 그리오. 제4의 아해도 무섭다고 그리오. 제5의 아해도 무섭다고 그리오. 제6의 아해도 무섭다고 그리오. 제7의 아해도 무섭다고 그리오. 제8의 아해도 무섭다고 그리오. 제9의 아해도 무섭다고 그리오. 제10의 아해도 무섭다고 그리오. 제11의 아해가 무섭다고 그리오. 제12의 아해도 무섭다고 그리오. 제13의 아해도 무섭다고 그리오. 13인의 아해는 무서운 아해와 무서워하는 아해와 그렇게뿐이 모였소.(다른 사정은 없는 것이 차라리 나았소) 그중에 1인의 아해가 무서운 아해라도 좋소. 그중에 2인의 아해가 무서운 아해라도 좋소. 그중에 2인의 아해가 무서워하는 아해라도 좋소. 그중에 1인의 아해가 무서워하는 아해라도 좋소. (길은 뚫린 골목이라도 적당하오.) 13인의 아해가 도로로 질주하지 아니하여도 좋소.\" 아직 한글화가 되어 있지 않지만, stringr 패키지에 담긴 str_*로 시작하는 다양한 함수에 대한 사항은 String manipulation with stringr cheatsheet를 참고한다. "],["text-cleansing.html", "7 . 정제 (전처리) 7.1 토큰화 7.2 불용어 제거 7.3 정규화 7.4 연습 7.5 어간 추출 7.6 표제어 추출 7.7 연습 7.8 과제", " 7 . 정제 (전처리) 정제는 자료정보지식지혜(DIKW, Data/Information/Knowledge, Wisdom)위계론의 1차부호화 단계에 해당한다. 정제를 거친 자료를 분석하는 2차부호화 과정을 거쳐 자료가 정보로 가공된다. 광물 정제과정에 비유할 수 있다. 금광석 등 광물을 캐면 먼저 잘게 분쇄한다. 불순물을 제거하고, 규격화한 금괴로 가공한다. 마찬가지로 원자료를 분석할 수 있는 단위로 분쇄(토큰화)하고, 불순물을 제거(불용어 제거)한 다음, 규격화한 양식으로 정규화한다. 토큰화 불용어제거 정규화 pkg_l &lt;- c(&quot;tidyverse&quot;, &quot;tidytext&quot;, &quot;textdata&quot;) purrr::map(pkg_l, require, ch = TRUE) ## [[1]] ## [1] TRUE ## ## [[2]] ## [1] TRUE ## ## [[3]] ## [1] TRUE 7.1 토큰화 텍스트 원자료를 분석할 수 있도록 토큰(token)으로 잘게 나누는 단계다. 토큰의 단위는 분석 목적에 따라 글자, 단어, 엔그램(n-gram), 문장, 문단 등 다양하게 지정할 수 있다. tidytext 패키지 unnest_tokens() 함수에 인자를 달리하여 텍스트를 다양한 형태로 분절하여 후속작업을 할 수 있다. unnest_tokens(df, output, input, token = &quot;words&quot;, format = c(&quot;text&quot;, &quot;man&quot;, &quot;latex&quot;, &quot;html&quot;, &quot;xml&quot;), to_lower = TRUE, drop = TRUE, collapse = NULL, ...) 토큰으로 나누는 단위는 분석의 목적에 따라 다양한 단어, 글자, 문장 등 다양한 수준으로 설정할 수 있다. “characters” 글자 단위 “character_shingles” 복수의 글자 단위 “words” 단어 단위 “ngrams” 복수의 단어 단위 “regex” 정규표현식으로 지정 tidytext패키지에서는 unnest_tokens()함수에서는 token =인자로 토큰 단위를 지정할 수 있다. 7.1.1 단어 unnest_tokens()함수에서 토큰의 기본값으로 설정된 단위는 단어(“words”)다. text_v &lt;- &quot;You still fascinate and inspire me. You influence me for the better. You’re the object of my desire, the #1 Earthly reason for my existence.&quot; tibble(text = text_v) %&gt;% unnest_tokens(output = word, input = text, token = &quot;words&quot;) ## # A tibble: 25 × 1 ## word ## &lt;chr&gt; ## 1 you ## 2 still ## 3 fascinate ## 4 and ## 5 inspire ## 6 me ## # … with 19 more rows 7.1.2 글자 토큰 token =인자에 “characters”를 투입하면 글자 단위로 토큰화한다. tibble(text = text_v) %&gt;% unnest_tokens(output = word, input = text, token = &quot;characters&quot;) %&gt;% count(word, sort = TRUE) ## # A tibble: 21 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 e 20 ## 2 t 10 ## 3 o 8 ## 4 r 8 ## 5 i 7 ## 6 n 7 ## # … with 15 more rows 7.1.3 복수의 글자 복수의 글자를 토큰의 단위로 할 때는 “character_shingles”을 token =인자에 투입한다. 기본값은 3글자. tibble(text = text_v) %&gt;% unnest_tokens(output = word, input = text, token = &quot;character_shingles&quot;, n = 4) %&gt;% count(word, sort = TRUE) ## # A tibble: 104 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 ence 2 ## 2 ethe 2 ## 3 reth 2 ## 4 1ear 1 ## 5 andi 1 ## 6 arth 1 ## # … with 98 more rows 7.1.4 복수의 단어(n-gram) 복수의 단어를 토콘 단위로 나눌 때는 token =인자에 “ngrams”인자를 투입한다. 기본값은3개이다. tibble(text = text_v) %&gt;% unnest_tokens(output = word, input = text, token = &quot;ngrams&quot;, n = 4) %&gt;% count(word, sort = TRUE) ## # A tibble: 22 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 1 earthly reason for 1 ## 2 and inspire me you 1 ## 3 better you’re the object 1 ## 4 desire the 1 earthly 1 ## 5 earthly reason for my 1 ## 6 fascinate and inspire me 1 ## # … with 16 more rows 7.1.5 정규표현식 정규표현식(regex: regular expressions)을 이용하면, 토콘을 보다 다양한 방식으로 나눌 수 있다. token =인자에 “regex”를 지정한다. pattern =에 정규표현식을 투입한다. 정규표현식에서 “new line”을 의미하는 \"\\n\"를 이용해 토큰화할 경우 문장 단위로 토큰화할 경우, 수 있다. 만일 공백 단위로 토큰화한다면, 공백을 의미하는 \"\\\\s\"를 투입한다. tibble(text = text_v) %&gt;% unnest_tokens(output = word, input = text, token = &quot;regex&quot;, pattern = &quot;\\n&quot;) ## # A tibble: 3 × 1 ## word ## &lt;chr&gt; ## 1 \"you still fascinate and inspire me.\" ## 2 \"you influence me for the better. \" ## 3 \"you’re the object of my desire, the #1 earthly reason f… 7.1.6 문서형식 format =인자를 format = c(\"text\", \"man\", \"latex\", \"html\", \"xml\") 다양하게 적용해 토큰화하는 문서의 형식을 지정할 수 있다. “html”문서를 토큰화해보자. pp_html &lt;- read_lines(&quot;https://www.gutenberg.org/files/1342/1342-h/1342-h.htm&quot;) pp_html_df &lt;- tibble(text = pp_html) pp_html_df %&gt;% slice(1:5) ## # A tibble: 5 × 1 ## text ## &lt;chr&gt; ## 1 \"&lt;!DOCTYPE html PUBLIC \\\"-//W3C//DTD XHTML 1.0 Strict//EN… ## 2 \"\\\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\\\"&gt;\" ## 3 \"&lt;html xmlns=\\\"http://www.w3.org/1999/xhtml\\\" xml:lang=\\\"… ## 4 \"&lt;head&gt;\" ## 5 \"&lt;meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html;ch… pp_html_df %&gt;% unnest_tokens(word, text, format = &quot;html&quot;) %&gt;% slice(1:5) ## # A tibble: 5 × 1 ## word ## &lt;chr&gt; ## 1 the ## 2 project ## 3 gutenberg ## 4 ebook ## 5 of 7.1.7 영어 소문자 영문은 대문자와 소문자 구분이 있다. to_lower =인자의 기본값은 TRUE다. 즉, to_lower = TRUE 이 기본설정이다. FALSE를 로 지정하면 대문자를 모두 소문자로 변경하지 않는다. 영문문서에서 사람이름이나 지명을 구분해야 한다면 토큰화 과정에서 모든 단어를 소문자화하지 말아야 한다. tibble(text = text_v) %&gt;% unnest_tokens(output = word, input = text, to_lower = FALSE) ## # A tibble: 25 × 1 ## word ## &lt;chr&gt; ## 1 You ## 2 still ## 3 fascinate ## 4 and ## 5 inspire ## 6 me ## # … with 19 more rows 7.1.8 문장부호 추가인자는 tokenizers함수로 전달해 다양한 설정을 할 수 있다. 예를 들어, strip_punct =인자에 FALSE를 투입하면, 문장부호를 제거하지 않는다. tibble(text = text_v) %&gt;% unnest_tokens(output = word, input = text, token = &quot;words&quot;, strip_punct = FALSE) ## # A tibble: 30 × 1 ## word ## &lt;chr&gt; ## 1 you ## 2 still ## 3 fascinate ## 4 and ## 5 inspire ## 6 me ## # … with 24 more rows 7.2 불용어 제거 불용어(stop words)는 말 그대로 사용하지 않는 단어다. 불용어를 문자 그대로 해석하면 사용하지 않는 단어에 국한된다. 넓은 의미로 해석하면, 사용빈도가 높아 분석에 의미가 없거나, 내용을 나타내는데 기여하지 않는 단어, 숫자, 특수문자, 구두점, 공백문자, 기호 등이 포함된다. 무엇이 불용어가 돼야 하는지는 상황에 따라 다르다. 에를 들어, 대명사는 대부분의 불용어사전에 불용어로 포함돼 있지만, 분석 목적에 따라서는 대명사는 분석의 핵심단위가 되기도 한다. 기호도 마찬가지다. 기호를 이용한 이모티콘은 문서의 의미를 전달하기 때문에 모든 기호를 일괄적으로 제거해서는 안된다. 앞서 제시한 연애편지를 문자 단위로 토큰화해 단어의 빈도를 계산해보자. tibble(text = text_v) %&gt;% unnest_tokens(output = word, input = text) %&gt;% count(word, sort = TRUE) ## # A tibble: 19 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 the 3 ## 2 for 2 ## 3 me 2 ## 4 my 2 ## 5 you 2 ## 6 1 1 ## # … with 13 more rows count로 단어빈도를 계산한 결과를 보면 “the”가 3회, “for”, “me”, “my”, “you”가 각각 2회 사용됐다. 즉, 이 글은 너와 나에 대한 글이런 것을 알수 있다. 사랑고백이란 것이 너와 나의 일이므로 타당하다. 분석결과를 보면 단어빈도로 의미를 파악하는데 불필요한 단어도 있다. “the”, “for”, “of”, “and” 등과 같은 관사, 전치사, 접속사들처럼 자주 사용하는 단어들이다. 이런 단어는 불용어(stop words)로 처리해 분석대상에 제외하는 것이 보다 정확한 의미를 파악하는데 도움이 되는 경우도 있다. 불용어를 제거하는 방법은 크게 두가지가 있는데 혼용한다. anti_join() 불용어목록을 데이터프레임에 저장한 다음, anti_join()함수를 이용해 텍스트데이터프레임과 배제결합하는 방법이다. 두 데이터프레임에서 겹치는 행을 제외하고 결합(join)한다. 이 경우 불용어 목록에 포함된 행이 제외된다. filter()함수와 str_detect()함수를 함께 이용해 불용어 지정해 걸러내는 방법이다. 불용어사전에 포함돼 있지 않는 단어를 제거할 때 이용한다. 7.2.1 불용어 사전 주로 사용되는 불용어목록은 불용어사전으로 제공된다. tidytext패키지는 stop_words에 불용어를 모아 놓았다. stop_words의 구조부터 살며보자. kableExtra패키지를 이용하면 데이터프레임을 깔끔하게 출력할 수 있다.(사용법은 여기) install.packages(&quot;kableExtra&quot;) 데이터셋을 R세션에 올리는 함수는 data()함수다. library(kableExtra) data(stop_words) stop_words %&gt;% glimpse() ## Rows: 1,149 ## Columns: 2 ## $ word &lt;chr&gt; \"a\", \"a's\", \"able\", \"about\", \"above\", \"acc… ## $ lexicon &lt;chr&gt; \"SMART\", \"SMART\", \"SMART\", \"SMART\", \"SMART… stop_words %&gt;% slice( c(1:3, 701:703, 1001:1003) ) %&gt;% kbl() %&gt;% kable_classic(full_width = FALSE) word lexicon a SMART a’s SMART able SMART during snowball before snowball after snowball parted onix parting onix parts onix stop_words는 행이 1,149개(불용어 1,149개)이고, 열이 2개(word와 lexicon)인 데이터프레임이다. word열에 있는 단어가 불용어고, lexicon열에 있는 값은 불용어 용어집의 이름이다. tidytext패키지의 stop_words에는 세 개의 불용어 용어집(SMART, snowball, onix) 이 포함돼 있다. filter함수로 특정 용어집에 있는 불용어 사전만 골라 이용할 수 있다. stop_words %&gt;% select(lexicon) %&gt;% unique() ## # A tibble: 3 × 1 ## lexicon ## &lt;chr&gt; ## 1 SMART ## 2 snowball ## 3 onix 불용어사전으로 불용어를 걸러낸 다음 단어빈도를 계산해보자. data(stop_words) tibble(text = text_v) %&gt;% unnest_tokens(output = word, input = text) %&gt;% anti_join(stop_words) %&gt;% count(word, sort = TRUE) ## # A tibble: 10 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 1 1 ## 2 desire 1 ## 3 earthly 1 ## 4 existence 1 ## 5 fascinate 1 ## 6 influence 1 ## # … with 4 more rows 결과를 보면 “you”등 대명사가 포함된 토큰은 모두 제거됐는데, “you’re”는 그대로 남아 있다. 불용어 사전에는 “you’re”로 홑따옴표(quotation mark)'를 이용했는데, 본문에는 “you’re”로 홑낫표(aphostrophe)’를 이용했기 때문이다. 불용어사전으로 본문의 “you’re”를 제거하기 위해서는 둘 중 한가지는 해야 한다. 본몬의 홑낫표를 홑따옴표로 변경하거나, 불용어사전을 수정한다. 7.2.2 본문 수정 먼저 본문 수정을 해보자. tapo_v &lt;- text_v %&gt;% str_replace_all(&quot;’&quot;, &quot;&#39;&quot;) tibble(text = tapo_v) %&gt;% unnest_tokens(output = word, input = text) %&gt;% anti_join(stop_words) %&gt;% count(word, sort = TRUE) ## # A tibble: 9 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 1 1 ## 2 desire 1 ## 3 earthly 1 ## 4 existence 1 ## 5 fascinate 1 ## 6 influence 1 ## # … with 3 more rows 7.2.3 불용어 사전 수정 불용어 사전에 “you’re”를 추가해보자. 데이터프레임을 만들어 bind_rows()로 데이터프레임을 결합할수도 있고, add_row()로 불용사전에 열을 곧바로 추가할수도 있다. 먼저 add_row()로 행에 곧바로 추가하는 방법을 이용해보자. 추가됐는지 확인이 수월하도록 첫째행 전에 추가하자. stop_words %&gt;% add_row(word = &quot;you’re&quot;, lexicon = &quot;NEW&quot;, .before = 1) %&gt;% head(3) ## # A tibble: 3 × 2 ## word lexicon ## &lt;chr&gt; &lt;chr&gt; ## 1 you’re NEW ## 2 a SMART ## 3 a's SMART 이번에는 데이터프레임을 결합해보자. 또한 숫자 “1”도 함께 불용어사전에 추가하자. 먼저 추가할 용어를 불용어사전과 같은 구조의 데이터프레임에 저장한다. names(stop_words) ## [1] \"word\" \"lexicon\" stop_add &lt;- tibble(word = c(&quot;you’re&quot;, &quot;1&quot;), lexicon = &quot;added&quot;) stop_add ## # A tibble: 2 × 2 ## word lexicon ## &lt;chr&gt; &lt;chr&gt; ## 1 you’re added ## 2 1 added bind_rows()함수로 불용어사전과 결합한다. stop_words2 &lt;- bind_rows(stop_words, stop_add) stop_words2 %&gt;% tail() ## # A tibble: 6 × 2 ## word lexicon ## &lt;chr&gt; &lt;chr&gt; ## 1 younger onix ## 2 youngest onix ## 3 your onix ## 4 yours onix ## 5 you’re added ## 6 1 added 새로 만든 불용어사전으로 정체한 후 단어 빈도를 계산해보자. tibble(text = text_v) %&gt;% unnest_tokens(output = word, input = text) %&gt;% anti_join(stop_words2) %&gt;% count(word, sort = TRUE) ## # A tibble: 8 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 desire 1 ## 2 earthly 1 ## 3 existence 1 ## 4 fascinate 1 ## 5 influence 1 ## 6 inspire 1 ## # … with 2 more rows “you’re”와 숫자가 모두 제거됐다. 7.2.4 고급 불용어 사전 수정 통상적으로 쓰이는 불용어 중에는 실은 문서의 의미를 파악하는데 중요한 단서를 제공하는 단어도 있다. “you” “me” “my” 등과 같은 대명사는 흔하게 사용되기 때문에 불용어로 분류되지만, 맥락를 파악하는데 중요한 역할을 하기도 한다. 불용어 사전에서 대명사를 찾아 불용어 사전에서 제거하자. stop_words %&gt;% filter( str_detect(word, &quot;(^i$|^i[:punct:]+|^mys*|^me$|mine)&quot;)) %&gt;% pull(word) ## [1] \"i\" \"i'd\" \"i'll\" \"i'm\" \"i've\" \"me\" ## [7] \"my\" \"myself\" \"i\" \"me\" \"my\" \"myself\" ## [13] \"i'm\" \"i've\" \"i'd\" \"i'll\" \"i\" \"me\" ## [19] \"my\" \"myself\" stop_words_pronoun &lt;- stop_words %&gt;% filter( !str_detect(word, &quot;(^i$|^i[:punct:]+|^mys*|^me$|^mine$)&quot;) ) stop_words_pronoun %&gt;% filter( str_detect(word, &quot;^i&quot;)) %&gt;% pull(word) ## [1] \"ie\" \"if\" \"ignored\" \"immediate\" ## [5] \"in\" \"inasmuch\" \"inc\" \"indeed\" ## [9] \"indicate\" \"indicated\" \"indicates\" \"inner\" ## [13] \"insofar\" \"instead\" \"into\" \"inward\" ## [17] \"is\" \"isn't\" \"it\" \"it'd\" ## [21] \"it'll\" \"it's\" \"its\" \"itself\" ## [25] \"it\" \"its\" \"itself\" \"is\" ## [29] \"it's\" \"isn't\" \"if\" \"into\" ## [33] \"in\" \"if\" \"important\" \"in\" ## [37] \"interest\" \"interested\" \"interesting\" \"interests\" ## [41] \"into\" \"is\" \"it\" \"its\" ## [45] \"itself\" 7.2.5 불용어 목록 만들기 제거하고 싶은 불용어를 최소화하고 싶을 때는 불용어 목록을 직접 만들수도 있다. “the, for, and”등이 포함된 불용어 목록을 만들어 정제해 보자. “the, for, and”등 불용어목록을 데이터프레임에 저장한 다음, anti_join()함수를 이용해 토큰데이터프레임과 배제결합한다. stop_df &lt;- tibble(word = c(&quot;the&quot;,&quot;for&quot;, &quot;and&quot;)) tibble(text = text_v) %&gt;% unnest_tokens(output = word, input = text) %&gt;% anti_join(stop_df) %&gt;% filter(!str_detect(word, &quot;\\\\d+&quot;)) %&gt;% count(word, sort = TRUE) ## # A tibble: 15 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 me 2 ## 2 my 2 ## 3 you 2 ## 4 better 1 ## 5 desire 1 ## 6 earthly 1 ## # … with 9 more rows 7.2.6 dplyr 동사로 제거 dplyr 패키지 filter()함수를 이용해 불용어사전을 수정하지 않고 불용어를 추가로 제거할 수 있다. 예를 들어, 숫자를 불용어로 취급해 제거하는 상황에서 숫자를 불용어 사전에 넣지 말고 filter()로 걸러보자. 정규표현식(regex: regular expression)에서 숫자를 의미하는 [:digit:] 또는 \\\\d를 이용해 filter()함수와 str_detect()함수 및 부정연산자 !를 이용해 걸러낸다. filter()함수를 str_detect()함수와 함께 이용하는 이유는 다음과 같다. str_subset()함수는 패턴이 일치하는 문자를 출력하는 반면, str_detect()함수는 패턴이 일치하는 문자에 대한 논리값(TRUE or FALSE)을 출력한다. df &lt;- tibble(text = text_v) %&gt;% unnest_tokens(output = word, input = text) %&gt;% anti_join(stop_words) df$word %&gt;% str_subset(pattern = &quot;\\\\d+&quot;) ## [1] \"1\" df$word %&gt;% str_detect(pattern = &quot;\\\\d+&quot;) ## [1] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE ## [10] FALSE 불용어를 제거한 다음 추가로 본문에서 숫자와 홑낫표”’“가 포함된 문제를 제거하자. tibble(text = text_v) %&gt;% unnest_tokens(output = word, input = text) %&gt;% anti_join(stop_words) %&gt;% filter( !str_detect(word, pattern = &quot;\\\\d+&quot;), !str_detect(word, pattern = &quot;you’re&quot;) ) %&gt;% count(word, sort = TRUE) ## # A tibble: 8 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 desire 1 ## 2 earthly 1 ## 3 existence 1 ## 4 fascinate 1 ## 5 influence 1 ## 6 inspire 1 ## # … with 2 more rows 7.3 정규화 정규화(Normalization)는 추출한 단어를 일정한 틀로 규격화하는 작업이다. 한 단어는 문법적인 기능에 따라 다양한 표현이 있다. ‘먹다’에는 ’먹었니’ ‘먹었다’ ‘먹고’ 등의 표현이 있다. ‘I’는 격에 따라 ’my’ ‘me’ ‘mine’ 등의 변형이 있다. 다양한 표현이 같은 의미를 나타낸다면 정규화를 통해 일정한 틀로 규격화해야 한다. 정규화는 형태소(morpheme) 추출, 어간(stem) 추출, 표제어(lemme) 추출 등을 이용해 달성할 수 있다. 단어(word): 형태소의 집합. 자립이 가능한 최소 형태(예: 사과나무) 형태소(morpheme): 뜻을 지닌 가장 작은 말의 단위. 예를 들어, ’사과나무’는 ’사과’와 ’나무’로 나눠도 뜻을 지니지만, ’사과’를 ’사’와 ’과’로 나누면 의미가 사라진다. 어기(base): 어근과 어간 등 단어에서 실질적인 의미를 나타내는 형태소 어근(root) 어미와 직접결합이 안되는 어기. 예: ‘급하다’의 ’급’ ‘시원하다’의 ’시원’. 어간(stem) 어미와 직접결합이 되는 어기. 예: ‘뛰어라’의 ’뛰-’. ‘먹다’의 ’먹-’. 표제어(lemme) 사전에 등재된 대표단어. 원형 혹은 기본형(canonical form)이라고도 한다. 7.3.1 형태소 형태소(morpheme)는 뜻을 지난 가장 작은 말의 단위다. ‘바지가 크다’는 문장에서 단어는 ’바지’ ‘가’ ‘크다’가 있다. 형태소는 ’바지’ ‘가’ ‘크’ ‘다’로 구분할 수 있다. 명사인 ’바지’를 ’바’와 ’지’로 나누면 ’아랫도리에 입는 옷’이란 의미가 사라진다. 반면 형용사인 ’크다’에는 어간인 ’크-’에 ’크다’는 의미가 담겨 있고,’-다’에는 문장을 마무리하는 의미가 담겨 있다. 7.3.2 품사태깅 형태소를 추출하기 위해서 문장의 단어에 품사를 붙인다(tag). 이를 영어로 Parts of Speech Tagging, 줄여서 PoS Tagging이라고 부른다. 형태소 분석기마다 품사태깅의 방법이 조금씩 다르다. 한나눔(Hannaum)은 크게 9개 품사로 분류한뒤 22개로 세부 분류했다. Mecab-ko는 43개로 분류했다. 한국어 품사 태그 비교표 한나눔과 MeCab-ko의 품사태그 (Table ??). Table 7.1: 한나눔과 MeCab-ko의 한국어 품사 태그 비교 Hannanum09 (ntags=9) Hannanum22 (ntags=22) Mecab-ko (ntags=43) Tag Description Tag Description Tag Description N 체언 NC 보통명사 NNG 일반 명사 NQ 고유명사 NNP 고유 명사 NB 의존명사 NNB 의존 명사 NNBC 단위를 나타내는 명사 NN 수사 NR 수사 NP 대명사 NP 대명사 P 용언 PV 동사 VV 동사 PA 형용사 VA 형용사 PX 보조 용언 VX 보조 용언 VCP 긍정 지정사 VCN 부정 지정사 M 수식언 MM 관형사 MM 관형사 MA 부사 MAG 일반 부사 MAJ 접속 부사 I 독립언 II 감탄사 IC 감탄사 J 관계언 JC 격조사 JKS 주격 조사 JKC 보격 조사 JKG 관형격 조사 JKO 목적격 조사 JKB 부사격 조사 JKV 호격 조사 JKQ 인용격 조사 JC 접속 조사 JX 보조사 JX 보조사 JP 서술격 조사 E 어미 EP 선어말어미 EP 선어말어미 EF 종결 어미 EF 종결 어미 EC 연결 어미 EC 연결 어미 ET 전성 어미 ETN 명사형 전성 어미 ETM 관형형 전성 어미 X 접사 XP 접두사 XPN 체언 접두사 XS 접미사 XSN 명사파생 접미사 XSV 동사 파생 접미사 XSA 형용사 파생 접미사 XR 어근 S 기호 S 기호 SF 마침표, 물음표, 느낌표 SE 줄임표 … SSO 여는 괄호 (, [ ] SC 구분자 , · / : SY 기타 기호 SH 한자 SL 외국어 SN 숫자 F 외국어 F 외국어 7.3.3 형태소 추출 정규화는 형태소를 추출해 달성할 수 있다. R의 대표적인 형태소 분석기로는 RcppMeCab와 KoNLP가 있다. 7.3.3.1 RcppMeCab 한국어뿐 아니라 일본어와 중국어 형태소 분석도 가능하고 실행속도가 빠르다. 일본교토대학정보학연구대학원과 일본전신전화의 커뮤니케이션기본과학연구소가 공동으로 개발한 오픈소스 형태소 분석기 MeCab 기반이다. 은전 프로젝트로 한국어 형태소를 분석할수 있게 개발했다. RcppMeCab패키지는 MeCab을 R에서 사용할 수 있도록 한 패키지다. 일본어 기반이라 띄어쓰기에 덜 민감하다. 7.3.4 RcppMeCab RcppMeCab패키지를 설치하고 실행하자. 설치되는 기본폴더는 C:\\mecab다. 설치폴더를 변경하지 않는다. install.packages(&#39;RcppMeCab&#39;) 주의 C드라이브에 C:\\mecab폴더가 생성됐는지 확인한다. 사전파일이 이곳에 있다. 만일 C:\\mecab폴더가 생성되지 않았다면 설치가 안된것이다. RStudio를 관리자권한으로 실행해 설치한다. 만일, RStudio를 관리자권한으로 실행해 설치해도 C:\\mecab가 생성되지 않는경우, 이 링크에서 사전파일을 다운로드 받아 파일을 압축해제해 복사한다. C:\\mecab가 생성되고, 이 폴더 바로 아래에 libmecab.dll파일과 mecab-ko-dic폴더가 생성돼 있어야 한다. 기본함수는 pos()다. 문자벡터를 받아 리스트로 산출한다. library(RcppMeCab) test &lt;- &quot;한글 테스트 입니다.&quot; pos(test) ## $`한글 테스트 입니다.` ## [1] \"한글/NNG\" \"테스트/NNG\" \"입니다/VCP+EF\" ## [4] \"./SF\" 한글이 깨지는 경우가 있는데, 이는 한글인코딩 방식이 맞지 않기 때문이다. 윈도는 EUC-KR방식을 확장한 CP949방식을 사용하기 때문에 UTF-8방식과 호환이 안된다. 이 경우 enc2utf8함수를 이용해 한글인코딩 방식을 UTF-8으로 변경한다. library(tidyverse) test_v &lt;- enc2utf8(test) test_v %&gt;% pos ## $`한글 테스트 입니다.` ## [1] \"한글/NNG\" \"테스트/NNG\" \"입니다/VCP+EF\" ## [4] \"./SF\" 참고: UTF-8을 CP949로 인코딩을 바꾸고 싶으면 iconv함수를 이용한다. 꼭 필요한 경우 UTF-8을 CP949로 인코딩 변경한다. 이는 마치 디지털 정보를 다시 아날로그 정보로 변환시키는 팩스와 같은 작업이라 꼭 필요한 경우 아니면 진행하지 않길 권장한다. iconv(x, from = &quot;UTF-8&quot;, to = &quot;CP949&quot;)` x는 문자벡터. 자세한 사용법은 ?iconv 참조. 7.3.5 join = FALSE join = FALSE인자를 이용하면 품사태그를 제외하고 형태소만 산출한다. test_v %&gt;% pos(join = FALSE) ## $`한글 테스트 입니다.` ## NNG NNG VCP+EF SF ## \"한글\" \"테스트\" \"입니다\" \".\" 7.3.6 format = \"data.frame\" format = \"data.frame\"을 지정하면 데이터프레임으로 산출한다. test_v %&gt;% pos(format = &quot;data.frame&quot;) ## doc_id sentence_id token_id token pos subtype ## 1 1 1 1 한글 NNG ## 2 1 1 2 테스트 NNG ## 3 1 1 3 입니다 VCP+EF ## 4 1 1 4 . SF 7.3.7 posParallel(x) posParallel()함수는 메모리를 많이 사용하지만 병렬처리를 통해 처리속도가 빠르다. test_v %&gt;% posParallel(format = &quot;data.frame&quot;) ## doc_id sentence_id token_id token pos subtype ## 1 1 1 1 한글 NNG ## 2 1 1 2 테스트 NNG ## 3 1 1 3 입니다 VCP+EF ## 4 1 1 4 . SF 7.3.8 KoNLPy 형태소 분석기로는 한나눔(Hannanum)과 MeCab외 꼬꼬마(Kkma), 코모란(Komoran), Okt 등의 형태소는 파이썬 패키지인 KoNLPy로 추출가능하다. 구글 colab에서 파이썬을 구동하면 KoNLPy패키지를 설치해 다양한 패키지로 형태소를 분석할 수 있다. 7.4 연습 이상의 오감도를 RcppMeCab을 이용해 각각 형태소분석해 자주 사용된 단어의 빈도를 비교하자. 이 결과를 형태소를 추출하지 않은 결과와도 비교하자. ogamdo_txt &lt;- &quot;13인의 아해가 도로로 질주하오. (길은 막다른 골목이 적당하오.) 제1의 아해가 무섭다고 그리오. 제2의 아해도 무섭다고 그리오. 제3의 아해도 무섭다고 그리오. 제4의 아해도 무섭다고 그리오. 제5의 아해도 무섭다고 그리오. 제6의 아해도 무섭다고 그리오. 제7의 아해도 무섭다고 그리오. 제8의 아해도 무섭다고 그리오. 제9의 아해도 무섭다고 그리오. 제10의 아해도 무섭다고 그리오. 제11의 아해가 무섭다고 그리오. 제12의 아해도 무섭다고 그리오. 제13의 아해도 무섭다고 그리오. 13인의 아해는 무서운 아해와 무서워하는 아해와 그렇게뿐이 모였소.(다른 사정은 없는 것이 차라리 나았소) 그중에 1인의 아해가 무서운 아해라도 좋소. 그중에 2인의 아해가 무서운 아해라도 좋소. 그중에 2인의 아해가 무서워하는 아해라도 좋소. 그중에 1인의 아해가 무서워하는 아해라도 좋소. (길은 뚫린 골목이라도 적당하오.) 13인의 아해가 도로로 질주하지 아니하여도 좋소.&quot; RcppMeCab의 pos()함수를 이용하여 품사를 추출하여 시각화한다. # enc2utf8(ogamdo_txt) %&gt;% # 윈도에서 인코딩 이슈가 있는 경우 ogamdo_txt %&gt;% pos(format = &quot;data.frame&quot;) %&gt;% as_tibble() %&gt;% select(token, pos) %&gt;% count(token, sort = TRUE) %&gt;% filter(str_length(token) &gt; 1) %&gt;% slice_max(n, n = 10) %&gt;% mutate(token = reorder(token, n)) %&gt;% ggplot(aes(token, n)) + geom_col() + coord_flip() RcppMeCab의 pos()함수는 unnest_tokens()의 token =인자에 투입해도 된다. ogamdo_txt %&gt;% enc2utf8 %&gt;% tibble(text = .) %&gt;% unnest_tokens(word, text, token = pos) %&gt;% separate(col = word, into = c(&quot;word&quot;, &quot;morph&quot;), sep = &quot;/&quot; ) %&gt;% count(word, sort = TRUE) %&gt;% filter(str_length(word) &gt; 1) %&gt;% slice_max(n, n = 10) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(word, n)) + geom_col() + coord_flip() 7.4.1 형태소 미추출 형태소를 추출하지 않고 작업을 하게 되면 공백을 단위로 단어를 분절하여 작업이 진행되어 형태소 분석이 반영된 것과 차이를 확인할 수 있다. ogamdo_txt %&gt;% tibble(text = .) %&gt;% unnest_tokens(word, text) %&gt;% count(word, sort = T) %&gt;% filter(str_length(word) &gt; 1) %&gt;% slice_max(n, n = 10) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(word, n)) + geom_col() + coord_flip() 7.5 어간 추출 어간(stem) 추출과 표제어 추출에 대한 설명은 참고 문헌을 참고한다. 어간추출 패키지는 다른 알고리즘을 적용하고 있는 SnowballC와 hunspell 패키지가 있다. SnowballC: 어간추출로 널리 사용되는 알고리즘인 포터 알고리듬 스테밍을 적용. hunspell: 포터알고리즘에 사전 방식 결합 SnowballC와 hunspell은 tidytext 등 텍스트분석 패키지와 패키지로 함께 설치되나 함께 부착되지는 않는다. love_v &lt;- c(&quot;love&quot;, &quot;loves&quot;, &quot;loved&quot;,&quot;love&#39;s&quot; ,&quot;lovely&quot;, &quot;loving&quot;, &quot;lovingly&quot;, &quot;lover&quot;, &quot;lovers&quot;, &quot;lovers&#39;&quot;, &quot;go&quot;, &quot;went&quot;) SnowballC::wordStem(love_v) ## [1] \"love\" \"love\" \"love\" \"love'\" \"love\" ## [6] \"love\" \"lovingli\" \"lover\" \"lover\" \"lovers'\" ## [11] \"go\" \"went\" hunspell::hunspell_stem(love_v) %&gt;% unlist ## [1] \"love\" \"love\" \"loved\" \"love\" \"love\" \"lovely\" ## [7] \"love\" \"loving\" \"love\" \"loving\" \"lover\" \"love\" ## [13] \"love\" \"go\" \"went\" hunspell은 리스트로 산출하므로 unnest()함수로 리스트구조를 풀어준다. unnest()는 flatten_()계열과 달리 데이터프레임을 입력값으로 받는다. hunspell_stem() 함수로 어간추출을 한 다음 어간추출 결과를 단어별로 확인해본다. 이를 위해서 glue 패키지 glue() 함수를 사용하는데 Base R에서 paste()에 해당하는 함수와 개념이 동일하지만 구문작성이 직관적이고 가독성도 뛰어나다. library(hunspell) love_v %&gt;% tibble(text = .) %&gt;% unnest_tokens(word, text) %&gt;% mutate(hunspell = hunspell_stem(word)) %&gt;% mutate(hunspell_word = glue::glue(&quot;{hunspell}&quot;, collapse= &quot;,&quot;)) ## # A tibble: 12 × 3 ## word hunspell hunspell_word ## &lt;chr&gt; &lt;list&gt; &lt;glue&gt; ## 1 love &lt;chr [1]&gt; love ## 2 loves &lt;chr [1]&gt; love ## 3 loved &lt;chr [2]&gt; c(\"loved\", \"love\") ## 4 love's &lt;chr [1]&gt; love ## 5 lovely &lt;chr [2]&gt; c(\"lovely\", \"love\") ## 6 loving &lt;chr [2]&gt; c(\"loving\", \"love\") ## # … with 6 more rows unnest()로 리스트를 풀면 토큰의 수가 늘어난다. hunspell_stem()함수가 스테밍 전후의 단어를 모두 산출하기 때문이다. hunspell로 어간추출할때는 주의해야 한다. hunspell패키지의 목적이 텍스트분석이 아니라 철자확인이다. library(SnowballC) love_v %&gt;% tibble(text = .) %&gt;% unnest_tokens(word, text) %&gt;% mutate(SnowballC = wordStem(word), hunspell = hunspell_stem(word)) %&gt;% mutate(hunspell = glue::glue(&quot;{hunspell}&quot;, collapse= &quot;,&quot;)) ## # A tibble: 12 × 3 ## word SnowballC hunspell ## &lt;chr&gt; &lt;chr&gt; &lt;glue&gt; ## 1 love love love ## 2 loves love love ## 3 loved love c(\"loved\", \"love\") ## 4 love's love' love ## 5 lovely love c(\"lovely\", \"love\") ## 6 loving love c(\"loving\", \"love\") ## # … with 6 more rows 7.6 표제어 추출 7.6.1 어간과 표제어의 차이 어근은 단어의 일부로서 변하지 않는다. 예를 들어, “produced” “producing” “production”의 표제어(lemme)는 “produce”이고 어근은 “produc-”다. “me”와 “my” 그리고, “you”와 “you’re”는 형태는 다르지만, 같은 의미를 공유하하고 있다. 각각 같은 의미이므로 하나로 묶어 줄 필요가 있지만, 어근추출로는 그 목적을 달성할 수 없다. 형태가 달라 어근추출처럼 규칙성을 찾을 수 없기 때문이다. word_v &lt;- c(&quot;love&quot;, &quot;loves&quot;, &quot;loved&quot;, &quot;You&quot;, &quot;You&#39;re&quot;, &quot;You&#39;ll&quot;, &quot;me&quot;, &quot;my&quot;, &quot;myself&quot;, &quot;go&quot;, &quot;went&quot;) SnowballC::wordStem(word_v) ## [1] \"love\" \"love\" \"love\" \"You\" \"You'r\" \"You'll\" ## [7] \"me\" \"my\" \"myself\" \"go\" \"went\" 7.6.2 spacyr 표제어 추출 어간추출과 달리, went의 표제어인 go로 산출한다. me에 대해서는 I를 표제어로 산출하나, my에 대해서는 my를 표제어로 제시한다. library(spacyr) spacy_initialize() word_v &lt;- c(&quot;love&quot;, &quot;loves&quot;, &quot;loved&quot;, &quot;You&quot;, &quot;You&#39;re&quot;, &quot;You&#39;ll&quot;, &quot;me&quot;, &quot;my&quot;, &quot;myself&quot;, &quot;go&quot;, &quot;went&quot;) spacy_parse(word_v, tag = TRUE, entity = FALSE, lemma = FALSE) ## doc_id sentence_id token_id token pos tag ## 1 text1 1 1 love VERB VB ## 2 text2 1 1 loves VERB VBZ ## 3 text3 1 1 loved VERB VBD ## 4 text4 1 1 You PRON PRP ## 5 text5 1 1 You PRON PRP ## 6 text5 1 2 're AUX VBP ## 7 text6 1 1 You PRON PRP ## 8 text6 1 2 'll AUX MD ## 9 text7 1 1 me PRON PRP ## 10 text8 1 1 my PRON PRP$ ## 11 text9 1 1 myself PRON PRP ## 12 text10 1 1 go VERB VB ## 13 text11 1 1 went VERB VBD 7.7 연습 셰익스피어의 소네트27을 SnowbalC와 spacyr을 이용해 분석해 보자 s27_v &lt;- &quot;Weary with toil I haste me to my bed, The dear repose for limbs with travel tired; But then begins a journey in my head To work my mind when body&#39;s work&#39;s expired; For then my thoughts, from far where I abide, Intend a zealous pilgrimage to thee, And keep my drooping eyelids open wide Looking on darkness which the blind do see: Save that my soul&#39;s imaginary sight Presents thy shadow to my sightless view, Which like a jewel hung in ghastly night Makes black night beauteous and her old face new. Lo! thus by day my limbs, by night my mind, For thee, and for myself, no quietness find.&quot; 7.7.1 SnowballC unnest_tokens()의 token =인자에 wordStem()함수를 투입하면 오류 발생. library(SnowballC) s27_v %&gt;% tibble(text = .) %&gt;% unnest_tokens(word, text, token = wordStem) 어근추츨(stemming)을 먼저 한 다음 정돈텍스트(tidy text)로 전환한다. 행(row) 하나에 토큰(token)이 하나만 할당 (one-token-per-row). s27_v %&gt;% SnowballC::wordStem() ## [1] \"Weary with toil I haste me to my bed,\\nThe dear repose for limbs with travel tired;\\nBut then begins a journey in my head\\nTo work my mind when body's work's expired;\\nFor then my thoughts, from far where I abide,\\nIntend a zealous pilgrimage to thee,\\nAnd keep my drooping eyelids open wide\\nLooking on darkness which the blind do see:\\nSave that my soul's imaginary sight\\nPresents thy shadow to my sightless view,\\nWhich like a jewel hung in ghastly night\\nMakes black night beauteous and her old face new.\\nLo! thus by day my limbs, by night my mind,\\nFor thee, and for myself, no quietness find.\" 토큰화를 먼저 한 다음에 어간을 추출한다. s27_v %&gt;% tibble(text = . ) %&gt;% unnest_tokens(word, text) %&gt;% mutate(stemmed = wordStem(word)) %&gt;% count(stemmed, sort = T) ## # A tibble: 81 × 2 ## stemmed n ## &lt;chr&gt; &lt;int&gt; ## 1 my 9 ## 2 for 4 ## 3 to 4 ## 4 a 3 ## 5 and 3 ## 6 night 3 ## # … with 75 more rows 7.7.2 spacyr spacy_parse()는 표제어(lemme)와 품사 태그(pos) 등의 정보가 포함된 데이터프레임으로 산출한다. s27_v %&gt;% spacy_parse() 분석에 필요한 열만 선택한다. s27_v %&gt;% spacy_parse() %&gt;% select(token:pos) unnest_tokens()로 출력 형식 통일 s27_v %&gt;% spacy_parse() %&gt;% select(token:pos) %&gt;% unnest_tokens(word, lemma) %&gt;% count(word, sort = T) 7.7.3 비교 불용어를 제거하지 않고 SnowballC 및 spacyr를 이용한 정규화 결과와 정규화하지 않은 결과를 비교해보자. SnowballC_df &lt;- s27_v %&gt;% tibble(text = . ) %&gt;% unnest_tokens(word, text) %&gt;% mutate(stemmed = wordStem(word)) %&gt;% count(stemmed, sort = T) %&gt;% slice_max(n, n = 15) spacyr_df &lt;- s27_v %&gt;% spacy_parse() %&gt;% select(token:pos) %&gt;% unnest_tokens(word, lemma) %&gt;% count(word, sort = T) %&gt;% slice_max(n, n = 15) noNor_df &lt;- SnowballC_df &lt;- s27_v %&gt;% tibble(text = . ) %&gt;% unnest_tokens(word, text) %&gt;% count(word, sort = T) %&gt;% slice_max(n, n = 15) df &lt;- bind_rows(SnowballC = SnowballC_df, spacyr = spacyr_df, noNor = noNor_df, .id = &quot;ID&quot;) df %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot() + geom_col(aes(word, n, fill = ID), show.legend = F) + coord_flip() + facet_wrap(~ID, scales = &quot;free&quot;) + ggtitle(&quot;정규화 결과 비교&quot;) + xlab(&quot;단어&quot;) + ylab(&quot;빈도&quot;) + theme(plot.title = element_text(size = 24, hjust = 0.5), axis.title.x = element_text(size = 18), axis.title.y = element_text(size = 18)) 불용어를 제거하고 SnowballC 및 spacyr를 이용한 정규화 결과와 정규화하지 않은 결과를 비교해보자. SnowballC_df &lt;- s27_v %&gt;% tibble(text = . ) %&gt;% unnest_tokens(word, text) %&gt;% mutate(stemmed = wordStem(word)) %&gt;% anti_join(stop_words) %&gt;% count(stemmed, sort = T) %&gt;% head(20) spacyr_df &lt;- s27_v %&gt;% spacy_parse() %&gt;% select(token:pos) %&gt;% unnest_tokens(word, lemma) %&gt;% anti_join(stop_words) %&gt;% count(word, sort = T) %&gt;% head(20) noNor_df &lt;- SnowballC_df &lt;- s27_v %&gt;% tibble(text = . ) %&gt;% unnest_tokens(word, text) %&gt;% anti_join(stop_words) %&gt;% count(word, sort = T) %&gt;% head(20) df &lt;- bind_rows(SnowballC = SnowballC_df, spacyr = spacyr_df, noNor = noNor_df, .id = &quot;ID&quot;) df %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot() + geom_col(aes(word, n, fill = ID), show.legend = F) + coord_flip() + facet_wrap(~ID, scales = &quot;free&quot;) + ggtitle(&quot;정규화 결과 비교&quot;) + xlab(&quot;단어&quot;) + ylab(&quot;빈도&quot;) + theme(plot.title = element_text(size = 24, hjust = 0.5), axis.title.x = element_text(size = 18), axis.title.y = element_text(size = 18)) 막대도표 대신 표를 만들어 비교해 보자. 이를 위해서는 데이터프레임을 행방향으로 결합해야 한다. 행결합하면 행의 이름이 구분할 필요가 있으므로, unnest_tokens()함수에서 output =인자를 설정할 때 해당 패키지 이름으로 설정한다. 불용어 처리 전과 후를 구분해서 비교해보자. SnowballC_df &lt;- s27_v %&gt;% tibble(text = . ) %&gt;% unnest_tokens(SnowballC, text) %&gt;% mutate(SnowballC = wordStem(SnowballC)) %&gt;% count(SnowballC) %&gt;% arrange(SnowballC) %&gt;% head(40) spacyr_df &lt;- s27_v %&gt;% spacy_parse() %&gt;% select(lemma) %&gt;% unnest_tokens(spacyr, lemma) %&gt;% count(spacyr) %&gt;% arrange(spacyr) %&gt;% head(40) noNor_df &lt;- s27_v %&gt;% tibble(text = . ) %&gt;% unnest_tokens(noNor, text) %&gt;% count(noNor) %&gt;% arrange(noNor) %&gt;% head(40) bind_cols(noNor_df, SnowballC_df, spacyr_df) 불용어 처리한 다음에도 결과를 비교해보자. SnowballC2_df &lt;- s27_v %&gt;% tibble(text = . ) %&gt;% unnest_tokens(SnowballC, text) %&gt;% mutate(word = wordStem(SnowballC)) %&gt;% anti_join(stop_words) %&gt;% mutate(SnowballC = word) %&gt;% count(SnowballC) %&gt;% arrange(SnowballC) %&gt;% head(40) spacyr2_df &lt;- s27_v %&gt;% spacy_parse() %&gt;% select(lemma) %&gt;% unnest_tokens(word, lemma) %&gt;% anti_join(stop_words) %&gt;% rename(spacyr = word) %&gt;% count(spacyr) %&gt;% arrange(spacyr) %&gt;% head(40) noNor2_df &lt;- s27_v %&gt;% tibble(text = . ) %&gt;% unnest_tokens(word, text) %&gt;% anti_join(stop_words) %&gt;% count(word) %&gt;% arrange(word) %&gt;% head(40) bind_cols(noNor2_df, SnowballC2_df, spacyr2_df) s27_v &lt;- &quot;Weary with toil I haste me to my bed, The dear repose for limbs with travel tired; But then begins a journey in my head To work my mind when body&#39;s work&#39;s expired; For then my thoughts, from far where I abide, Intend a zealous pilgrimage to thee, And keep my drooping eyelids open wide Looking on darkness which the blind do see: Save that my soul&#39;s imaginary sight Presents thy shadow to my sightless view, Which like a jewel hung in ghastly night Makes black night beauteous and her old face new. Lo! thus by day my limbs, by night my mind, For thee, and for myself, no quietness find.&quot; 7.8 과제 구텐베르크 프로젝트에서 영문 문서 한편을 선택해 두 가지 방식의 정규화(SnowballC를 이용한 어근 추출과 spacyr을 이용한 표제어 추출)한 결과와 정규화하지 않은 결과를 비교한다. 막대도표로 시각화해 비교 표로 만들어 비교 3건의 결과에 대해 간략하게 설명 "],["tf-idf.html", "8 . 단어 빈도수 8.1 tf_idf 8.2 승산비 (odds ratio) 8.3 지표 비교 8.4 연습 8.5 과제", " 8 . 단어 빈도수 문서에 사용된 단어(term)를 통해 그 문서의 주제를 추론할 수 있다. 말뭉치는 문서들의 ’뭉치’다. 신문 말뭉치는 여러 신문들의 기사를 모아놓은 것이고, 소설말뭉치는 여러 소설을 모아놓은 문서들의 집합이다. 말뭉치는 상대적인 개념이다. 예를 들어, 소설집을 말뭉치라고 하면 개별 소설이 문서가 된다. 반면, 소설 한편을 말뭉치라고 하면, 소설의 각 장이 문서가 된다. 개별 문서의 주제를 추론하려면 모든 문서에 걸쳐 사용빈도가 높은 단어보다는 개별 문서에서만 사용빈도가 높은 단어를 찾아야 한다. 즉, 개별 문서에 등장하는 단어의 상대빈도가 높은 단어가 개별 문서의 의미를 잘 나타낸다. 상대빈도를 계산하는 방법으로 널리 사용되는 지표가 tf_idf, 승산비, 가중로그승산비 등이다. tf_idf 널리 사용되는 상대빈도 지표. 문서 전반에 걸쳐 등장하는 단어의 점수를 낮게 계산하고, 특정 문서에서 등장하는 빈도에 점수를 높게 부여해 상대 빈도를 계산한다. 예를 들어, ‘운수좋은 날’과 ’사랑손님과 어머니’ 등 2편으로 이뤄진 소설집 말뭉치가 있다고 하자. “은/는/이/가”와 같은 단어는 이 말뭉치에가 사용빈도가 꽤 높지만, 각 소설의 주제를 파악하는데 크게 기여하지 못한다. 반면 ’어머니’와 같은 단어는 ’운수좋은 날’에는 등장하지 않고, ’사랑손님과 어머니’에는 많이 등장해, 해당 문서의 주제를 파악하는데 크게 기여한다. 승산비(odds ratio) 승산(odds)의 비를 이용해 복수의 문서에 등장한 단어의 상대적인 빈도를 계산한다. 2종의 문서에 대해서만 사용할 수 있다. 가중 로그 승산비(weighted log odds) 베이지언 확률모형으로 가중치를 계산해 단어의 상대적인 빈도를 계산한다. tf_idf와 승산비의 단점을 보완한 방법이다. 8.1 tf_idf 단어빈도-역문서빈도(term frequency-inverse document frequency: tf_idf). 말 그대로 개별 문서의 단어빈도(tf)와 문서전반에 걸쳐 사용된 정도의 역수(idf)를 곱해 구한다. 8.1.1 공식 tf_idf의 요소인 tf와 idf에 대해 각각 알아보자. 8.1.1.1 tf (term frequency) 개별 문서(d)에 등장하는 단어(t)의 수를 각 문서에 등장한 모든 단어의 수로 나눈 값. \\[ tf(t, d) = \\frac{tf_{document}}{tf_{total}} \\] \\(tf_{document}\\): 각 문서에 등장한 해당 단어의 빈도 \\(tf_{total}\\): 각 문서에 등장한 모든 단어의 수 8.1.1.2 df (document frequency) 특정 단어(t)가 나타난 문서(D)의 수. df가 높다면 ’은/는/이/가’처럼 대부분의 문서에서 사용되는 단어임을 나타낸다. \\[ df(t, D) \\] 8.1.1.3 idf (inverse document frequency) 전체 문서의 수(N)를 해당 단어의 df로 나눈 뒤 로그를 취한 값. 값이 클수록 특정 문서에서만 등장하는 특이한 단어라는 의미가 된다. \\[ idf(t, D) = log(\\frac{N}{df}) \\] 8.1.1.4 tf_idf tf와 idf를 곱하면 tf_idf를 구할 수 있다. \\[ \\text{tf_idf}(t, d, D) = tf(t, d) \\times idf(t, D) \\] t: 단어 d: 개별 문서 D: 개별 문서의 집합(말뭉치) 8.1.2 적용 짧은 문장을 이용해 tf_idf의 원리를 파악해 보자. 먼저 토큰화한다. 여기서 각 행은 개별 문서에 해당하고, 4개 행의 합의 말뭉치에 해당한다. 각 행을 식별하기 위해 문장별로 토큰화해 ID를 부여하자. library(magrittr) # 파이프 연산자 %&gt;% library(stringr) # 문자열 처리 library(tidytext) # 깔끔한 텍스트 티블에서 처리 sky_v &lt;- c( &quot;The sky is blue.&quot;, &quot;The sun is bright today.&quot;, &quot;The sun in the sky is bright.&quot;, &quot;We can see the shining sun, the bright sun.&quot;) sky_doc &lt;- tibble(text = sky_v) %&gt;% unnest_tokens(sentence, text, token = &quot;sentences&quot;) %&gt;% mutate(line_id = row_number()) %&gt;% mutate(line_id = as.factor(line_id)) sky_doc ## # A tibble: 4 × 2 ## sentence line_id ## &lt;chr&gt; &lt;fct&gt; ## 1 the sky is blue. 1 ## 2 the sun is bright today. 2 ## 3 the sun in the sky is bright. 3 ## 4 we can see the shining sun, the bright sun. 4 8.1.2.1 tf 개별 문서(d)에 등장하는 단어(t)의 수를 각 문서에 등장한 모든 단어의 수로 나눈 값. \\[ tf(t, d) = \\frac{tf_{document}}{tf_{total}} \\] \\(tf_{document}\\): 각 문서에 등장한 해당 단어의 빈도 \\(tf_{total}\\): 각 문서에 등장한 모든 단어의 수 tf의 분자인 \\(tf_{document}\\)(각 문서에 등장한 해당 단어의 빈도)를 구한다. 여기서 각 문서는 각 행이다. 따라서 각 행에서 등장한 단어의 빈도를 계산하면 된다. sky_tfd &lt;- sky_doc %&gt;% unnest_tokens(word, sentence) %&gt;% anti_join(stop_words) %&gt;% count(line_id, word, name = &quot;tf_doc&quot;) sky_tfd ## # A tibble: 10 × 3 ## line_id word tf_doc ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 blue 1 ## 2 1 sky 1 ## 3 2 bright 1 ## 4 2 sun 1 ## 5 3 bright 1 ## 6 3 sky 1 ## # … with 4 more rows tf의 분모 \\(tf_{total}\\)(각 문서에 등장한 모든 단어의 수)를 구한다. 각 행을 이루는 모든 단어의 수를 계산하면 된다. sky_tft &lt;- sky_tfd %&gt;% mutate(N = length(unique(line_id))) %&gt;% # total number of documnets count(line_id, name = &quot;tf_total&quot;) sky_tft ## # A tibble: 4 × 2 ## line_id tf_total ## &lt;fct&gt; &lt;int&gt; ## 1 1 2 ## 2 2 2 ## 3 3 3 ## 4 4 3 \\(tf_{document}\\)(각 문서에 등장한 해당 단어의 빈도)를 \\(tf_{total}\\)(각 문서에 등장한 모든 단어의 수)로 나눈다. sky_tf &lt;- left_join(sky_tfd, sky_tft) %&gt;% mutate(tf = tf_doc/tf_total) sky_tf ## # A tibble: 10 × 5 ## line_id word tf_doc tf_total tf ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 blue 1 2 0.5 ## 2 1 sky 1 2 0.5 ## 3 2 bright 1 2 0.5 ## 4 2 sun 1 2 0.5 ## 5 3 bright 1 3 0.333 ## 6 3 sky 1 3 0.333 ## # … with 4 more rows 8.1.2.2 df (document frequency) 특정 단어(t)가 나타난 문서(D)의 수. df가 높다면 ’은/는/이/가’처럼 대부분의 문서에서 사용되는 단어임을 나타낸다. \\[ df(t, D) \\] 특정 단어가 등장한 문서의 수를 계산해 구할 수 있다. # sky_tf %&gt;% # count(word) sky_df &lt;- table(sky_tf$word) %&gt;% as.data.frame() %&gt;% rename(word = Var1, df = Freq) sky_df ## word df ## 1 blue 1 ## 2 bright 3 ## 3 shining 1 ## 4 sky 2 ## 5 sun 3 ‘blue’: 1번행에만 등장했으므로 1 ‘sun’: 2,3, 4번 행에 등장했으므로 3 8.1.2.3 idf (inverse document frequency) 전체 문서의 수(N)를 해당 단어의 df로 나눈 뒤 로그를 취한 값. 값이 클수록 특정 문서에서만 등장하는 특이한 단어라는 의미가 된다. \\[ idf(t, D) = log(\\frac{N}{df}) \\] 공식을 그대로 적용해 계산한다. sky_tf에 N의 값을 계산해 두 데이터프레임을 열방향 결합한다. sky_tf &lt;- sky_tf %&gt;% mutate(N = length(unique(line_id))) #total number of documnets sky_idf &lt;- left_join(sky_tf, sky_df) %&gt;% mutate(idf = log(N / df)) sky_idf ## # A tibble: 10 × 8 ## line_id word tf_doc tf_total tf N df idf ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 blue 1 2 0.5 4 1 1.39 ## 2 1 sky 1 2 0.5 4 2 0.693 ## 3 2 bright 1 2 0.5 4 3 0.288 ## 4 2 sun 1 2 0.5 4 3 0.288 ## 5 3 bright 1 3 0.333 4 3 0.288 ## 6 3 sky 1 3 0.333 4 2 0.693 ## # … with 4 more rows 3개 행에 걸쳐 등장하는 ’bright’의 idf가 1개 행에만 등장한 ’blue’보다 idf가 작다. 8.1.2.4 tf_idf tf와 idf를 곱하면 tf_idf를 구할 수 있다. \\[ tfidf(t, d, D) = tf(t, d) \\times idf(t, D) \\] t: 단어 d: 개별 문서 D: 개별 문서의 집합(말뭉치) sky_idf %&gt;% mutate(tf_idf = tf * idf) %&gt;% arrange(-tf_idf) ## # A tibble: 10 × 9 ## line_id word tf_doc tf_total tf N df idf ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 blue 1 2 0.5 4 1 1.39 ## 2 4 shining 1 3 0.333 4 1 1.39 ## 3 1 sky 1 2 0.5 4 2 0.693 ## 4 3 sky 1 3 0.333 4 2 0.693 ## 5 4 sun 2 3 0.667 4 3 0.288 ## 6 2 bright 1 2 0.5 4 3 0.288 ## # … with 4 more rows, and 1 more variable: tf_idf &lt;dbl&gt; 8.1.3 bind_tf_idf() 함수 tidytext패키지에서 제공하는 bind_tf_idf(tbl, term, document, n)함수를 이용하면 tf_idf를 계산할 수 있다. tbl: 정돈데이터(한행에 값 하나). term: 문자열이나 기호 등의 단어가 저장된 열. document: 문자열이나 기호 등의 문서식별부호가 저장된 열. n: 문자열이나 기호 등의 문서-용어의 빈도가 저장된 열. sky_tfd %&gt;% bind_tf_idf(tbl = ., term = word, document = line_id, n = tf_doc) ## # A tibble: 10 × 6 ## line_id word tf_doc tf idf tf_idf ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 blue 1 0.5 1.39 0.693 ## 2 1 sky 1 0.5 0.693 0.347 ## 3 2 bright 1 0.5 0.288 0.144 ## 4 2 sun 1 0.5 0.288 0.144 ## 5 3 bright 1 0.333 0.288 0.0959 ## 6 3 sky 1 0.333 0.693 0.231 ## # … with 4 more rows 8.2 승산비 (odds ratio) 문서의 상대빈도를 구하는 또 다른 방법이 승산비다. 승산(odds)의 비를 이용해 복수의 문서에 등장한 단어의 상대적인 빈도 계산할 수 있다. 따라서 승산비는 2종의 문서에 대해서만 구할 수 있다. 승산비(odds ratio) 한 사건의 승산(odds)에 대한 다른 한 사건 승산의 비(ratio)다. 다음은 승산B에 대한 승산A의 비. \\[ 승산비 = \\frac{승산A}{승산B} \\] 비(ratio) 두 부분 중 한 부분에 대한 다른 한 부분의 비. 예를 들어, 축구경기를 할때 한국이 3골을 넣고, 일본이 1골을 넣었다면, 비는 3대 1로서, 일본팀 1점에 대한 한국팀 3점의 비(ratio)다. \\[ A:B = \\frac{A}{B} \\] 승산(odds) 어느 한 사건이 일어날 가능성이 승산(odds)이다. 영어를 그대로 읽어 ’오즈’라고도 한다. 한 사건의 발생빈도(n)를 전체의 값(total)으로 나눈다. \\[ 승산(odds) = \\frac{발생빈도(n) + 1}{총빈도(total) + 1} \\] 분자와 분모에 각각 1을 더하는 이유는 발생빈도가 0인 경우도 있기 때문이다. 분모가 0이면 무한대가 된다. 8.2.1 자료준비 네이버영화 댓글의 승산비를 구해보자. 댓글을 긍정과 부정으로 구분해 라벨링한 자료를 이용한다. Naver sentiment movie corpus url_v &lt;- &#39;https://github.com/e9t/nsmc/raw/master/ratings.txt&#39; dest_v &lt;- &#39;data/ratings.txt&#39; download.file(url_v, dest_v, mode = &quot;wb&quot;) list.files(&#39;data/.&#39;) 다운로드한 자료 이입 및 검토 read_lines(&quot;data/ratings.txt&quot;) %&gt;% glimpse() ## chr [1:200001] \"id\\tdocument\\tlabel\" ... 공백문자 \\t으로 구분된 문자데이터이므로, read_tsv()로 이입한다. read_tsv(&quot;data/ratings.txt&quot;) %&gt;% glimpse() ## Rows: 200,000 ## Columns: 3 ## $ id &lt;dbl&gt; 8112052, 8132799, 4655635, 9251303, 10067… ## $ document &lt;chr&gt; \"어릴때보고 지금다시봐도 재밌어요ㅋㅋ\", \"… ## $ label &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ratings_df &lt;- read_tsv(&quot;data/ratings.txt&quot;) head(ratings_df) ## # A tibble: 6 × 3 ## id document label ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 8112052 어릴때보고 지금다시봐도 재밌어요ㅋㅋ 1 ## 2 8132799 디자인을 배우는 학생으로, 외국디자이너와 … 1 ## 3 4655635 폴리스스토리 시리즈는 1부터 뉴까지 버릴께 … 1 ## 4 9251303 와.. 연기가 진짜 개쩔구나.. 지루할거라고 … 1 ## 5 10067386 안개 자욱한 밤하늘에 떠 있는 초승달 같은 … 1 ## 6 2190435 사랑을 해본사람이라면 처음부터 끝까지 웃을… 1 데이터셋이 커 처리에 시간이 걸리므로 각 라벨별로 1000개 행씩 추출한다. set.seed(37) by1000_df &lt;- ratings_df %&gt;% group_by(label) %&gt;% sample_n(size = 1000) by1000_df %&gt;% glimpse() ## Rows: 2,000 ## Columns: 3 ## Groups: label [2] ## $ id &lt;dbl&gt; 10101634, 5925555, 9809791, 4541051, 8063… ## $ document &lt;chr&gt; \"액션이 부족한 가운데 이야기 자체가 허접… ## $ label &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… RcppMeCab패키지에서 pos()함수로 형태소를 추출한 뒤, 일반명사(‘nng’)만 추출해 빈도를 계산한다. by1000_df %&gt;% unnest_tokens(word, document, token = RcppMeCab::pos) %&gt;% separate(word, c(&quot;word&quot;, &quot;pos&quot;), sep = &quot;/&quot;) %&gt;% filter(pos == &quot;nng&quot;) %&gt;% filter(word != &quot;영화&quot;) %&gt;% count(word, sort = TRUE) ## # A tibble: 2,819 × 3 ## # Groups: label [2] ## label word n ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 최고 83 ## 2 1 감동 58 ## 3 1 연기 58 ## 4 1 생각 51 ## 5 0 감독 46 ## 6 0 스토리 46 ## # … with 2,813 more rows 영화평은 긍정적인 평에는 ‘1’, 부정적인 평에는 ’0’으로 분류돼 있다. 긍정적인 평과 부정적인 평에 사용된 명사의 승산비를 구해보자. 이를 위해 long form을 wide form으로 변형해 label열의 값을 열의 헤더로 변환하고, 각 열의 값으로는 토큰을 투입한다. 결측값이 있으면 연산이 안되므로 NA값은 ’0’으로 채운다. by1000_df %&gt;% unnest_tokens(word, document, token = RcppMeCab::pos) %&gt;% separate(word, c(&quot;word&quot;, &quot;pos&quot;), sep = &quot;/&quot;) %&gt;% filter(pos == &quot;nng&quot;) %&gt;% filter(word != &quot;영화&quot;) %&gt;% count(word) %&gt;% pivot_wider(names_from = label, values_from = n, values_fill = list(n = 0)) ## # A tibble: 2,294 × 3 ## word `0` `1` ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 가가 1 0 ## 2 가관 1 0 ## 3 가능 1 4 ## 4 가든 1 0 ## 5 가리 1 0 ## 6 가문 1 0 ## # … with 2,288 more rows 8.2.2 승산비 계산 긍정평과 부정평의 승산을 구한다음, 승산비를 계산한다. by1000_df %&gt;% unnest_tokens(word, document, token = RcppMeCab::pos) %&gt;% separate(word, c(&quot;word&quot;, &quot;pos&quot;), sep = &quot;/&quot;) %&gt;% filter(pos == &quot;nng&quot;) %&gt;% filter(word != &quot;영화&quot;) %&gt;% count(word) %&gt;% pivot_wider(names_from = label, values_from = n, values_fill = list(n = 0)) %&gt;% rename(posi = `1`, nega = `0`) %&gt;% mutate(odds_posi = ((posi+1)/sum(posi+1)), odds_nega = ((nega+1)/sum(nega+1))) %&gt;% mutate(posi_odds_ratio = (odds_posi / odds_nega)) ## # A tibble: 2,294 × 6 ## word nega posi odds_posi odds_nega posi_odds_ratio ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 가가 1 0 0.000181 0.000334 0.541 ## 2 가관 1 0 0.000181 0.000334 0.541 ## 3 가능 1 4 0.000903 0.000334 2.70 ## 4 가든 1 0 0.000181 0.000334 0.541 ## 5 가리 1 0 0.000181 0.000334 0.541 ## 6 가문 1 0 0.000181 0.000334 0.541 ## # … with 2,288 more rows 승산비를 이용해 영화의 긍정평과 부정평에서 상대적으로 많이 사용된 명사를 추출했다. 순서대로 정렬하자. rate_odds_df &lt;- by1000_df %&gt;% unnest_tokens(word, document, token = RcppMeCab::pos) %&gt;% separate(word, c(&quot;word&quot;, &quot;pos&quot;), sep = &quot;/&quot;) %&gt;% filter(pos == &quot;nng&quot;) %&gt;% filter(word != &quot;영화&quot;) %&gt;% count(word) %&gt;% pivot_wider(names_from = label, values_from = n, values_fill = list(n = 0)) %&gt;% rename(posi = `1`, nega = `0`) %&gt;% mutate(odds_posi = ((posi+1)/sum(posi+1)), odds_nega = ((nega+1)/sum(nega+1))) %&gt;% mutate(posi_odds_ratio = (odds_posi / odds_nega)) %&gt;% # 긍정평과 부정평 각각 상위 20개씩 필터 filter(rank(posi_odds_ratio) &lt;= 20 | rank(-posi_odds_ratio) &lt;= 20) %&gt;% arrange(-posi_odds_ratio) rate_odds_df %&gt;% head() ## # A tibble: 6 × 6 ## word nega posi odds_posi odds_nega posi_odds_ratio ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 굿 0 17 0.00325 0.000167 19.5 ## 2 밋 0 16 0.00307 0.000167 18.4 ## 3 ㄷ 0 15 0.00289 0.000167 17.3 ## 4 삶 0 11 0.00217 0.000167 13.0 ## 5 최고 8 83 0.0152 0.00150 10.1 ## 6 만족 0 8 0.00162 0.000167 9.73 rate_odds_df %&gt;% tail() ## # A tibble: 6 × 6 ## word nega posi odds_posi odds_nega posi_odds_ratio ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 낭비 9 0 0.000181 0.00167 0.108 ## 2 얘기 10 0 0.000181 0.00184 0.0983 ## 3 쓰레기 36 2 0.000542 0.00618 0.0877 ## 4 돈 19 0 0.000181 0.00334 0.0541 ## 5 짜증 24 0 0.000181 0.00417 0.0433 ## 6 최악 24 0 0.000181 0.00417 0.0433 ’밋’과 ’ㄷ’이 사용된 문장을 살펴보자. drop =인자에 FALSE를 투입하면 토큰화한 문장을 제거하지 않고 남겨둔다. by1000_df %&gt;% unnest_tokens(word, document, token = RcppMeCab::pos, drop = FALSE) %&gt;% separate(word, c(&quot;word&quot;, &quot;pos&quot;), sep = &quot;/&quot;) %&gt;% filter(pos == &quot;nng&quot;) %&gt;% filter(word == &quot;밋&quot; | word == &quot;ㄷ&quot;) ## # A tibble: 31 × 4 ## # Groups: label [1] ## label document word pos ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 \"이보다 더 유쾌할 수 없고 이보다 더 무… 밋 nng ## 2 1 \"이보다 더 유쾌할 수 없고 이보다 더 무… ㄷ nng ## 3 1 \"이보다 더 유쾌할 수 없고 이보다 더 무… ㄷ nng ## 4 1 \"이보다 더 유쾌할 수 없고 이보다 더 무… ㄷ nng ## 5 1 \"이보다 더 유쾌할 수 없고 이보다 더 무… ㄷ nng ## 6 1 \"이보다 더 유쾌할 수 없고 이보다 더 무… ㄷ nng ## # … with 25 more rows ’재밋었음’에서 MeCab형태소 분석기가 ’밋’을 명사로 추출한 결과이다. enc2utf8(&quot;재밋었음10자는뭐여&quot;) %&gt;% RcppMeCab::pos() ## $재밋었음10자는뭐여 ## [1] \"재/XPN\" \"밋/NNG\" \"었/EP\" \"음/ETN\" ## [5] \"10/SN\" \"자/NNG\" \"는/JX\" \"뭐/NP\" ## [9] \"여/VCP+EC\" 막대도표로 시각화하자. 긍정평과 부정평 도표를 분리해 표시하기 위해 승산비 1을 기준으로 크면 ‘긍정평’ 작으면 ’부정평’을 부여한다. rate_odds_df %&gt;% mutate(label = ifelse(posi_odds_ratio &gt; 1, &quot;긍정평&quot;, &quot;부정평&quot;)) %&gt;% mutate(word = reorder(word, posi_odds_ratio)) %&gt;% ggplot(aes(x = posi_odds_ratio, y = word, fill = label)) + geom_col(show.legend = F) + facet_wrap(~label, scales = &quot;free&quot;) 중요도가 비슷한 단어를 추출해보자. 승산비가 1이면 분자와 분모가 같으므로 긍정평과 부정평의 승산이 같다는 의미다. by1000_df %&gt;% unnest_tokens(word, document, token = RcppMeCab::pos) %&gt;% separate(word, c(&quot;word&quot;, &quot;pos&quot;), sep = &quot;/&quot;) %&gt;% filter(pos == &quot;nng&quot;) %&gt;% filter(word != &quot;영화&quot;) %&gt;% count(word) %&gt;% pivot_wider(names_from = label, values_from = n, values_fill = list(n = 0)) %&gt;% rename(posi = `1`, nega = `0`) %&gt;% mutate(odds_posi = ((posi+1)/sum(posi+1)), odds_nega = ((nega+1)/sum(nega+1))) %&gt;% mutate(posi_odds_ratio = (odds_posi / odds_nega)) %&gt;% # 승산비 1을 중심으로 정렬 arrange(abs(1 - posi_odds_ratio)) %&gt;% head(20) ## # A tibble: 20 × 6 ## word nega posi odds_posi odds_nega posi_odds_ratio ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 이야기 14 13 0.00253 0.00250 1.01 ## 2 개봉 10 9 0.00181 0.00184 0.983 ## 3 정도 18 16 0.00307 0.00317 0.968 ## 4 결말 7 6 0.00126 0.00134 0.946 ## 5 몰입 7 6 0.00126 0.00134 0.946 ## 6 이상 15 13 0.00253 0.00267 0.946 ## # … with 14 more rows 중요도가 비슷하면서 빈도가 높은 단어를 찾아 보자. by1000_df %&gt;% unnest_tokens(word, document, token = RcppMeCab::pos) %&gt;% separate(word, c(&quot;word&quot;, &quot;pos&quot;), sep = &quot;/&quot;) %&gt;% filter(pos == &quot;nng&quot;) %&gt;% filter(word != &quot;영화&quot;) %&gt;% count(word) %&gt;% pivot_wider(names_from = label, values_from = n, values_fill = list(n = 0)) %&gt;% rename(posi = `1`, nega = `0`) %&gt;% # 긍정평과 부정평에서 각각 10회 초과한 단어 filter filter(nega &gt; 10 &amp; posi &gt; 10) %&gt;% mutate(odds_posi = ((posi+1)/sum(posi+1)), odds_nega = ((nega+1)/sum(nega+1))) %&gt;% mutate(posi_odds_ratio = (odds_posi / odds_nega)) %&gt;% arrange(abs(1 - posi_odds_ratio)) %&gt;% head(20) ## # A tibble: 20 × 6 ## word nega posi odds_posi odds_nega posi_odds_ratio ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 이야기 14 13 0.0201 0.0199 1.01 ## 2 정도 18 16 0.0244 0.0252 0.968 ## 3 이상 15 13 0.0201 0.0212 0.947 ## 4 연출 14 12 0.0187 0.0199 0.938 ## 5 드라마 35 35 0.0516 0.0477 1.08 ## 6 배우 27 22 0.0330 0.0371 0.889 ## # … with 14 more rows 8.2.3 로그승산비 로그승산비(log odds ratio) 승산비에 로그를 위한 값. 로그를 취하면 1보다 작은수는 음수가 되고, 1보다 큰수는 양수가 된다. \\[ 로그 승산비 = log(\\frac{승산A}{승산B}) \\] by1000_df %&gt;% unnest_tokens(word, document, token = RcppMeCab::pos) %&gt;% separate(word, c(&quot;word&quot;, &quot;pos&quot;), sep = &quot;/&quot;) %&gt;% filter(pos == &quot;nng&quot;) %&gt;% filter(word != &quot;영화&quot;) %&gt;% count(word) %&gt;% pivot_wider(names_from = label, values_from = n, values_fill = list(n = 0)) %&gt;% rename(posi = `1`, nega = `0`) %&gt;% mutate(odds_posi = ((posi+1)/sum(posi+1)), odds_nega = ((nega+1)/sum(nega+1))) %&gt;% # 승산비에 로그를 취한다. mutate(log_odds_ratio = log(odds_posi / odds_nega)) ## # A tibble: 2,294 × 6 ## word nega posi odds_posi odds_nega log_odds_ratio ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 가가 1 0 0.000181 0.000334 -0.615 ## 2 가관 1 0 0.000181 0.000334 -0.615 ## 3 가능 1 4 0.000903 0.000334 0.995 ## 4 가든 1 0 0.000181 0.000334 -0.615 ## 5 가리 1 0 0.000181 0.000334 -0.615 ## 6 가문 1 0 0.000181 0.000334 -0.615 ## # … with 2,288 more rows 로그 승산비를 이용하면 하나의 도표에 상대빈도를 표시할 수 있다. 먼저 로그승산비를 구한뒤, 0을 기준으로 긍정평과 부정평을 group으로 구분한다. 긍정평과 부정평 집단별로 구분돼 있으므로, 로그승산비의 절대값 상위 10개를 지정하면, 긍정평과 부정평 별로 각각 상위 10개 단어를 추출할 수 있다. rate_log_df &lt;- by1000_df %&gt;% unnest_tokens(word, document, token = RcppMeCab::pos) %&gt;% separate(word, c(&quot;word&quot;, &quot;pos&quot;), sep = &quot;/&quot;) %&gt;% filter(pos == &quot;nng&quot;) %&gt;% filter(word != &quot;영화&quot;) %&gt;% count(word) %&gt;% pivot_wider(names_from = label, values_from = n, values_fill = list(n = 0)) %&gt;% rename(posi = `1`, nega = `0`) %&gt;% mutate(odds_posi = ((posi+1)/sum(posi+1)), odds_nega = ((nega+1)/sum(nega+1))) %&gt;% # 승산비에 로그를 취한다. mutate(log_odds_ratio = log(odds_posi / odds_nega)) rate_log_df %&gt;% group_by(label = ifelse(log_odds_ratio &gt; 0, &quot;긍정평&quot;, &quot;부정평&quot;)) %&gt;% # 긍정평과 부정평별 각각 상위 10개 slice_max(abs(log_odds_ratio), n = 10) ## # A tibble: 26 × 7 ## # Groups: label [2] ## word nega posi odds_posi odds_nega log_odds_ratio label ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 굿 0 17 0.00325 0.000167 2.97 긍정… ## 2 밋 0 16 0.00307 0.000167 2.91 긍정… ## 3 ㄷ 0 15 0.00289 0.000167 2.85 긍정… ## 4 삶 0 11 0.00217 0.000167 2.56 긍정… ## 5 최고 8 83 0.0152 0.00150 2.31 긍정… ## 6 만족 0 8 0.00162 0.000167 2.28 긍정… ## # … with 20 more rows 막대도표로 시각화하면 다음과 같은 긍부정에 기여하는 중요단어를 한눈에 일별할 수 있다. rate_log_df %&gt;% group_by(label = ifelse(log_odds_ratio &gt; 0, &quot;긍정평&quot;, &quot;부정평&quot;)) %&gt;% slice_max(abs(log_odds_ratio), n = 10) %&gt;% ggplot(aes(x = log_odds_ratio, y = reorder(word, log_odds_ratio), fill = label)) + geom_col() 8.2.4 가중로그승산비 로그승산비는 문서 2종에 대한 승산으로 비를 구하므로, 3종 이상의 문서로 구성된 말뭉치에 적용할 수 없는 한계가 있다. tf_idf는 3종 이상의 문서로 구성된 말뭉치에 적용할 수 있지만, tf_idf계산 방식에서 오는 한계가 있다. 모든 문서에 걸쳐 등장하는 단어라 해서 반드시 불용어처럼 문서의 의미파악에 기여하지 못하는 것이 아니기 때문이다. tf_idf의 한계를 Tyler Schnoebelen이 [Monroe, Colaresi, and Quinn(2008)](Monroe, Colaresi, and Quinn(2008)의 연구를 토대 블로그문서 “I dare say you will never use tf-idf again”을 통해 지적하며, 대안으로 로그승산비에 베이지언 확률모형을 적용한 가중로그승산비(weighted log odds ratio)를 제안했다. 이에 tidytext 개발자인 Julia Silge가 tydylo패키지를 개발해 가중로그승산비를 간단하게 계산할 수 있도록 했다. (Silge의 패키지 소개 글) install.packages(&quot;tidylo&quot;) 8.2.5 bind_log_odds() 함수 bind_log_odds(tbl, set, feature, n, uninformative = FALSE, unweighted = FALSE) 함수 주요 인자를 살펴보면 다음과 같다. tbl: 정돈데이터(feature와 set이 하나의 행에 저장). set: feature를 비교하기 위한 set(group)에 대한 정보(예: 긍정 vs. 부정)이 저장된 열. feature: feature(단어나 바이그램 등의 텍스트자료)가 저장된 열. n: feature-set의 빈도를 저장한 열. uninformative: uninformative 디리슐레 분포 사용 여부. 기본값은 FALSE. unweighted: 비가중 로그승산 사용여부. 기본값은 FALSE. TRUE로 지정하면 비가중 로그승산비(log_odds) 열을 추가한다. 가중로그승산비를 이용해 네이버영화 댓글 중 긍정평과 부정평에 사용된 단어의 상대빈도를 구해보자. library(tidylo) weighted_log_odds_df &lt;- by1000_df %&gt;% unnest_tokens(word, document, token = RcppMeCab::pos) %&gt;% separate(word, c(&quot;word&quot;, &quot;pos&quot;), sep = &quot;/&quot;) %&gt;% filter(pos == &quot;nng&quot;) %&gt;% filter(word != &quot;영화&quot;) %&gt;% count(word) %&gt;% bind_log_odds(set = label, feature = word, n = n) %&gt;% arrange(-log_odds_weighted) weighted_log_odds_df ## # A tibble: 2,819 × 4 ## # Groups: label [2] ## label word n log_odds_weighted ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 짜증 24 3.18 ## 2 0 최악 24 3.18 ## 3 1 굿 17 3.07 ## 4 1 최고 83 3.06 ## 5 1 밋 16 2.98 ## 6 1 ㄷ 15 2.88 ## # … with 2,813 more rows 긍정평과 부정평별 가중로그승산비 상위 10개 추출한다. weighted_log_odds_df %&gt;% group_by(label = ifelse(label &gt; 0, &quot;긍정평&quot;, &quot;부정평&quot;)) %&gt;% slice_max(abs(log_odds_weighted), n = 10) # 긍정평과 부정평별 각각 상위 10개 ## # A tibble: 22 × 4 ## # Groups: label [2] ## label word n log_odds_weighted ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 긍정평 굿 17 3.07 ## 2 긍정평 최고 83 3.06 ## 3 긍정평 밋 16 2.98 ## 4 긍정평 ㄷ 15 2.88 ## 5 긍정평 삶 11 2.46 ## 6 긍정평 감동 58 2.28 ## # … with 16 more rows 막대도표로 시각화한다. 특히 reorder_within() 함수를 사용하게 되면 작은창(facet) 에 따라 내림차순하여 시각적으로 깔끔하게 구현할 수 있다. weighted_log_odds_df %&gt;% group_by(label = ifelse(label &gt; 0, &quot;긍정평&quot;, &quot;부정평&quot;)) %&gt;% slice_max(abs(log_odds_weighted), n = 10) %&gt;% # 긍정평과 부정평별 각각 상위 10개 ggplot(aes(x = log_odds_weighted, y = reorder_within(word, log_odds_weighted, label), fill = label)) + scale_y_reordered() + geom_col(show.legend = FALSE) + facet_wrap(~label, scale = &quot;free_y&quot;) 부정평으로 분류된 내용 중 ’최고’와 ’감동’이 부정평에 들어 있다. 어떤 문장인지 확인해보자. by1000_df %&gt;% unnest_tokens(word, document, token = RcppMeCab::pos, drop = FALSE) %&gt;% separate(word, c(&quot;word&quot;, &quot;pos&quot;), sep = &quot;/&quot;) %&gt;% filter(pos == &quot;nng&quot;) %&gt;% filter(word != &quot;영화&quot;) %&gt;% filter(label == 0) %&gt;% filter(word == &quot;최고&quot; | word == &quot;감동&quot;) ## # A tibble: 19 × 4 ## # Groups: label [1] ## label document word pos ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0 \"액션이 부족한 가운데 이야기 자체가 허… 감동 nng ## 2 0 \"액션이 부족한 가운데 이야기 자체가 허… 감동 nng ## 3 0 \"액션이 부족한 가운데 이야기 자체가 허… 최고 nng ## 4 0 \"액션이 부족한 가운데 이야기 자체가 허… 최고 nng ## 5 0 \"액션이 부족한 가운데 이야기 자체가 허… 최고 nng ## 6 0 \"액션이 부족한 가운데 이야기 자체가 허… 최고 nng ## # … with 13 more rows ‘억지로 끼워 맞추려는 감동 설정’ ‘감동 없음’ 등 단어 하나만을 토큰으로 사용했을 때의 한계를 잘 보여주는 사례. ’감동적 그자체 영화’를 부정평으로 분류한 것은 라벨링 오류. 8.3 지표 비교 8.3.1 로그승산비 먼저 가중치를 사용하지않은 로그승산비를 구한다. rate_log_df %&gt;% # 승산비에 로그를 취한다. group_by(label = ifelse(log_odds_ratio &gt; 0, &quot;긍정평&quot;, &quot;부정평&quot;)) %&gt;% slice_max(abs(log_odds_ratio), n = 10) %&gt;% # 긍정평과 부정평별 각각 상위 10개 ggplot(aes(x = log_odds_ratio, y = reorder(word, log_odds_ratio), fill = label)) + geom_col(show.legend = F) + facet_wrap(~label, scale = &quot;free&quot;) 8.3.2 tf_idf tf_idf 계산를 계산한다. tf_idf_df &lt;- by1000_df %&gt;% unnest_tokens(word, document, token = RcppMeCab::pos) %&gt;% separate(word, c(&quot;word&quot;, &quot;pos&quot;), sep = &quot;/&quot;) %&gt;% filter(pos == &quot;nng&quot;) %&gt;% filter(word != &quot;영화&quot;) %&gt;% count(word) %&gt;% bind_tf_idf(term = word, document = label, n = n) tf_idf_df ## # A tibble: 2,819 × 6 ## # Groups: label [2] ## label word n tf idf tf_idf ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 가가 1 0.000270 0.693 0.000187 ## 2 0 가관 1 0.000270 0.693 0.000187 ## 3 0 가능 1 0.000270 0 0 ## 4 0 가든 1 0.000270 0.693 0.000187 ## 5 0 가리 1 0.000270 0.693 0.000187 ## 6 0 가문 1 0.000270 0.693 0.000187 ## # … with 2,813 more rows 긍정평과 부정평별 상위 10개 추출한다. tf_idf_df %&gt;% group_by(label = ifelse(label &gt; 0, &quot;긍정평&quot;, &quot;부정평&quot;)) %&gt;% slice_max(tf_idf, n = 10) # 긍정평과 부정평별 각각 상위 10개 ## # A tibble: 21 × 6 ## # Groups: label [2] ## label word n tf idf tf_idf ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 긍정평 굿 17 0.00524 0.693 0.00363 ## 2 긍정평 밋 16 0.00493 0.693 0.00342 ## 3 긍정평 ㄷ 15 0.00462 0.693 0.00320 ## 4 긍정평 삶 11 0.00339 0.693 0.00235 ## 5 긍정평 만족 8 0.00246 0.693 0.00171 ## 6 긍정평 행복 7 0.00216 0.693 0.00149 ## # … with 15 more rows 막대도표로 시각화한다. tf_idf_df %&gt;% group_by(label = ifelse(label &gt; 0, &quot;긍정평&quot;, &quot;부정평&quot;)) %&gt;% slice_max(tf_idf, n = 10) %&gt;% # 긍정평과 부정평별 각각 상위 10개 ggplot(aes(x = tf_idf, y = reorder(word, tf_idf), fill = label)) + geom_col(show.legend = F) + facet_wrap(~label, scale = &quot;free&quot;) 8.3.3 비교 가중로그승산비, 로그승산비, tf_idf의 값을 비교해보자. 먼저 3개 데이터프레임을 행방향으로 결합해 하나의 데이터프레임으로 저장한다. wlo_df &lt;- weighted_log_odds_df %&gt;% group_by(label = ifelse(label &gt; 0, &quot;긍정평&quot;, &quot;부정평&quot;)) %&gt;% slice_max(abs(log_odds_weighted), n = 10) %&gt;% select(label, word, score = log_odds_weighted) rlo_df &lt;- rate_log_df %&gt;% group_by(label = ifelse(log_odds_ratio &gt; 0, &quot;긍정평&quot;, &quot;부정평&quot;)) %&gt;% slice_max(abs(log_odds_ratio), n = 10) %&gt;% select(label, word, score = log_odds_ratio) ti_df &lt;- tf_idf_df %&gt;% group_by(label = ifelse(label &gt; 0, &quot;긍정평&quot;, &quot;부정평&quot;)) %&gt;% slice_max(tf_idf, n = 10) %&gt;% select(label, word, score = tf_idf) %&gt;% mutate(score = score * 600) bind_rows(wlo_df, rlo_df, ti_df, .id = &quot;ID&quot;) %&gt;% tail(20) ## # A tibble: 20 × 4 ## # Groups: label [2] ## ID label word score ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 3 긍정평 밋 2.05 ## 2 3 긍정평 ㄷ 1.92 ## 3 3 긍정평 삶 1.41 ## 4 3 긍정평 만족 1.02 ## 5 3 긍정평 행복 0.897 ## 6 3 긍정평 뒤 0.769 ## # … with 14 more rows bind_rows(wlo_df, rlo_df, ti_df, .id = &quot;ID&quot;) %&gt;% mutate(ID = case_when(ID == &quot;1&quot; ~ &quot;1.가중로그승산비&quot;, ID == &quot;2&quot; ~ &quot;2.로그승산비&quot;, TRUE ~ &quot;3.tf_idf&quot;)) %&gt;% ggplot(aes(x = score, y = reorder_within(word, score, list(ID, label) ), fill = label) ) + geom_col(show.legend = F) + scale_y_reordered() + facet_wrap( ~ label+ID, scales = &quot;free_y&quot;, ncol = 3) 8.4 연습 ’백신’관련 보도의 내용을 빅카인즈의 자료를 이용해 탐색해보자. 8.4.1 빅카인즈 빅카인즈는 언론진흥재단이 운영하는 뉴스빅데이터 분석서비스다. 종합일간지, 경제지, 지역일간지, 방송사 등의 기사를 분석할수 인터페이스를 제공한다. 빅카인즈인터페이스를 통해 빅카인즈에서 제공하는 인터페이스으로 뉴스텍스트를 분석할 수 있다. 관계도, 키워드트렌드, 연관어분석 등이 가능하다. 사용자가 직접 분석할 수 있도록 데이터다운로드 서비스도 제공한다. 무료데이터는 일자, 언론사, 기고자, 제목, 분류, 인물, 위치, 본문(200자), 기사ULR 등이 포함돼 있다. 데이터다운로도 방법: 빅카인즈 접솝 상단 메뉴의 ‘뉴스분석’ 클릭 ‘뉴스분석’ 메뉴가 아래로 펼쳐지면 ‘뉴스·검색 분석’ 클릭. ‘STEP01·뉴스검색’ 창에서 다운로드할 기사의 범위 설정. ‘STEP03·분석결과 및 시각화’를 클릭하면 ’데이터다운로드’ 메뉴가 나온다. 오른쪽 하단의 ‘엑셀다운로드’ 클릭. 8.4.1.1 백신관련 보도 수집 모든 기사를 분석하기 위해서는 시간이 많이 걸리므로, 데이터셋의 크기를 줄이기 위해, 분석 대상을 2021년 2월 한달간 ‘경향신문’ ‘한겨레’ ‘문화일보’ ‘조선일보’ 등 4개 일간지에서 보도된 내용에 국한하자. 4. 'STEP01·뉴스검색' 창에서 다운로드할 기사의 범위 설정 검색어: ‘백신’ 기간: 2021년 2월 1일 ~ 2월 28일 통합분류: ‘사회’, ‘IT_과학’ 언론사: ‘경향신문’ ‘한겨레’ ‘문화일보’ ‘조선일보’ 선택 작업디렉토리 아래 data폴더에 다운로드 받는다. 파일명을 확인한다. NewsResult_20210201-20210228.xlsx다. list.files(&quot;data/.&quot;) readxl패키지의 read_excel()함수로 해당 파일을 R환경으로 이입한다. readxl::read_excel(&quot;data/NewsResult_20210201-20210228.xlsx&quot;) %&gt;% glimpse() 분석에 활용할 열만 추출해 vac_df에 할당한다. vac_df &lt;- readxl::read_excel(&quot;data/NewsResult_20210201-20210228.xlsx&quot;) %&gt;% select(제목, 언론사, 본문, URL) vac_df %&gt;% glimpse() ## Rows: 602 ## Columns: 4 ## $ 제목 &lt;chr&gt; \"백신 접종 3일째 ‘큰 탈’ 없어 곳곳서 방역… ## $ 언론사 &lt;chr&gt; \"경향신문\", \"경향신문\", \"경향신문\", \"경향신… ## $ 본문 &lt;chr&gt; \"이틀간 2만322명에 접종 마쳐 중증 이상반응 … ## $ URL &lt;chr&gt; \"http://news.khan.co.kr/kh_news/khan_art_vi… 8.4.2 정제 먼저 drop_na()함수로 결측값을 제거하고, 토큰화한 다음, pos함수로 일반명사만 추출해, vac_tk로 저장한다. vac_tk &lt;- vac_df %&gt;% drop_na() %&gt;% unnest_tokens(word, 제목, token = RcppMeCab::pos) %&gt;% separate(word, c(&quot;word&quot;, &quot;pos&quot;), sep = &quot;/&quot;) %&gt;% filter(pos == &#39;nng&#39;) vac_tk ## # A tibble: 3,633 × 5 ## 언론사 본문 URL word pos ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 경향신문 \"이틀간 2만322명에 접종 마쳐 … http:… 백신 nng ## 2 경향신문 \"이틀간 2만322명에 접종 마쳐 … http:… 접종 nng ## 3 경향신문 \"이틀간 2만322명에 접종 마쳐 … http:… 탈 nng ## 4 경향신문 \"이틀간 2만322명에 접종 마쳐 … http:… 곳곳 nng ## 5 경향신문 \"이틀간 2만322명에 접종 마쳐 … http:… 방역 nng ## 6 경향신문 \"이틀간 2만322명에 접종 마쳐 … http:… 해이 nng ## # … with 3,627 more rows 8.4.3 분석 및 소통 총빈도, 감정어빈도, tf_idf, 로그승산비, 가중로그승산비 등 다양한 분석 방법을 배웠다. 총빈도와 가중로그승산비를 구해 언론사별 어떤 차이가 있는지 살펴보자. (분석 결과를 해석할 때는 2021년 2월 한달 기간에 국한한 자료를 이용한 점에 주의해야 한다.) 8.4.4 언론사별 총빈도 vac_tk %&gt;% count(언론사, word, sort = TRUE) %&gt;% filter(word != &quot;경향&quot;) %&gt;% filter(word != &quot;포토&quot;) %&gt;% filter(word != &quot;문화&quot;) %&gt;% filter(word != &quot;조선&quot;) %&gt;% filter(word != &quot;한겨레&quot;) %&gt;% filter(word != &quot;백신&quot;) %&gt;% filter(word != &quot;접종&quot;) %&gt;% group_by(언론사) %&gt;% slice_max(n, n = 7) %&gt;% ggplot(aes(x = n, y = reorder_within(word, n, 언론사), fill = 언론사) ) + geom_col(show.legend = F) + scale_y_reordered() + facet_wrap( ~ 언론사, scales = &quot;free_y&quot;) 주요 단어가 사용된 내용(제목)을 살펴보자. vac_df %&gt;% drop_na() %&gt;% unnest_tokens(word, 제목, token = RcppMeCab::pos, drop = F) %&gt;% separate(word, c(&quot;word&quot;, &quot;pos&quot;), sep = &quot;/&quot;) %&gt;% filter(pos == &#39;nng&#39;) %&gt;% filter(word == &quot;카&quot;) %&gt;% pull(제목) ## [1] \"“독일, 65세 이상 아스트라제네카 백신 접종 곧 허가할 듯”\" ## [2] \"대통령은 아스트라제네카, 총리는 화이자 백신 접종 참관\" ## [3] \"접종 D-1 정세균 총리 “아스트라제네카 백신 의구심 근거 없어”\" ## [4] \"인천시, 26일부터 아스트라제네카 백신 접종 시작\" ## [5] \"아스트라제네카 26일 화이자 27일 접종 시작\" ## [6] \"‘스푸트니크-아스트라제네카 결합, 접종 기간 단축’\" ## [7] \"아스트라제네카 백신 접종 '동의한다' 93.8%\" ## [8] \"아스트라제네카 백신 국가출하승인 화이자는 3월초 허가 전망\" ## [9] \"호주, 아스트라제네카 백신 승인 “65세 이상도 사용 가능”\" ## [10] \"WHO, 아스트라제네카 백신 승인\" ## [11] \"[2 3월 백신접종]정부 \\\"아스트라제네카 백신 만 65세 이상 접종 보류\\\"\" ## [12] \"아스트라제네카, 65살 이상 고령자 접종 연기 3월말 이후 결정\" ## [13] \"아스트라제네카, 유럽 주요국도 65세 이상에겐 접종 제한\" ## [14] \"아스트라제네카 백신 고령 접종여부 오늘 발표\" ## [15] \"아스트라제네카(AZ) 백신 고령층 접종 금지한 스웨덴도 \\\"AZ 밖에 없으면 당연히 맞혀야\\\"\" ## [16] \"WHO 자문단 “아스트라제네카 백신, 65세 이상에게도 권고”\" ## [17] \"아스트라제네카 백신 ‘만 65세 이상도 맞나’...16일 발표\" ## [18] \"아스트라제네카 ‘모든 연령층 접종’ 허가 ‘65살 이상’은 의사가 판단\" ## [19] \"아스트라제네카 백신, 만 65세 고령층에도 접종 허가 식약처 \\\"현장에서 의사가 판단\\\"\" ## [20] \"아스트라제네카 백신 허가... 만 65세 이상 접종여부는 ‘불투명'\" ## [21] \"식약처, 아스트라제네카 백신 사용 허가 65살 이상도 포함\" ## [22] \"질병청 ”아스트라제네카 백신 26일부터 접종 시작”\" ## [23] \"26일 아스트라제네카 백신 접종 시작 요양병원 시설 먼저\" ## [24] \"“아스트라제네카 백신 75만명분, 24일부터 받아 곧바로 접종”\" ## [25] \"남아공, 아스트라제네카 백신 접종 보류 변이 바이러스에 효과 낮아\" ## [26] \"스위스 이어 남아공도 아스트라제네카 백신 접종 보류\" ## [27] \"“아스트라제네카 백신, 노인 접종에 신중해야”\" ## [28] \"식약처 “아스트라제네카 만 65세 접종 신중” 질병청에 공 넘겨\" ## [29] \"노인에 대한 아스트라제네카 백신 투여, 최종 결정은 질병청에 넘겨\" ## [30] \"식약처 “아스트라제네카, 65살 이상 접종 신중하게”\" ## [31] \"불투명해진 아스트라제네카 백신 고령층 접종 중앙약심위 \\\"만 65세 이상 접종 신중해야\\\"\" ## [32] \"아스트라제네카 고령층 접종 제한하는 유럽 스위스는 “승인 보류”\" ## [33] \"아스트라제네카 고령층 백신 효과 두고 유럽 시끌, 스위스는 승인 보류\" ## [34] \"아스트라제네카 백신 논란 지속 스위스, 승인 보류\" ## [35] \"스위스, 아스트라제네카 백신 승인 거부\" ## [36] \"유럽 7개국, 아스트라제네카 고령자 접종 제한 우리나라는?\" ## [37] \"프랑스, 스웨덴도 아스트라제네카 백신 65세 미만에만 접종 권고\" ## [38] \"[만물상] 말 많고 탈 많은 아스트라제네카\" ## [39] \"아스트라제네카 접종, 65세 이상 고령층 포함 가능성 커졌다\" ## [40] \"아스트라제네카 백신 ‘무용론’ 딛고 65세 이상도 접종할 듯\" ## [41] \"[속보] 아스트라제네카 백신 예방효과 62% “고령층 접종 배제할 필요 없다”\" ## [42] \"“아스트라제네카, 고령층 투여 배제할 수 없어” 1차 자문 결과\" ## [43] \"아스트라제네카, 조건부로 허용되나?\" ’카’의 빈도가 높았던 이유는 ’아스트라제네카’를 형태소분석기가 하나의 일반명사로 추출하지 않았기 때문이다. enc2utf8(&quot;아스트라제네카 ‘모든 연령층 접종’ 허가 ‘65살 이상’은 의사가 판단&quot;) %&gt;% RcppMeCab::pos() ## $`아스트라제네카 ‘모든 연령층 접종’ 허가 ‘65살 이상’은 의사가 판단` ## [1] \"아스트라/NNP\" \"제/NP\" \"네/XSN\" ## [4] \"카/NNG\" \"‘/SY\" \"모든/MM\" ## [7] \"연령/NNG\" \"층/XSN\" \"접종/NNG\" ## [10] \"’/SY\" \"허가/NNG\" \"‘/SY\" ## [13] \"65/SN\" \"살/NNBC\" \"이상/NNG\" ## [16] \"’/SY\" \"은/JX\" \"의사/NNG\" ## [19] \"가/JKS\" \"판단/NNG\" vac_df %&gt;% drop_na() %&gt;% unnest_tokens(word, 제목, token = RcppMeCab::pos, drop = F) %&gt;% separate(word, c(&quot;word&quot;, &quot;pos&quot;), sep = &quot;/&quot;) %&gt;% filter(pos == &#39;nng&#39;) %&gt;% filter(word == &quot;이상&quot;) %&gt;% pull(제목) ## [1] \"독일, \\\"65세 이상 AZ백신 접종 검토\\\"\" ## [2] \"27일 백신 이상반응 97건 두통 발열 등 모두 경증\" ## [3] \"“독일, 65세 이상 아스트라제네카 백신 접종 곧 허가할 듯”\" ## [4] \"[속보] 코로나 백신접종 첫날 이상반응 15건 두통 발열 구토 등 경증\" ## [5] \"백신 맞은 인천 간호사 2명도 이상증세... 숨차고 혈압 올라 병원행\" ## [6] \"포항서 아스트라 첫 이상증세 고혈압 어지럼증에 응급실 이송\" ## [7] \"백신 맞았다면 최소 15분이상 대기 ‘이상반응’ 관찰해야\" ## [8] \"백신 맞았다면 최소 15분이상 대기 ‘이상반응’ 관찰해야\" ## [9] \"중앙약심 “화이자 백신, 16세 이상 대상으로 접종 허가 타당”\" ## [10] \"2차 자문에서도 화이자 백신 \\\"예방효과 안전성 충분... 16세 이상 허가 권고\\\"\" ## [11] \"서울 65세 이상 성인은 4월이후 코로나 백신 맞는다\" ## [12] \"시민 10명 중 7명 \\\"금고형 이상 범죄, 의사면허 취소 찬성\\\"\" ## [13] \"“아스트라, 65세 이상도 입원 위험 80% 감소”\" ## [14] \"정 총리 “65세 이상 고령층에 화이자 백신 먼저 접종 가능성”\" ## [15] \"식약처 “화이자 백신 16살 이상 사용 타당”\" ## [16] \"식약처 전문가 검증 자문단 “화이자 백신, 16살 이상 사용 허가 타당”\" ## [17] \"식약처 자문단 “화이자 백신, 예방 효과 충분 16세 이상 접종도 타당”\" ## [18] \"“화이자 백신 예방효과 충분 16세 이상 허가 타당”\" ## [19] \"졸업사진 찍을땐 5명 이상도 가능\" ## [20] \"호주, 아스트라제네카 백신 승인 “65세 이상도 사용 가능”\" ## [21] \"65세 이상, 아스트라 백신 접종 연기\" ## [22] \"아스트라 백신 접종, 65살 이상 일단 보류 3월말께 재결정\" ## [23] \"[2 3월 백신접종]정부 \\\"아스트라제네카 백신 만 65세 이상 접종 보류\\\"\" ## [24] \"정은경 “65세 이상 접종 미루게 돼 안타까워”\" ## [25] \"아스트라제네카, 65살 이상 고령자 접종 연기 3월말 이후 결정\" ## [26] \"아스트라제네카, 유럽 주요국도 65세 이상에겐 접종 제한\" ## [27] \"AZ백신 ‘65세이상 접종’ 일단 후순위로\" ## [28] \"백신접종 계획 내일 발표 65세 이상도 아스트라 맞나\" ## [29] \"수도권 영업 오후10시까지, 5인 이상 모임 금지는 유지\" ## [30] \"WHO 자문단 “아스트라제네카 백신, 65세 이상에게도 권고”\" ## [31] \"아스트라제네카 백신 ‘만 65세 이상도 맞나’...16일 발표\" ## [32] \"아스트라제네카 ‘모든 연령층 접종’ 허가 ‘65살 이상’은 의사가 판단\" ## [33] \"아스트라제네카 백신 허가... 만 65세 이상 접종여부는 ‘불투명'\" ## [34] \"식약처, 아스트라제네카 백신 사용 허가 65살 이상도 포함\" ## [35] \"65세 이상 고령층 대신 경찰 군인부터 백신 접종순위 바뀔 가능성\" ## [36] \"식약처 “아스트라제네카, 65살 이상 접종 신중하게”\" ## [37] \"불투명해진 아스트라제네카 백신 고령층 접종 중앙약심위 \\\"만 65세 이상 접종 신중해야\\\"\" ## [38] \"아스트라 백신 65세이상 접종 결정보류 일정 차질 불가피\" ## [39] \"식약처 자문단 “화이자 백신, 만 16세 이상도 접종 타당”\" ## [40] \"아스트라제네카 접종, 65세 이상 고령층 포함 가능성 커졌다\" ## [41] \"의협회장 “만65세이상은 아스트라 백신 안돼, 화이자 모더나 접종을”\" ## [42] \"아스트라제네카 백신 ‘무용론’ 딛고 65세 이상도 접종할 듯\" ## [43] \"식약처 자문단, 아스트라 65세 이상 접종 결론 못내...다수 의견은 “가능”\" ## [44] \"단속 힘들고 불편만 ‘5인 이상 가족모임 금지’ 유지 논란\" ## [45] \"[사이언스카페] 코로나 백신 2회중 1회만 맞아도 감염 전염력 60% 이상 떨어뜨려\" ## [46] \"설 차례, 직계가족이라도 주소 다르면 5인이상 금지\" 8.4.5 언론사별 가중로그승산비 언론사별 가중로그승산비를 구하고 이를 시각화하여 비교해보자. vac_tk %&gt;% count(언론사, word) %&gt;% filter(word != &quot;경향&quot;) %&gt;% filter(word != &quot;포토&quot;) %&gt;% filter(word != &quot;문화&quot;) %&gt;% filter(word != &quot;조선&quot;) %&gt;% filter(word != &quot;한겨레&quot;) %&gt;% filter(word != &quot;백신&quot;) %&gt;% filter(word != &quot;접종&quot;) %&gt;% bind_log_odds(set = 언론사, feature = word, n = n) %&gt;% group_by(언론사) %&gt;% slice_max(abs(log_odds_weighted), n = 7) %&gt;% ggplot(aes(x = log_odds_weighted, y = reorder(word, log_odds_weighted), fill = 언론사) ) + geom_col(show.legend = F) + facet_wrap( ~ 언론사, scales = &quot;free&quot;) 주요 단어가 사용된 내용(제목)을 살펴보자. vac_df %&gt;% drop_na() %&gt;% unnest_tokens(word, 제목, token = RcppMeCab::pos, drop = F) %&gt;% separate(word, c(&quot;word&quot;, &quot;pos&quot;), sep = &quot;/&quot;) %&gt;% filter(pos == &#39;nng&#39;) %&gt;% filter(언론사 == &quot;경향신문&quot;) %&gt;% filter(word == &quot;현장&quot;) %&gt;% pull(제목) ## [1] \"문 대통령, 접종 현장 방문 후 “일상 회복 머지않았다”\" ## [2] \"'긴장 탓 울렁' 했지만 \\\"백신 접종 기쁠 뿐\\\" 첫 날 백신접종 현장은\" ## [3] \"국내 첫 백신 접종 시작 문 대통령 접종 현장 참관\" ## [4] \"아스트라제네카 백신, 만 65세 고령층에도 접종 허가 식약처 \\\"현장에서 의사가 판단\\\"\" ## [5] \"[오늘은 이런 경향] 2월 9일 정 총리 답신 한달 지냈지만...현장 간호사들은...\" 품사로 명사(nng), 언론사로 “문화일보”, 단어 “확정”이 들어간 신문기사 제목을 추출해보자. vac_df %&gt;% drop_na() %&gt;% unnest_tokens(word, 제목, token = RcppMeCab::pos, drop = F) %&gt;% separate(word, c(&quot;word&quot;, &quot;pos&quot;), sep = &quot;/&quot;) %&gt;% filter(pos == &#39;nng&#39;) %&gt;% filter(언론사 == &quot;문화일보&quot;) %&gt;% filter(word == &quot;확정&quot;) %&gt;% pull(제목) ## [1] \"백신 접종 D-7 명단 확정 65세미만 27만명으로 줄어\" ## [2] \"국내 1호 AZ백신 첫 접종 대상은 누구? 19일까지 명단 확정\" 품사로 명사(nng), 언론사로 “조선일보”, 단어 “국민”이 들어간 신문기사 제목을 추출해보자. vac_df %&gt;% drop_na() %&gt;% unnest_tokens(word, 제목, token = RcppMeCab::pos, drop = F) %&gt;% separate(word, c(&quot;word&quot;, &quot;pos&quot;), sep = &quot;/&quot;) %&gt;% filter(pos == &#39;nng&#39;) %&gt;% filter(언론사 == &quot;조선일보&quot;) %&gt;% filter(word == &quot;국민&quot;) %&gt;% pull(제목) ## [1] \"文 “백신 관리 접종 과정, 국민 신뢰 주기에 충분”\" ## [2] \"국민 열에 일곱은 “코로나 백신 접종할 것”\" ## [3] \"백신 불안감 여전 국민 30% “안 맞거나 미룰래요”\" ## [4] \"국민 41%가 백신 맞았다, 이스라엘 제치고 세계 1위된 이 나라\" ## [5] \"“과학자 입 막으면 국민 건강권 침해” 기자협회 과학기자협회 공동 발표\" 품사로 명사(nng), 언론사로 “한겨레”, 단어 “뒤”가 들어간 신문기사 제목을 추출해보자. vac_df %&gt;% drop_na() %&gt;% unnest_tokens(word, 제목, token = RcppMeCab::pos, drop = F) %&gt;% separate(word, c(&quot;word&quot;, &quot;pos&quot;), sep = &quot;/&quot;) %&gt;% filter(pos == &#39;nng&#39;) %&gt;% filter(언론사 == &quot;한겨레&quot;) %&gt;% filter(word == &quot;뒤&quot;) %&gt;% pull(제목) ## [1] \"백신 ‘1호 접종’은 야간근무 뒤 일찍 온 요양보호사\" ## [2] \"통합관제센터서 “온도 벗어났다” 제주행 백신 전량 회수 뒤 교체\" ## [3] \"백신접종 뒤 입원 위험, 아스트라가 화이자보다 낮았다\" ## [4] \"예진 통과하면 간호사 4명 배치된 주사실로 접종 뒤 15~30분 관찰\" 8.4.6 총빈도와 상대빈도 비교 언론사별로 각 기사에서 사용된 단어의 총빈도와 상대빈도를 비교하자. 먼저 총빈도를 구한다. total_fq &lt;- vac_tk %&gt;% count(언론사, word, sort = TRUE) %&gt;% filter(word != &quot;경향&quot;) %&gt;% filter(word != &quot;포토&quot;) %&gt;% filter(word != &quot;문화&quot;) %&gt;% filter(word != &quot;조선&quot;) %&gt;% filter(word != &quot;한겨레&quot;) %&gt;% filter(word != &quot;백신&quot;) %&gt;% filter(word != &quot;접종&quot;) %&gt;% group_by(언론사) %&gt;% slice_max(n, n = 7) total_fq ## # A tibble: 32 × 3 ## # Groups: 언론사 [4] ## 언론사 word n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 경향신문 코로나 23 ## 2 경향신문 오늘 19 ## 3 경향신문 카 14 ## 4 경향신문 시작 13 ## 5 경향신문 고령 10 ## 6 경향신문 이상 10 ## # … with 26 more rows 이번에는 상대빈도를 구한다. wlo_fq &lt;- vac_tk %&gt;% count(언론사, word) %&gt;% filter(word != &quot;경향&quot;) %&gt;% filter(word != &quot;포토&quot;) %&gt;% filter(word != &quot;문화&quot;) %&gt;% filter(word != &quot;조선&quot;) %&gt;% filter(word != &quot;한겨레&quot;) %&gt;% filter(word != &quot;백신&quot;) %&gt;% filter(word != &quot;접종&quot;) %&gt;% bind_log_odds(set = 언론사, feature = word, n = n) %&gt;% group_by(언론사) %&gt;% slice_max(abs(log_odds_weighted), n = 7) wlo_fq ## # A tibble: 58 × 4 ## # Groups: 언론사 [4] ## 언론사 word n log_odds_weighted ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 경향신문 현장 5 2.76 ## 2 경향신문 국산 4 2.47 ## 3 경향신문 오늘 19 2.44 ## 4 경향신문 사회 3 2.14 ## 5 경향신문 삶 3 2.14 ## 6 경향신문 입소자 3 2.14 ## # … with 52 more rows 총빈도와 상대빈도 데이터프레임을 행방향으로 결합한다. bind_rows( select(total_fq, 언론사, word, score = n), select(wlo_fq, 언론사, word, score = log_odds_weighted), .id = &quot;ID&quot;) %&gt;% mutate(ID = ifelse(ID == 1, &quot;total&quot;, &quot;wlo&quot;)) %&gt;% ungroup() %&gt;% ggplot(aes(x = reorder_within(word, score, list(ID, 언론사)), y = score, fill = 언론사) ) + geom_col(show.legend = FALSE) + scale_x_reordered() + facet_wrap(~ID+언론사, scales = &quot;free_y&quot;, ncol = 4) + coord_flip() 전반적으로 4개 언론사는 백신과 관련하여 2021년 2월 한달간 ‘이상’(특정 연령 이상)과 아스트라제네카 백신에 대한 내용을 주로 다뤘다. 언론사별로는 경향신문은 ‘오늘’과 ’현장’, 문화일보는 ‘미국’과 ’누적’, 조선일보는 ‘국민’과 ’속도’, 한겨레는 ‘뒤’와 ’팔’ 등에 관련된 기사를 다른 언론사에 비해 상대적으로 더 다뤘다. 8.5 과제 빅카인즈에서 언론사별로 관심 분야의 기사를 추출해 보도 내용을 분석할 수 있도록 시각화하시오. 보도에 사용된 단어의 총빈도에 대해 선택한 언론사별 비교 시각화 보도에 사용된 단어의 상대빈도에 대해 선택한 언론사별 비교 시각화 총빈도와 상대빈도에 대해 언론사별로 비교 시각화 "],["sentiment.html", "9 . 감성분석 9.1 감정어 빈도 9.2 영문 감정사전 9.3 연습 9.4 소설 감정분석", " 9 . 감성분석 9.1 감정어 빈도 단어가 모여 문서가 되고 문서를 모아 말뭉치를 구성한다. 문서나 말뭉치에 내재된 감성(sentiment)을 찾아내기 위해 크게 3가지 방식의 접근법이 많이 활용되고 있다. 사전방식: 말뭉치에서 많이 사용된 일군의 단어집단의 빈도 계산. 감정사전을 이용하면 감정어 빈도를 구할 수 있다. tf_idf: 말뭉치의 문서별로 중요하게 사용된 단어의 빈도 계산 주제모형(topic modeling): 말뭉치에서 주제를 나타내는 단어의 분포 계산 사전방식은 말뭉치에서 많이 사용된 일군의 단어를 사전으로 만들어 해당 단어 집단의 빈도 계산. 감정사전: 각 단어를 감정에 따라 분류 도덕기반사전: 각 단어를 도덕감정에 따라 분류 LIWC(Linguistic Inquiry and Word Count): 일상에서 사용하는 단어를 통해 생각, 느낌 등의 심리 측정. 상용. 9.2 영문 감정사전 tidytext 팩키지에 감성어 사전(sentiment lexicon)이 3개 포함되어 있다. get_sentiments() 함수에 다음 감성어 사전을 인자로 넣어 깔끔한 텍스트 데이터프레밍과 inner_join하여 텍스트에 포하된 감성을 계량화한다. afinn: 개발자 Finn Årup Nielsen bing: 개발자 Bing Liu와 기여자 nrc: 개발자 Saif Mohammad, Peter Turney 텍스트 데이터를 토큰화 과정을 거쳐 데이터프레임으로 변환시키고 나서 영문 사전과 결합해서 감성 총합을 계산한다. 그리고 나서 ggplot을 통해 시각화하는 것이 일반적인 작업흐름이다. ## 국문 감정사전 KNU한국어감성사전은 군산대 소프트웨어융합공학과 Data Intelligence Lab에서 개발한 한국어감정사전이다. 표준국어대사전을 구성하는 각 단어의 뜻풀이를 분석하여 긍부정어를 추출했다. 보편적인 기본 감정 표현을 나타내는 긍부정어로 구성된다. 보편적인 긍정 표현으로는 ‘감동받다’, ‘가치 있다’, ‘감사하다’와 보편적인 부정 표현으로는 ‘그저 그렇다’, ‘도저히 ~수 없다’, ‘열 받다’ 등이 있다. 이 사전을 토대로 각 영역(음식, 여행지 등)별 감정사전을 구축하는 기초 자료로 활용할 수 있다. KNU한국어감성사전은 다음 자료를 통합해 개발했다. 국립국어원 표준국어대사전의 뜻풀이(glosses) 분석을 통한 긍부정 추출(이 방법을 통해 대부분의 긍부정어 추출) 김은영(2004)의 긍부정어 목록 SentiWordNet 및 SenticNet-5.0에서 주로 사용되는 긍부정어 번역 최근 온라인에서 많이 사용되는 축약어 및 긍부정 이모티콘 목록 위 자료를 통합해 총 1만4천여개의 1-gram, 2-gram, 관용구, 문형, 축약어, 이모티콘 등에 대한 긍정, 중립, 부정 판별 및 정도(degree)의 값 계산했다. pkg_v &lt;- c(&quot;tidyverse&quot;, &quot;tidytext&quot;, &quot;epubr&quot;, &quot;RcppMeCab&quot;) lapply(pkg_v, require, ch = T) ## [[1]] ## [1] TRUE ## ## [[2]] ## [1] TRUE ## ## [[3]] ## [1] TRUE ## ## [[4]] ## [1] TRUE 9.2.1 사전 데이터프레임 만들기 KNU한국어감정사전을 다운로드 받아 압축을 풀어 데이터프레임 “senti_dic_df”에 할당하자. 먼저 파일을 data폴더에 다운로드해 knusenti.zip파일로 저장한다. url_v &lt;- &quot;https://github.com/park1200656/KnuSentiLex/archive/refs/heads/master.zip&quot; dest_v &lt;- &quot;data/knusenti.zip&quot; download.file(url = url_v, destfile = dest_v, mode = &quot;wb&quot;) list.files(&quot;data/.&quot;) 다운로드받은 knusenti.zip파일을 압축해제해 필요한 사전자료의 위치를 파악한다. data/KnuSentiLex-master 폴더에 있는 파일의 종류를 탐색한다. unzip(&quot;data/knusenti.zip&quot;, exdir = &quot;data&quot;) list.files(&quot;data/KnuSentiLex-master/&quot;) SentiWord_Dict.txt가 사전내용이 들어있는 파일이다. 9번째 있으므로, 이를 선택해 R환경으로 이입하자. list.files() 함수는 지정된 디렉토리 파일과 디렉토리 정보를 알려준다. full.names = TRUE 인자를 함께 넣어주면 파일과 디렉토리를 결합하여 주기 때문에 경우에 따라 타이핑을 줄일 수 있다. library(tidyverse) senti_file_list &lt;- list.files(&quot;data/KnuSentiLex-master/&quot;, full.names = TRUE) read_lines(senti_file_list[9]) %&gt;% head(10) ## [1] \"(-;\\t1\" \"(;_;)\\t-1\" \"(^^)\\t1\" \"(^-^)\\t1\" ## [5] \"(^^*\\t1\" \"(^_^)\\t1\" \"(^_^;\\t-1\" \"(^o^)\\t1\" ## [9] \"(-_-)\\t-1\" \"(T_T)\\t-1\" 개별 요소와 요소 사이가 공백문자로 구분돼 있다. read_lines(senti_file_list[9]) %&gt;% head(10) %&gt;% str_extract(&quot;\\t|\\n| &quot;) 요소를 구분한 공백문자는 탭(\\t)이다. read_tsv로 이입해 내용을 검토하자. read_tsv(senti_file_list[9]) %&gt;% head(10) read_tsv(senti_file_list[9], col_names = FALSE) %&gt;% head(10) senti_dic_df에 할당하자. glimpse() 함수로 감성사전 데이터프레임을 살펴보고, 첫 5행을 출력해서 함께 살펴본다. senti_dic_df &lt;- read_tsv(senti_file_list[9], col_names = FALSE) glimpse(senti_dic_df) ## Rows: 14,855 ## Columns: 2 ## $ X1 &lt;chr&gt; \"(-;\", \"(;_;)\", \"(^^)\", \"(^-^)\", \"(^^*\", \"(^_^)… ## $ X2 &lt;dbl&gt; 1, -1, 1, 1, 1, 1, -1, 1, -1, -1, -1, -1, 1, 1,… senti_dic_df %&gt;% slice(1:5) ## # A tibble: 5 × 2 ## X1 X2 ## &lt;chr&gt; &lt;dbl&gt; ## 1 (-; 1 ## 2 (;_;) -1 ## 3 (^^) 1 ## 4 (^-^) 1 ## 5 (^^* 1 열의 이름을 내용에 맞게 변경하자. 감정단어의 열은 word로, 감정점수의 열은 sScore로 변경. senti_dic_df &lt;- senti_dic_df %&gt;% rename(word = X1, sScore = X2) glimpse(senti_dic_df) ## Rows: 14,855 ## Columns: 2 ## $ word &lt;chr&gt; \"(-;\", \"(;_;)\", \"(^^)\", \"(^-^)\", \"(^^*\", \"(… ## $ sScore &lt;dbl&gt; 1, -1, 1, 1, 1, 1, -1, 1, -1, -1, -1, -1, 1… 이제 KNU한국어감성사전을 R환경에서 사용할 수 있다. 9.2.2 사전내용 KNU감성사전에 포함된 긍정단어와 부정단어를 살펴보자. senti_dic_df %&gt;% filter(sScore == 2) %&gt;% arrange(word) ## # A tibble: 2,603 × 2 ## word sScore ## &lt;chr&gt; &lt;dbl&gt; ## 1 가능성이 늘어나다 2 ## 2 가능성이 있다고 2 ## 3 가능하다 2 ## 4 가볍고 상쾌하다 2 ## 5 가볍고 상쾌한 2 ## 6 가볍고 시원하게 2 ## # … with 2,597 more rows senti_dic_df %&gt;% filter(sScore == -2) %&gt;% arrange(word) ## # A tibble: 4,799 × 2 ## word sScore ## &lt;chr&gt; &lt;dbl&gt; ## 1 가난 -2 ## 2 가난뱅이 -2 ## 3 가난살이 -2 ## 4 가난살이하다 -2 ## 5 가난설음 -2 ## 6 가난에 -2 ## # … with 4,793 more rows 각 감정점수별로 단어의 수를 계산해보자. 텍스트를 데이터프레임으로 변환하는 과정에서 “갈등 -1” 단어가 제대로 파싱되어 데이터프레임에 반영되지 않아 이를 바로잡는 코드를 추가한다. knu_dic_df &lt;- senti_dic_df %&gt;% mutate(word = ifelse( is.na(sScore), &quot;갈등&quot;, word), sScore = ifelse( is.na(sScore), -1, sScore) ) knu_dic_df %&gt;% count(sScore) ## # A tibble: 5 × 2 ## sScore n ## &lt;dbl&gt; &lt;int&gt; ## 1 -2 4799 ## 2 -1 5031 ## 3 0 154 ## 4 1 2268 ## 5 2 2603 감정점수가 -2 ~ 2까지 정수로 부여돼 있다. 이를 3단계로 줄여 각각 ’긍정, 중립, 부정’으로 명칭을 바꾸자. knu_dic_df %&gt;% mutate(emotion = case_when( sScore &gt;= 1 ~ &quot;positive&quot;, sScore &lt;= -1 ~ &quot;negative&quot;, TRUE ~ &quot;neutral&quot;)) %&gt;% count(emotion) ## # A tibble: 3 × 2 ## emotion n ## &lt;chr&gt; &lt;int&gt; ## 1 negative 9830 ## 2 neutral 154 ## 3 positive 4871 원데이터 sScore 열에 결측값이 한 행에 있다. 감정단어와 감정값 사이에 \\t이 아닌 기호로 분리돼 있었기 때문이다. 결측값이 있는 행을 제거하고, add_row() 함수를 사용해서 해당 내용을 추가하는 방법도 있다. knu_dic_df &lt;- read_tsv(senti_file_list[9], col_names = FALSE) %&gt;% rename(word = X1, sScore = X2) %&gt;% filter(!is.na(sScore)) %&gt;% add_row(word = &quot;갈등&quot;, sScore = -1) 9.3 연습 헌번전문을 토큰화한 다음, 긍정어와 부정어의 빈도를 계산하려고 한다. 이를 위해 토큰화한 헌법전문과 KNU한국어감성사전을 결합해 감정 단어만 추려내려 한다. 다음 중 어느 결합을 이용해야 하는가? 왜? semi_join anti_join inner_join left_join right_join full_join 위에서 선택한 _join으로 KNU한국어감성사전을 이용해 헌법전문에서 긍정어와 부정어를 추출하자. library(tidytext) library(tidyverse) con_v &lt;- &quot;유구한 역사와 전통에 빛나는 우리 대한국민은 3·1운동으로 건립된 대한민국임시정부의 법통과 불의에 항거한 4·19민주이념을 계승하고, 조국의 민주개혁과 평화적 통일의 사명에 입각하여 정의·인도와 동포애로써 민족의 단결을 공고히 하고, 모든 사회적 폐습과 불의를 타파하며, 자율과 조화를 바탕으로 자유민주적 기본질서를 더욱 확고히 하여 정치·경제·사회·문화의 모든 영역에 있어서 각인의 기회를 균등히 하고, 능력을 최고도로 발휘하게 하며, 자유와 권리에 따르는 책임과 의무를 완수하게 하여, 안으로는 국민생활의 균등한 향상을 기하고 밖으로는 항구적인 세계평화와 인류공영에 이바지함으로써 우리들과 우리들의 자손의 안전과 자유와 행복을 영원히 확보할 것을 다짐하면서 1948년 7월 12일에 제정되고 8차에 걸쳐 개정된 헌법을 이제 국회의 의결을 거쳐 국민투표에 의하여 개정한다.&quot; tibble(text = con_v) %&gt;% unnest_tokens(output = word, input = text) %&gt;% inner_join(knu_dic_df) ## # A tibble: 2 × 2 ## word sScore ## &lt;chr&gt; &lt;dbl&gt; ## 1 조화를 2 ## 2 행복을 2 형태소를 추출해 분석해보자. RcppMeCab 사용 con_v %&gt;% enc2utf8 %&gt;% tibble(text = .) %&gt;% unnest_tokens(output = word, input = text, token = RcppMeCab::pos) %&gt;% separate(col = word, into = c(&quot;word&quot;, &quot;morph&quot;), sep = &quot;/&quot;) %&gt;% inner_join(knu_dic_df) ## # A tibble: 5 × 3 ## word morph sScore ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 조화 nng 2 ## 2 능력 nng 1 ## 3 향상 nng 1 ## 4 안전 nng 2 ## 5 행복 nng 2 헌법 전문의 감정점수를 구해보자. con_v %&gt;% enc2utf8 %&gt;% tibble(text = .) %&gt;% unnest_tokens(output = word, input = text, token = RcppMeCab::pos) %&gt;% separate(col = word, into = c(&quot;word&quot;, &quot;morph&quot;), sep = &quot;/&quot;) %&gt;% inner_join(knu_dic_df) %&gt;% mutate(sScore = ifelse(is.na(sScore), 0, sScore)) %&gt;% summarise(score = sum(sScore)) ## # A tibble: 1 × 1 ## score ## &lt;dbl&gt; ## 1 8 9.4 소설 감정분석 소설과 같은 긴 문서를 데이터로 감성분석 작업을 수행한다. 9.4.1 수집 한글소설을 모아놓은 직지프로젝트 파일을 다운로드 받는다. dir.create(&quot;data&quot;) file_url &lt;- &#39;https://www.dropbox.com/s/ug3pi74dnxb66wj/jikji.epub?dl=1&#39; download.file(url = file_url, destfile = &quot;data/jikji.epub&quot;, mode=&quot;wb&quot;) 다운로드받은 epub파일을 R환경으로 이입해 데이터구조를 살핀다. library(epubr) epub(&quot;data/jikji.epub&quot;) %&gt;% glimpse() ## Rows: 1 ## Columns: 8 ## $ identifier &lt;chr&gt; \"urn:uuid:15037ed1-7721-4f33-8534-f6a74… ## $ title &lt;chr&gt; \"직지 프로젝트|직지프로텍트\" ## $ creator &lt;chr&gt; \"수학방\" ## $ language &lt;chr&gt; \"ko\" ## $ date &lt;chr&gt; \"2013-05-10\" ## $ rights &lt;chr&gt; \"크리에이티브 커먼즈 저작자표시-비영리-… ## $ source &lt;chr&gt; \"직지프로젝트(http://www.jikji.org)\" ## $ data &lt;list&gt; [&lt;tbl_df[93 x 4]&gt;] 데이터는 리스트구조의 data열 안의 93행 4열의 데이터프레임에 저장돼 있다. data 열만 선택하고 unnest() 함수로 리스트를 풀어 일반적인 데이터프레임으로 변환한 뒤에 글자수 기준으로 역정렬한다. jikji_epub &lt;- epub(&quot;data/jikji.epub&quot;) jikji_epub %&gt;% select(data) %&gt;% unnest(data) %&gt;% arrange( desc(nchar) ) ## # A tibble: 93 × 4 ## section text nword nchar ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 x1.xhtml \"12월 12일(十二月 十二日)\\n\\n 지… 449 102011 ## 2 x9.xhtml \"구운몽(완판 105장본) 현대문\\n\\n … 15 86078 ## 3 x90.xhtml \"혈의 누(血の淚)\\n\\n 지은이: 이인… 95 61151 ## 4 x84.xhtml \"타락자 (墮落者)\\n\\n 지은이: 현진… 130 51379 ## 5 x78.xhtml \"지형근(池亨根)\\n\\n 지은이: 나도… 25 36357 ## 6 x85.xhtml \"퇴별가 -완판본-\\n\\n 이 글은 인권… 8 32444 ## # … with 87 more rows 9.4.2 정제 text열에 93편의 소설 본문이 들어있다. 자료구조가 데이터프레임을 확장한 티블(tibble)이라 후속 텍스트 정제나 분석작업에 필요한 소설이 텍스트 형태로 어떻게 담겨있고 몇개나 있는지 쉽게 파악된다. novel_raw &lt;- jikji_epub %&gt;% select(data) %&gt;% unnest(data) %&gt;% select(text) novel_raw ## # A tibble: 93 × 1 ## text ## &lt;chr&gt; ## 1 \"17원 50전(十七圓五十錢)--- 젋은 화가 A의 눈물 한 방울\\n\\… ## 2 \"B사감(舍監)과 러브레터\\n\\n 지은이: 현진건\\n\\n 출전: … ## 3 \"경희(瓊姬)\\n\\n 지은이: 나혜석\\n\\n 출판사: 여자계1\\n\\… ## 4 \"계집 하인\\n\\n 지은이: 나도향\\n\\n 출전: 조선 문단 5월… ## 5 \"고향\\n\\n 지은이: 현진건\\n\\n 출전: 조선의 얼굴 &lt;1926.… ## 6 \"공포(恐怖)의 기록(記錄)\\n\\n 지은이: 이상\\n\\n 출전: … ## # … with 87 more rows 소설 제목, 지은이, 출전, 본문을 추출해보자. transmute() 함수는 mutate() 함수와 동일한 기능을 수행하나 변환작업에 관련된 변수만 남긴다는 점에서 이번 작업에 적합하다. 제목, 지은이 등 변수를 text 칼럼을 원자료로 삼아 str_extract() 함수와 정규표현식을 결합하여 필요한 정보만 추출한다. novel_tbl &lt;- novel_raw %&gt;% transmute(제목 = str_extract(text, &quot;.*\\\\b&quot;), 지은이 = str_extract(text, &quot;지은이.*\\\\b&quot;) %&gt;% str_remove(&quot;지은이: &quot;), 출전 = str_extract(text, &quot;출전.*\\\\b&quot;) %&gt;% str_remove(&quot;:&quot;) %&gt;% str_remove(&quot;\\\\)&quot;) %&gt;% str_remove(&quot;출전 &quot;), 본문 = str_squish(text) %&gt;% str_extract(&quot;본문.*&quot;) %&gt;% str_remove(&quot;본문|:&quot;)) novel_tbl ## # A tibble: 93 × 4 ## 제목 지은이 출전 본문 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 17원 50전(十七圓五十錢)--- 젋은 화가 A… 나도향 개벽… \" 첫… ## 2 B사감(舍監)과 러브레터 현진건 조선… \" 여… ## 3 경희(瓊姬 나혜석 NA \" 1 … ## 4 계집 하인 나도향 조선… \" 1 … ## 5 고향 현진건 조선… \" 대… ## 6 공포(恐怖)의 기록(記錄 이상 매일… \" 서… ## # … with 87 more rows 데이터프레임에 저장하자. 마지막 4행은 소설본문이 아니므로 제외하고 저장. sosul_df &lt;- novel_tbl %&gt;% rename(title = 제목, author = 지은이, source = 출전, main = 본문) %&gt;% slice(1:89) sosul_df %&gt;% glimpse() ## Rows: 89 ## Columns: 4 ## $ title &lt;chr&gt; \"17원 50전(十七圓五十錢)--- 젋은 화가 A의 … ## $ author &lt;chr&gt; \"나도향\", \"현진건\", \"나혜석\", \"나도향\", \"현… ## $ source &lt;chr&gt; \"개벽, &lt;1923\", \"조선문단 5호 &lt;1925\", NA, \"… ## $ main &lt;chr&gt; \" 첫 째 사랑하시는 C선생님께 어린 심정에서 … 비슷한 길이의 소설 1편씩 추출해보자. sosul2_df &lt;- sosul_df %&gt;% slice(c(9:10)) sosul2_df ## # A tibble: 2 × 4 ## title author source main ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 권태 이상 NA \" 1 어서, 차라리 어둬버… ## 2 규원 나혜석 신가정1, 1921년 7월 \" 때는 정히 오월 중순이… 9번째와 10번째 소설의 길이가 둘다 21000자로 비슷하다. 이상의 ’권태’와 나혜석의 ’규원’이다. 두 편의 소설은 웹에서도 볼수 있다. 이상의 ‘권태’ 일제식민지 시기 지식인의 무기력함과 좌절을 여름날 벽촌에서의 권태로운 생활에 대한 묘사를 통해 나타낸 수필. 나혜석의 ‘규원’ 1921년 여성 최초의 서양화가이자 소설가 나혜석의 단편소설. 양반집 규수였던 여인이 사랑하는 이에에 버림받은 한을 여성 특유의 섬세한 표현으로 그린 단편. 규수의 원한이라 규원. 9.4.3 분석 및 소통 감정어휘 빈도를 계산해 두 문서에 감정을 분석해보자. sosul2_senti_df &lt;- sosul2_df %&gt;% unnest_tokens(word, main) %&gt;% inner_join(knu_dic_df) %&gt;% group_by(author) %&gt;% count(word, sScore, sort = TRUE) %&gt;% filter(str_length(word) &gt; 1) %&gt;% mutate(word = reorder(word, n)) %&gt;% slice_head(n = 20) %&gt;% mutate(sScore = as.factor(sScore)) sosul2_senti_df %&gt;% ggplot(aes(n, reorder_within(word, n, author), fill = sScore)) + geom_col() + scale_y_reordered() + scale_fill_brewer(type=&quot;div&quot;, palette = &quot;RdBu&quot;) + facet_wrap( ~ author, scales = &quot;free&quot;) 형태소를 분석해 감정을 분석해 보자. RcppMeCab의 pos()함수를 이용하자. sosul2_senti_tk &lt;- sosul2_df %&gt;% unnest_tokens(word, main, token = RcppMeCab::pos) %&gt;% separate(col = word, into = c(&quot;word&quot;, &quot;morph&quot;), sep = &quot;/&quot; ) %&gt;% inner_join(knu_dic_df) %&gt;% group_by(author) %&gt;% count(word, sScore, sort = T) %&gt;% filter(str_length(word) &gt; 1) %&gt;% mutate(word = reorder(word, n)) %&gt;% slice_head(n = 20) %&gt;% mutate(sScore = as.factor(sScore)) sosul2_senti_tk %&gt;% ggplot(aes(n, reorder_within(word, n, author), fill = sScore)) + geom_col() + scale_y_reordered() + scale_fill_brewer(type=&quot;div&quot;, palette = &quot;RdBu&quot;) + facet_wrap( ~ author, scales = &quot;free&quot;) 형태소를 분석한 결과를 통해 나타난 두 문서의 감정이 조금 더 긍정적으로 변했다. 형태소분석하면서 부정어휘로 구분됐던 단어들이 한글자로 분리돼 불용어로 제거된 결과다. 9.4.4 소설 감정점수 ’권태’와 ’규원’의 감정점수를 비교해 어느 글이 더 부정적인지 살펴보자. 각 토큰에 감정점수 부여한다. 감정사전에 없는 단어를 남겨두기 위해 left_join()이용 sosul2_df %&gt;% unnest_tokens(word, main) %&gt;% left_join(knu_dic_df) %&gt;% mutate(sScore = ifelse(is.na(sScore), 0, sScore)) %&gt;% arrange(sScore) ## # A tibble: 5,959 × 5 ## title author source word sScore ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 권태 이상 NA 위험한 -2 ## 2 권태 이상 NA 아프게 -2 ## 3 권태 이상 NA 어려운 -2 ## 4 권태 이상 NA 밉다 -2 ## 5 권태 이상 NA 거칠고 -2 ## 6 권태 이상 NA 아니다 -2 ## # … with 5,953 more rows 감정단어를 극성별로 분류하자. 사전의 감정점수가 5수준으로 돼 있는 것을 3수준으로 줄인다. sosul2_df %&gt;% unnest_tokens(word, main) %&gt;% left_join(knu_dic_df) %&gt;% mutate(sScore = case_when(sScore &gt;= 1 ~ &quot;긍정&quot;, sScore &lt;= -1 ~ &quot;부정&quot;, is.na(sScore) ~ NA_character_, TRUE ~ &quot;중립&quot;)) %&gt;% count(sScore) ## # A tibble: 4 × 2 ## sScore n ## &lt;chr&gt; &lt;int&gt; ## 1 긍정 39 ## 2 부정 174 ## 3 중립 11 ## 4 NA 5735 ‘권태’와 ’규원’ 두소설의 감정점수 계산하면 다음과 같다. sosul2_df %&gt;% unnest_tokens(word, main) %&gt;% left_join(knu_dic_df) %&gt;% mutate(sScore = ifelse(is.na(sScore), 0, sScore)) %&gt;% group_by(title) %&gt;% summarise(emotion = sum(sScore)) ## # A tibble: 2 × 2 ## title emotion ## &lt;chr&gt; &lt;dbl&gt; ## 1 권태 -90 ## 2 규원 -91 9.4.5 감정 극성별 단어빈도 감정사전에서 긍정감정 단어와 부정단어를 각각 긍정단어 사전과 부정단어 사전을 만들어보자. knu_pos_df &lt;- knu_dic_df %&gt;% filter(sScore &gt; 0) knu_pos_df %&gt;% distinct(sScore) ## # A tibble: 2 × 1 ## sScore ## &lt;dbl&gt; ## 1 1 ## 2 2 knu_neg_df &lt;- knu_dic_df %&gt;% filter(sScore &lt; 0) knu_neg_df %&gt;% distinct(sScore) ## # A tibble: 2 × 1 ## sScore ## &lt;dbl&gt; ## 1 -1 ## 2 -2 긍정어 사전과 부정어 사전을 결합하여 두 소설에 포함된 단어 감성빈도를 계산해보자. sosul2_df %&gt;% unnest_tokens(word, main) %&gt;% inner_join(knu_pos_df) %&gt;% count(author, word, sort = TRUE) ## # A tibble: 32 × 3 ## author word n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 이상 귀여운 3 ## 2 이상 잘 3 ## 3 나혜석 사랑으로 2 ## 4 나혜석 잘 2 ## 5 이상 좋다 2 ## 6 나혜석 고쳐 1 ## # … with 26 more rows sosul2_df %&gt;% unnest_tokens(word, main) %&gt;% inner_join(knu_neg_df) %&gt;% count(author, word, sort = TRUE) ## # A tibble: 93 × 3 ## author word n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 이상 없다 30 ## 2 나혜석 해 11 ## 3 이상 권태 7 ## 4 나혜석 못하고 5 ## 5 이상 답답한 4 ## 6 이상 모르는 4 ## # … with 87 more rows 9.4.6 극성별 단어빈도 단어구름 두 소설에 많이 언급된 단어 빈도수를 구해서 단어구름 시각화를 한다. # install.packages(&quot;wordcloud&quot;) library(wordcloud) sosul2_df %&gt;% unnest_tokens(word, main) %&gt;% count(word) %&gt;% filter(str_length(word) &gt; 1) %&gt;% with(wordcloud(word, n, max.words = 50)) 긍정단어와 부정단어 분리하여 단어구름으로 시각화한다. 이를 위해서 comparison.cloud() 함수에 적합한 형태로 데이터프레임 변환작업을 수행하고 comparison.cloud() 함수에 행렬을 넣어준다. sosul2_df %&gt;% unnest_tokens(word, main) %&gt;% inner_join(knu_dic_df) %&gt;% mutate(emotion = case_when(sScore &gt; 0 ~ &quot;긍정&quot;, sScore &lt; 0 ~ &quot;부정&quot;, TRUE ~ &quot;중립&quot;)) %&gt;% filter(emotion != &quot;중립&quot;) %&gt;% count(word, emotion, sort = TRUE) %&gt;% filter(str_length(word) &gt; 1) %&gt;% ## wordcloud() 함수에 적합한 형태로 데이터프레임 변환 pivot_wider(names_from = emotion, values_from = n, values_fill = 0) %&gt;% column_to_rownames(var = &quot;word&quot;) %&gt;% as.matrix() %&gt;% # reshape2::acast(word ~ emotion, value.var = &quot;n&quot;, fill = 0) # 이전 코드 # 비교 단어구름 시각화 comparison.cloud(colors = c(&quot;red&quot;, &quot;blue&quot;), max.words = 50) 9.4.7 문장 전개에 따른 감정변화 문장 단위로 토큰화하고, row_number()를 이용해 각 문장단위로 번호를 부여한다. sosul2_df %&gt;% unnest_tokens(sentence, main, token = &quot;sentences&quot;) %&gt;% mutate(linenumber = row_number()) %&gt;% unnest_tokens(word, sentence) ## # A tibble: 5,959 × 5 ## title author source linenumber word ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 권태 이상 NA 1 1 ## 2 권태 이상 NA 1 어서 ## 3 권태 이상 NA 1 차라리 ## 4 권태 이상 NA 1 어둬버리기나 ## 5 권태 이상 NA 1 했으면 ## 6 권태 이상 NA 1 좋겠는데 ## # … with 5,953 more rows 5수준인 감정사전의 감정점수를 긍정과 부정 등 2수준으로 축소하자. sosul2_df %&gt;% unnest_tokens(sentence, main, token = &quot;sentences&quot;) %&gt;% mutate(linenumber = row_number()) %&gt;% unnest_tokens(word, sentence) %&gt;% inner_join(knu_dic_df) %&gt;% mutate(emotion = case_when(sScore &gt; 0 ~ &quot;긍정&quot;, sScore &lt; 0 ~ &quot;부정&quot;, TRUE ~ &quot;중립&quot;)) ## # A tibble: 224 × 7 ## title author source linenumber word sScore emotion ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 권태 이상 NA 7 바른 2 긍정 ## 2 권태 이상 NA 14 사랑 2 긍정 ## 3 권태 이상 NA 15 좋다 2 긍정 ## 4 권태 이상 NA 19 잘 1 긍정 ## 5 권태 이상 NA 24 없다 -1 부정 ## 6 권태 이상 NA 25 권태 -1 부정 ## # … with 218 more rows 긍정감정과 부정감정의 차이를 계산한다. 지은이를 기준으로 각 문장(행)별로 감정의 빈도를 계산하면, 글의 진행에 따른 감정의 변화를 나타낼 수 있다. 그런데, 개별 문장에는 감정어가 들어있는 빈도가 적기 때문에, 여러 문장을 하나의 단위로 묶을 필요가 있다. 이를 위해 몫만을 취하는 정수 나눗셈(%/%)활용. c(1, 10, 20, 30, 40, 50) / 20 ## [1] 0.05 0.50 1.00 1.50 2.00 2.50 c(1, 10, 20, 30, 40, 50) %/% 20 ## [1] 0 0 1 1 2 2 여러 행을 묶지 않고 지은이 별로 감정어의 빈도를 계산하면 334행에는 긍정단어 2개, 부정단어가 1개 있음을 알수 있다. sosul2_df %&gt;% unnest_tokens(sentence, main, token = &quot;sentences&quot;) %&gt;% mutate(linenumber = row_number()) %&gt;% unnest_tokens(word, sentence) %&gt;% inner_join(knu_dic_df) %&gt;% mutate(emotion = case_when(sScore &gt; 0 ~ &quot;긍정&quot;, sScore &lt; 0 ~ &quot;부정&quot;, TRUE ~ &quot;중립&quot;)) %&gt;% count(author, index = linenumber, emotion) ## # A tibble: 190 × 4 ## author index emotion n ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 나혜석 334 긍정 2 ## 2 나혜석 334 부정 1 ## 3 나혜석 335 부정 1 ## 4 나혜석 336 부정 1 ## 5 나혜석 336 중립 1 ## 6 나혜석 338 긍정 1 ## # … with 184 more rows 각 행을 10으로 정수나눗셈을 하면 10개 행씩 모두 같은 값으로 묶을 수 있다. 334행부터 339행인 33이 된다. 긍정단어와 부정단어도 각각 5개가 된다. sosul2_df %&gt;% unnest_tokens(sentence, main, token = &quot;sentences&quot;) %&gt;% mutate(linenumber = row_number()) %&gt;% unnest_tokens(word, sentence) %&gt;% inner_join(knu_dic_df) %&gt;% mutate(emotion = case_when(sScore &gt; 0 ~ &quot;긍정&quot;, sScore &lt; 0 ~ &quot;부정&quot;, TRUE ~ &quot;중립&quot;)) %&gt;% count(author, index = linenumber %/% 10, emotion) ## # A tibble: 89 × 4 ## author index emotion n ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 나혜석 33 긍정 5 ## 2 나혜석 33 부정 5 ## 3 나혜석 33 중립 2 ## 4 나혜석 34 긍정 2 ## 5 나혜석 34 부정 8 ## 6 나혜석 35 긍정 1 ## # … with 83 more rows 긍정감정 점수에서 부정감정 점수를 빼 감정의 상대강도를 계산하기 위해 감정의 값을 열의 제목으로 만들고, 각 감정어 빈도를 해당 열의 값으로 할당한다. 감정어가 없어 결측값이 나올수 있으므로, values_fill =인자로 ’0’을 할당한다. sosul2_emo_df &lt;- sosul2_df %&gt;% unnest_tokens(sentence, main, token = &quot;sentences&quot;) %&gt;% mutate(linenumber = row_number()) %&gt;% unnest_tokens(word, sentence) %&gt;% inner_join(knu_dic_df) %&gt;% mutate(emotion = case_when(sScore &gt; 0 ~ &quot;긍정&quot;, sScore &lt; 0 ~ &quot;부정&quot;, TRUE ~ &quot;중립&quot;)) %&gt;% count(author, index = linenumber %/% 10, emotion) %&gt;% pivot_wider(names_from = emotion, values_from = n, values_fill = 0) %&gt;% mutate(sentiment = 긍정 - 부정) sosul2_emo_df ## # A tibble: 59 × 6 ## author index 긍정 부정 중립 sentiment ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 나혜석 33 5 5 2 0 ## 2 나혜석 34 2 8 0 -6 ## 3 나혜석 35 1 4 0 -3 ## 4 나혜석 37 2 2 1 0 ## 5 나혜석 38 0 1 0 -1 ## 6 나혜석 39 1 6 0 -5 ## # … with 53 more rows 막대도표로 소설 전개에 따른 감성변화를 시각화한다. sosul2_emo_df %&gt;% ggplot( aes(index, sentiment, fill = author) ) + geom_col( show.legend = FALSE) + facet_wrap( ~ author, scales = &quot;free_x&quot;) "],["topic-modeling.html", "10 . 주제모형 10.1 개관 10.2 자료 준비 10.3 분석 10.4 소통 10.5 과제", " 10 . 주제모형 10.1 개관 문서에 빈번하는 등장하는 단어를 통해 그 문서의 주제를 추론할 수 있다. 한 문서에는 다양한 주제가 들어있다. 예를 들어, 아래 문장은 AP가 보도한 1988년 허스트재단의 링컨센터 기부 기사다. ap_v &lt;- c(&quot;The William Randolph Hearst Foundation will give $1.25 million to Lincoln Center, Metropolitan Opera Co., New York Philharmonic and Juilliard School. “Our board felt that we had a real opportunity to make a mark on the future of the performing arts with these grants an act every bit as important as our traditional areas of support in health, medical research, education and the social services,” Hearst Foundation President Randolph A. Hearst said Monday in announcing the grants. Lincoln Center’s share will be $200,000 for its new building, which will house young artists and provide new public facilities. The Metropolitan Opera Co. and New York Philharmonic will receive $400,000 each. The Juilliard School, where music and the performing arts are taught, will get $250,000. The Hearst Foundation, a leading supporter of the Lincoln Center Consolidated Corporate Fund, will make its usual annual $100,000 donation, too.&quot;) 단어의 총빈도와 상대빈도를 계산하면, 이 문서의 주요 내용이 무엇인지 파악할 수 있다. 먼저 총빈도 상위단어를 찾아보자. pkg_v &lt;- c(&quot;tidyverse&quot;, &quot;tidytext&quot;, &quot;tidylo&quot;) purrr::walk(pkg_v, require, ch = T) ap_count &lt;- ap_v %&gt;% tibble(text = .) %&gt;% unnest_tokens(word, text, drop = FALSE) %&gt;% anti_join(stop_words) %&gt;% count(word, sort = TRUE) %&gt;% head(10) 상대빈도를 구해보자. 먼저 문장 단위로 토큰화해 문장별 ID를 구한다음 단어 단위로 토큰화한다. 대문자 앞의 마침표와 공백(\"\\\\.\\\\s(?=[:upper:])\")을 기준으로 구분하면 된다. ap_tfidf &lt;- ap_v %&gt;% tibble(text = .) %&gt;% mutate(text = str_squish(text)) %&gt;% unnest_tokens(sentence, text, token = &quot;regex&quot;, pattern = &quot;\\\\.\\\\s(?=[:upper:])&quot;) %&gt;% mutate(ID = row_number()) %&gt;% unnest_tokens(word, sentence, drop = F) %&gt;% anti_join(stop_words) %&gt;% count(ID, word, sort = T) %&gt;% bind_tf_idf(term = word, document = ID, n = n) %&gt;% arrange(-tf_idf) %&gt;% head(10) ap_wlo &lt;- ap_v %&gt;% tibble(text = .) %&gt;% mutate(text = str_squish(text)) %&gt;% unnest_tokens(sentence, text, token = &quot;regex&quot;, pattern = &quot;\\\\.\\\\s(?=[:upper:])&quot;) %&gt;% mutate(ID = row_number()) %&gt;% unnest_tokens(word, sentence, drop = F) %&gt;% anti_join(stop_words) %&gt;% count(ID, word, sort = T) %&gt;% bind_log_odds(feature = word, set = ID, n = n) %&gt;% arrange(-log_odds_weighted) %&gt;% head(10) 추출한 상위 10대 빈도 단어를 비교해 보자. bind_cols( select(ap_count, 총빈도 = word), select(ap_tfidf, tf_idf = word), select(ap_wlo, 가중승산비 = word) ) ## # A tibble: 10 × 3 ## 총빈도 tf_idf 가중승산비 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 hearst announcing hearst ## 2 foundation monday grants ## 3 lincoln 400,000 announcing ## 4 arts receive monday ## 5 center grants metropolitan ## 6 grants 250,000 opera ## # … with 4 more rows 총빈도와 상대빈도를 보면, 허스트재단이 링컨아트센터에 기부금 발표한 내용이란 것을 추론할 수 있지만, 기사에는 보다 다양한 주제를 담고 있다. 기사의 내용을 읽어보면 다양한 주제가 있음을 알수 있다 (Figure 10.1). 그림 10.1: AP 기사 기사에 포함된 단어 중 같은 색으로 구부된 단어들을 모아보면 예술, 재정, 아동, 교육 등의 주제를 나타내는 일관된 단어로 구성됐음을 알수 있다 (Figure 10.2). 그림 10.2: AP 기사 주제 Blei 등 일군의 전산학자들은 문서 내 단어의 확률분포를 계산해 찾아낸 일련의 단어 군집을 통해 문서의 주제를 추론하는 방법으로서 LDA(Latent Dirichlet Allocation)을 제시했다. Blei, D. M., Ng, A. Y., &amp; Jordan, M. I. (2003). Latent dirichlet allocation. Journal of Machine Learning Research, 3, 993-1022. 19세기 독일 수학자 러죈 디리클레(Lejeune Dirichlet, 1805 ~ 1859)가 제시한 디리클래 분포(Dirichlet distribution)를 이용해 문서에 잠재된 주제를 추론하기에 잠재 디리클레 할당(LDA: Latent Dirichlet Allocation)이라고 했다. 문서의 주제를 추론하는 방법이므로 주제모형(topic models)이라고 한다. Beli(2012)가 설명한 LDA에 대한 직관적인 이해는 다음과 같다. Figure10.3에 제시된 논문 “Seeking life’s bare (genetics) necessities”은 진화의 틀에서 유기체가 생존하기 위해 필요한 유전자의 수를 결정하기 위한 데이터분석에 대한 내용이다. 문서(documents)에 파란색으로 표시된 ‘computer’ ‘prediction’ 등은 데이터분석에 대한 단어들이다. 분홍색으로 표시된 ‘life’ ‘organism’은 진화생물학에 대한 내용이다. 노란색으로 표시된 ’sequenced’ ’genes’는 유전학에 대한 내용이다. 이 논문의 모든 단어를 이런 식으로 분류하면 아마도 이 논문은 유전학, 데이터분석, 진화생물학 등이 상이한 비율로 혼합돼 있음을 알게 된다. Blei, D. M. (2012). Probabilistic topic models. Communications of the ACM, 55(4), 77-84. 그림 10.3: LDA의 직관적 예시 LDA에는 다음과 같은 전제가 있다. 말뭉치에는 단어를 통해 분포된 다수의 주제가 있다 (위 그림의 가장 왼쪽). 각 문서에서 주제를 생성하는 과정은 다음과 같다. 주제에 대해 분포 선택(오른쪽 히스토그램) 각 단어에 대해 주제의 분포 선택(색이 부여된 동그라미) 해당 주제를 구성하는 단어 선택(가운데 화살표) LDA에서 정의하는 주제(topic)는 특정 단어에 대한 분포다. 예를 들어, 유전학 주제라면 유전학에 대하여 높은 확률로 분포하는 단어들이고, 진화생물학 주제라면 진화생물학에 대하여 높은 확률로 분포하는 단어들이다. LDA에서는 문서를 주머니에 무작위로 섞여 있는 임의의 혼합물로 본다(Bag of words). 일반적으로 사용하는 문장처럼 문법이라는 짜임새있는 구조로 보는 것이 아니다. 임의의 혼합물이지만 온전하게 무작위로 섞여 있는 것은 아니다. 서로 함께 모여 있는 군집이 확률적으로 존재한다. 즉, 주제모형에서 접근하는 문서는 잠재된 주제의 혼합물로서, 각 주제를 구성하는 단어 단위가 확률적으로 혼합된 주머니(bag)인 셈이다. 개별 문서: 여러 주제(topic)가 섞여 있는 혼합물 문서마다 주제(예술, 교육, 예산 등)의 분포 비율 상이 주제(예: 예술)마다 단어(예: 오페라, 교향악단)의 분포 상이 주제모형의 목표는 말뭉치에서 주제의 자동추출이다. 문서 자체는 관측가능하지만, 주제의 구조(문서별 주제의 분포와 문서별-단어별 주제할당)는 감춰져 있다. 감춰진 주제의 구조를 찾아내는 작업은 뒤집어진 생성과정이라고 할 수 있다. 관측된 말뭉치를 생성하는 감춰진 구조를 찾아내는 작업이기 때문이다. 문서에 대한 사전 정보없이 문서의 주제를 분류하기 때문에 주제모형은 비지도학습(unsupervised learning) 방식의 기계학습(machine learning)이 된다. 기계학습(machine learing) 인공지능 작동방식. 투입한 데이터에서 규칙성 탐지해 분류 및 예측. 지도학습, 비지도학습, 강화학습 등으로 구분. 지도학습(supervised learning) 인간이 사전에 분류한 결과를 학습해 투입한 자료에서 규칙성 혹은 경향 발견 비지도학습(unsupervised learning) 사전분류한 결과 없이 기계 스스로 투입한 자료에서 규칙성 혹은 경향 발견 강화학습(reinforced learning) 행동의 결과에 대한 피드백(보상, 처벌 등)을 통해 투입한 자료에서 규칙성 혹은 경향 발견 주제모형의 효용은 대량의 문서에서 의미구조를 닮은 주제구조를 추론해 주석을 자동으로 부여할 수 있다는데 있다. 주제모형은 다양한 패지키가 있다. lda topicmodels stm 여기서는 구조적 주제모형(structural topic model)이 가능한 stm패키지를 이용한다. stm은 메타데이터를 이용한 추출한 주제에 대하여 다양한 분석을 할 수 있는 장점이 있다. 10.1.1 작업흐름 단어가 모이면 토픽이 되고, 토픽이 모이면 문서가 되는 방식을 상상하는 것이 필요하다. 토픽(topic)은 문서 모임을 추상화한 것으로 토픽을 듣게 되면 토픽을 구성할 단어를 어림 짐작할 수 있게 된다. 예를 들어, 전쟁이라고 하면 총, 군인, 탱크, 비행기 등이 관련된 단어로 연관된다. 여러 토픽이 모여서 문서가 되고, 문서는 여러 토픽을 담게 된다. 토픽 모형(topci modeling)은 문서로부터 모형을 적합시켜 토픽을 찾아내는 과정으로 정의할 수 있다. 토픽모형을 활용함으로써 문서를 분류하는데 종종 활용된다. 특히, LDA(Latent Dirichlet Allocation) 모형이 가장 많이 활용되고 있다. 텍스트에서 토픽모형을 개발하는 순서는 대략 다음과 같다. 텍스트를 DTM을 변환시킨다. 명사를 추출할 경우와, 동사를 추출할 경우로 나눠서 살펴볼 수도 있다. LDA는 DTM을 입력값을 받아 문서별로 토픽에 대한 연관성을 나타내는 행렬과 토픽에 단어가 속할 확률 행렬을 출력값으로 반환한다. 제어 매개변수(control parameter)를 적절히 설정한다. 출력된 행렬은 세부적으로 정보를 확인할 때 필요하고 우선, 시각매체를 사용하여 시각화한다. \\(\\beta\\) 행렬은 토픽에 단어가 포함될 확률 \\(\\gamma\\) 행렬은 문서에 토픽이 포함될 확률 10.1.2 개념 예제 문장을 금융 관련 문서1, 문서2를 준비하고, 식당관련 문장을 문서3, 문서4로 준비한다. 문서5는 금융과 식당이 뒤섞이도록 준비한다. 이를 topicmodels 팩키지를 활용하여 LDA 분석작업을 수행한다. 그리고 나서 결과값을 문서-토픽 행렬로 표현하고 좀더 직관적으로 볼 수 있도록 ggplot으로 시각화한다. library(tidyverse) library(tidytext) library(topicmodels) ## 예제 데이터 sample_text &lt;- c(&quot;부실 대출로 인해서 은행은 벌금을 지불하는데 동의했다&quot;, &quot;은행에 대출을 늦게 갚은 경우, 은행에서 지연에 대해 이자를 물릴 것이다.&quot;, &quot;시내에 새로운 식당이 생겼습니다.&quot;, &quot;테헤란로에 맛집 식당이 있습니다.&quot;, &quot;새로 개장하려고 하는 식당 대출을 어떻게 상환할 계획입니까?&quot;) sample_df &lt;- tibble( document = paste0(&quot;문서&quot;, 1:5), text = sample_text ) ## BOW 데이터 변환 sample_bow &lt;- sample_df %&gt;% ## 문서 내 텍스트에서 명사 추출 mutate(mecab_pos = map(text, RcppMeCab::pos) ) %&gt;% unnest(mecab_pos) %&gt;% unnest(mecab_pos) %&gt;% separate(mecab_pos, into = c(&quot;nouns&quot;, &quot;pos&quot;), sep = &quot;/&quot;) %&gt;% filter(pos == &quot;NNG&quot;) %&gt;% ## 문서별 명사 빈도수 group_by(document) %&gt;% count(nouns, sort = TRUE) ## DTM 변환 sample_dtm &lt;- sample_bow %&gt;% cast_dtm(document = document, term = nouns, value = n) %&gt;% as.matrix ## LDA 모형 적합 sample_lda &lt;- LDA(sample_dtm, k = 2, method=&quot;Gibbs&quot;, control=list(alpha=0.1, delta=0.1, seed=1357)) ## 토픽 결과 - 행렬 tidy(sample_lda, matrix=&quot;gamma&quot;) %&gt;% arrange(document) %&gt;% spread(topic, gamma) ## # A tibble: 5 × 3 ## document `1` `2` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 문서1 0.0161 0.984 ## 2 문서2 0.0161 0.984 ## 3 문서3 0.955 0.0455 ## 4 문서4 0.955 0.0455 ## 5 문서5 0.596 0.404 ## 토픽 결과 - 시각화 ### 문서 - 토픽 doc_topic_g &lt;- tidy(sample_lda, matrix=&quot;gamma&quot;) %&gt;% mutate(topic = as.factor(topic)) %&gt;% ggplot(aes(x = document, y=gamma)) + geom_col(aes(fill = topic), position=position_dodge()) + scale_fill_manual(values = c(&quot;orange&quot;, &quot;midnightblue&quot;)) + theme_light() + labs(title=&quot;금융, 식당 분류 토픽모형&quot;, subtitle = &quot;문서 토픽 행렬&quot;) ### 토픽 - 단어 topic_word_g &lt;- tidy(sample_lda, matrix=&quot;beta&quot;) %&gt;% ggplot(aes(x = term, y=beta)) + geom_col(aes(fill=as.factor(topic)), position=position_dodge()) + scale_fill_manual(values = c(&quot;orange&quot;, &quot;midnightblue&quot;)) + theme_light() + labs(title=&quot;금융, 식당 분류 토픽모형&quot;, subtitle = &quot;토픽 단어 행렬&quot;) + theme(axis.text.x = element_text(angle=90), legend.position = &quot;none&quot;) cowplot::plot_grid(doc_topic_g, topic_word_g) 상기 사례를 통해서 문서가 두가지 주제 금융과 식당으로 분류되고 문서를 구성하는 단어는 금융과 식당 중 어떤 주제에 더 많은 기여를 하는지 확인이 가능하다. 10.2 자료 준비 빅카인즈에서 다음의 조건으로 기사를 추출한다. 검색어: 인공지능 기간: 2010.1.1 ~ 2020.12.31 언론사: 중앙일보, 조선일보, 한겨레, 경향신문, 한국경제, 매일경제 분석: 분석 기사 모두 5,145건이다. 10.2.1 자료 이입 다운로드한 기사를 작업디렉토리 아래 data폴더에 복사한다. list.files(&quot;data/.&quot;, pattern = &#39;^News.*20200101.*\\\\.xlsx$&#39;) ## [1] \"NewsResult_20200101-20201231.xlsx\" 파일명이 ’NewsResult_20200101-20201201.xlsx’다. readxl::read_excel(&quot;data/NewsResult_20200101-20201231.xlsx&quot;) %&gt;% names() ## [1] \"뉴스 식별자\" ## [2] \"일자\" ## [3] \"언론사\" ## [4] \"기고자\" ## [5] \"제목\" ## [6] \"통합 분류1\" ## [7] \"통합 분류2\" ## [8] \"통합 분류3\" ## [9] \"사건/사고 분류1\" ## [10] \"사건/사고 분류2\" ## [11] \"사건/사고 분류3\" ## [12] \"인물\" ## [13] \"위치\" ## [14] \"기관\" ## [15] \"키워드\" ## [16] \"특성추출(가중치순 상위 50개)\" ## [17] \"본문\" ## [18] \"URL\" ## [19] \"분석제외 여부\" 분석에 필요한 열을 선택해 데이터프레임으로 저장한다. 분석 텍스트는 제목과 본문이다. 빅카인즈는 본문을 200자까지만 무료로 제공하지만, 학습 목적을 달성하기에는 충분하다. 제목은 본문의 핵심 내용을 반영하므로, 제목과 본문을 모두 주제모형 분석에 투입한다. 시간별, 언론사별, 분류별로 분석할 계획이므로, 해당 열을 모두 선택한다. ai_df &lt;- readxl::read_excel(&quot;data/NewsResult_20200101-20201231.xlsx&quot;) %&gt;% select(일자, 제목, 본문, 언론사, cat = `통합 분류1`) ai_df %&gt;% head() ## # A tibble: 6 × 5 ## 일자 제목 본문 언론사 cat ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 20201231 \"`사모펀드 책임` 놓고 새해부… \"은… 매일… 경제… ## 2 20201231 \"[이광석의 디지털 이후](25)시… \"ㆍ… 경향… IT_… ## 3 20201231 \"\\\"소처럼 묵묵하게 경제 통합 … \"역… 매일… 지역… ## 4 20201231 \"GS건설, `강릉자이 파인베뉴` … \"GS… 매일… 경제… ## 5 20201231 \"디지털 강조한 금융협회장들 … \"\\\"… 매일… 경제… ## 6 20201231 \"정치인 거짓말도 진실로 둔갑 … \"◆ … 매일… IT_… 시간열에는 연월일의 값이 있다. 월별 추이에 따른 주제를 분석할 것이므로 열의 값을 월에 대한 값으로 바꾼다. tidyverse패키지에 함께 설치되는 lubridate패키지를 이용해 문자열을 날짜형으로 변경하고 월 데이터 추출해 새로운 열 month 생성한다. lubridate는 tidyverse에 포함돼 있으나 함께 부착되지 않으므로 별도로 실행해야 한다. library(lubridate) as_date(&quot;20201231&quot;) %&gt;% month() ## [1] 12 ymd(&quot;20201231&quot;) %&gt;% month() ## [1] 12 DB에 같은 기사가 중복 등록되는 경우가 있으므로, dplyr패키지의 distinct()함수를 이용해 중복된 문서를 제거한다. .keep_all =인자의 기본값은 FALSE다. 투입한 열 이외의 열은 유지하지 않는다. 다른 변수(열)도 분석에 필요하므로 .keep_all =인자를 TRUE로 지정한다. 분석 목적에 맞게 열을 재구성한다. library(lubridate) ai2_df &lt;- ai_df %&gt;% # 중복기사 제거 distinct(제목, .keep_all = TRUE) %&gt;% # 기사별 ID부여 mutate(ID = factor(row_number())) %&gt;% # 월별로 구분한 열 추가 mutate(month = month(ymd(일자))) %&gt;% # 기사 제목과 본문 결합 unite(제목, 본문, col = &quot;text&quot;, sep = &quot; &quot;) %&gt;% # 중복 공백 제거 mutate(text = str_squish(text)) %&gt;% # 언론사 분류: 보수 진보 경제 %&gt;% mutate(press = case_when( 언론사 == &quot;조선일보&quot; ~ &quot;종합지&quot;, 언론사 == &quot;중앙일보&quot; ~ &quot;종합지&quot;, 언론사 == &quot;경향신문&quot; ~ &quot;종합지&quot;, 언론사 == &quot;한겨레&quot; ~ &quot;종합지&quot;, 언론사 == &quot;한국경제&quot; ~ &quot;경제지&quot;, TRUE ~ &quot;경제지&quot;) ) %&gt;% # 기사 분류 구분 separate(cat, sep = &quot;&gt;&quot;, into = c(&quot;cat&quot;, &quot;cat2&quot;)) %&gt;% # IT_과학, 경제, 사회 만 선택 filter(str_detect(cat, &quot;IT_과학|경제|사회&quot;)) %&gt;% select(-cat2) ai2_df %&gt;% head(5) ## # A tibble: 5 × 7 ## 일자 text 언론사 cat ID month press ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 20201231 \"`사모펀드 책임` … 매일… 경제 1 12 경제… ## 2 20201231 \"[이광석의 디지털… 경향… IT_… 2 12 종합… ## 3 20201231 \"GS건설, `강릉자… 매일… 경제 4 12 경제… ## 4 20201231 \"디지털 강조한 금… 매일… 경제 5 12 경제… ## 5 20201231 \"정치인 거짓말도 … 매일… IT_… 6 12 경제… ai2_df %&gt;% names() ## [1] \"일자\" \"text\" \"언론사\" \"cat\" \"ID\" \"month\" ## [7] \"press\" 기사의 분류된 종류, 월 등 새로 생성한 열의 내용을 확인해보자. ai2_df$cat %&gt;% unique() ## [1] \"경제\" \"IT_과학\" \"사회\" ai2_df$month %&gt;% unique() ## [1] 12 11 10 9 8 7 6 5 4 3 2 1 ai2_df$press %&gt;% unique() ## [1] \"경제지\" \"종합지\" 분류별, 월별로 기사의 양을 계산해보자. ai2_df %&gt;% count(cat, sort = TRUE, name = &quot;기사건수&quot;) ## # A tibble: 3 × 2 ## cat 기사건수 ## &lt;chr&gt; &lt;int&gt; ## 1 IT_과학 1922 ## 2 경제 1409 ## 3 사회 460 ai2_df %&gt;% count(month, sort = TRUE, name = &quot;기사건수&quot;) ## # A tibble: 12 × 2 ## month 기사건수 ## &lt;dbl&gt; &lt;int&gt; ## 1 1 382 ## 2 9 379 ## 3 12 366 ## 4 7 345 ## 5 6 335 ## 6 5 334 ## # … with 6 more rows ai2_df %&gt;% count(press, sort = TRUE, name = &quot;기사건수&quot;) ## # A tibble: 2 × 2 ## press 기사건수 ## &lt;chr&gt; &lt;int&gt; ## 1 경제지 2049 ## 2 종합지 1742 10.2.2 정제 10.2.2.1 토큰화 RcppMeCab패키지의 pos()함수로 명사만 추출해 토큰화한다. 명사가 문서의 주제를 잘 나타내므로 주제모형에서는 주로 명사를 이용하지만, 목적에 따라서는 다른 품사(용언 등)를 분석에 투여하기도 한다. (* 형태소 추출전에 문자 혹은 공백 이외의 요소(예: 구둣점)를 먼저 제거한다.) library(RmecabKo) ai_tk &lt;- ai2_df %&gt;% # 문자 혹은 공백 이외 것 제거 mutate(text = str_remove_all(text, &quot;[^(\\\\w+|\\\\s)]&quot;)) %&gt;% # 메카브로 품사 중 명사만 추출 unnest_tokens(word, text, token = RcppMeCab::pos, drop = FALSE) %&gt;% separate(col = word, into = c(&quot;word&quot;, &quot;morph&quot;), sep = &quot;/&quot; ) %&gt;% filter(morph == &quot;nng&quot;) ai_tk %&gt;% glimpse() ## Rows: 145,965 ## Columns: 9 ## $ 일자 &lt;chr&gt; \"20201231\", \"20201231\", \"20201231\", \"202012… ## $ text &lt;chr&gt; \"사모펀드 책임 놓고 새해부터 금융위 탓한 금… ## $ 언론사 &lt;chr&gt; \"매일경제\", \"매일경제\", \"매일경제\", \"매일경… ## $ cat &lt;chr&gt; \"경제\", \"경제\", \"경제\", \"경제\", \"경제\", \"경… ## $ ID &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ month &lt;dbl&gt; 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,… ## $ press &lt;chr&gt; \"경제지\", \"경제지\", \"경제지\", \"경제지\", \"경… ## $ word &lt;chr&gt; \"사모\", \"펀드\", \"책임\", \"새해\", \"금융\", \"위… ## $ morph &lt;chr&gt; \"nng\", \"nng\", \"nng\", \"nng\", \"nng\", \"nng\", \"… 10.2.2.2 불용어 제거 ’인공지능’으로 검색한 기사이므로, ’인공지능’관련 단어는 제거한다. 문자가 아닌 요소를 모두 제거한다. (숫자를 반드시 제거해야 하는 것은 아니다.) ai_tk &lt;- ai_tk %&gt;% filter(!word %in% c(&quot;인공지능&quot;, &quot;AI&quot;, &quot;ai&quot;, &quot;인공지능AI&quot;, &quot;인공지능ai&quot;)) %&gt;% filter(str_detect(word, &quot;[:alpha:]+&quot;)) 단어의 총빈도와 상대빈도를 살펴보자 ai_tk %&gt;% count(word, sort = TRUE) ## # A tibble: 9,562 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 기술 1933 ## 2 기업 1674 ## 3 데이터 1138 ## 4 서비스 1114 ## 5 개발 1094 ## 6 코로나 1051 ## # … with 9,556 more rows 상대빈도가 높은 단어와 낮은 단어를 확인한다. ai_tk %&gt;% count(cat, word, sort = TRUE) %&gt;% bind_log_odds(set = cat, feature = word, n = n) %&gt;% arrange(log_odds_weighted) ## # A tibble: 16,139 × 4 ## cat word n log_odds_weighted ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 IT_과학 반도체 32 -5.78 ## 2 IT_과학 투자 230 -5.56 ## 3 IT_과학 펀드 16 -5.07 ## 4 경제 전자 92 -5.07 ## 5 IT_과학 종목 5 -4.92 ## 6 IT_과학 금융 140 -4.84 ## # … with 16,133 more rows ai_tk %&gt;% count(cat, word, sort = TRUE) %&gt;% bind_tf_idf(term = word, document = word, n = n) %&gt;% arrange(idf) ## # A tibble: 16,139 × 6 ## cat word n tf idf tf_idf ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 IT_과학 기술 1175 0.608 8.07 4.90 ## 2 경제 기업 831 0.496 8.07 4.00 ## 3 IT_과학 기업 771 0.461 8.07 3.72 ## 4 IT_과학 서비스 735 0.660 8.07 5.32 ## 5 IT_과학 데이터 671 0.590 8.07 4.76 ## 6 IT_과학 개발 633 0.579 8.07 4.67 ## # … with 16,133 more rows 한글자 단어는 문서의 주제를 나타내는데 기여하지 못하는 경우도 있고, 고유명사인데 형태소로 분리돼 있는 경우도 있다. 상대빈도가 높은 단어를 살펴 특이한 단어가 있으면 형태소 추출전 단어가 무엇인지 확인한다. 특이한 경우가 없으면 한글자 단어는 모두 제거한다. tibble 데이터프레임은 문자열의 일부만 보여준다. pull()함수로 열에 포함된 문자열을 벡터로 출력하므로, 모든 내용을 확인할 수 있다. ai_tk %&gt;% filter(word == &quot;하&quot;) %&gt;% pull(text) %&gt;% head(3) ## [1] \"정희수 생보협회장 빅테크의 금융 진출에 규제 공백 없게 정희수 생명보험협회장은 새해 신년사로 빅테크의 금융상품 판매에 규제 공백이 발생하지 않도록 노력하겠다 밝혔다 정희수 협회장은 31일 공개한 신축년 신년사를 통해 동일기능 동일규제 원칙 하에 빅테크와 관련한 기울어진 운동장 이슈를 해결하고 빅테크 플랫폼 기업의 금융상품판매 유사행위에 대한 규제 공백이 발생하지 않도록 지속적으로 노력하겠다고 강조했\" ## [2] \"한국화웨이 ICT 챌린지 경진대회 16명에 6500만원 상금 수여 한국화웨이가 개최한 제1회 한국 화웨이 ICT 챌린지 경진대회 결과 총 16명의 수상자가 선정됐다 한국화웨이는 지난 28일 온라인으로 시상식을 열고 상장과 상금(총 6500만원)을 수여했다 회사 관계자는 한국화웨이 ICT 챌린지는 화웨이가 한국에서 한국을 위하여라는 비전 하에 국내 ICT 인재 발굴 및 양성을 위해 계획하고 추진한 또 다른\" ## [3] \"왜 청년들은 중소기업에서 일하려고 하지 않는걸까 (하) 중기야사20 저는 인문계 고등학교를 졸업해 전형적인 모범생으로 살아와 공업계 고등학교에 대한 편견이 있습니다 그런데 공고 전체 중 10 정도에 해당하는 마이스터고는 졸업 후 취업률이 거의 90에 가깝습니다 이 중에는 정부에서 운영하는 국립 공고뿐 아니라 포항제철고 현대공업고 처럼 대기업에서 운영하는 공고도 있습니다 이런 공고는 졸업생들이\" ‘기업’ ‘기술’ 등의 단어는 총사용빈도가 높지만, 상대빈도는 낮다. 대부분의 분류에서 널리 사용된 단어다. 지금 제거할필요는 없지만, 제거가능성을 염두에 둔다. ai2_tk &lt;- ai_tk %&gt;% filter(str_length(word) &gt; 1) ai2_tk %&gt;% count(word, sort = TRUE) ## # A tibble: 8,970 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 기술 1933 ## 2 기업 1674 ## 3 데이터 1138 ## 4 서비스 1114 ## 5 개발 1094 ## 6 코로나 1051 ## # … with 8,964 more rows 상대빈도를 다시 확인하자. ai2_tk %&gt;% count(cat, word, sort = T) %&gt;% bind_log_odds(set = cat, feature = word, n = n) %&gt;% arrange(-log_odds_weighted) ## # A tibble: 14,990 × 4 ## cat word n log_odds_weighted ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 사회 교육 287 9.25 ## 2 사회 대학 185 7.62 ## 3 경제 자산 132 7.32 ## 4 사회 학생 127 7.23 ## 5 사회 치료 55 7.20 ## 6 IT_과학 건조기 158 6.86 ## # … with 14,984 more rows ai2_tf_idf &lt;- ai2_tk %&gt;% count(cat, word, sort = T) %&gt;% bind_tf_idf(term = word, document = word, n = n) %&gt;% arrange(tf_idf) 각 분야별 가장 tf-idf 값을 기준으로 높은 단어를 10개 뽑아 시각화를 한다. 특별한 모형이나 가정이 수반되지 않는 시각화로 직관적이지만 중복되는 tf-idf이 많은 경우 소통에 어려움이 있을 수 있다. ai2_tf_idf %&gt;% group_by(cat) %&gt;% slice_max(tf_idf, n = 10, with_ties = FALSE) %&gt;% ungroup() %&gt;% ggplot(aes(word, tf_idf, fill = cat)) + geom_col(alpha = 0.8, show.legend = FALSE) + facet_wrap(~ cat, scales = &quot;free&quot;, ncol = 3) + scale_x_reordered() + coord_flip() + theme(strip.text=element_text(size=11)) + labs(x = NULL, y = &quot;tf-idf&quot;, title = &quot;주제별 tf-idf 값이 가장 높은 단어&quot;) 10.2.3 stm 말뭉치 주제모형(topic model) 개발을 위해서 함수에 들어가는 입력 데이터를 두가지 방식으로 다음과 같이 나눠 작업도 가능하다. 10.2.3.1 cast_*() 방식 텍스트 데이터를 EDA 작업을 할 경우 대부분 시각화를 통해서 인사이트 도출 작업을 수행할 경우 깔끔한 데이터(tidy data)가 편리하지만 모형개발을 할 때는 깔끔한 데이터 보다는 cast_dtm() 함수를 사용해서 문서-단어-행렬(Document-term-matrix)로 바꿔줘야 주제모형을 개발할 수 있다. 주제모형(topic model)을 데이터에 적합시켜 도출한 후에 면멸한 모형 검정을 위해서 다시 tidytext::tidy() 함수를 사용해서 다시 깔끔한 데이터(tidy data) 변환시켜 후속 작업을 수행한다. 토픽모형 개발 작업흐름 한국언론진흥재단 빅카인즈에서 제공한 기사데이터는 “키워드”를 별도로 제공하고 있다. 주제모형 개발을 위해서 주제 발굴에 가장 영향이 큰 명사를 추출하여 이를 행렬로 변환시켜 stm::stm() 혹은 topicmodels::LDA() 함수를 사용해서 적절한 주제를 찾아내고 이를 해석하고 소통하는 과정이다. 따라서, 신속한 모형개발을 위해서 앞서 수행한 원문기사에서 명사를 추출하는 과정을 이미 사전에 기사마다 특정되어 있는 “키워드”를 사용한다. library(stm) ai_news_raw &lt;- readxl::read_excel(&quot;data/NewsResult_20200101-20201231.xlsx&quot;) set.seed(02138) ai_news_tidy &lt;- ai_news_raw %&gt;% janitor::clean_names(ascii = FALSE) %&gt;% select(news_id = 뉴스_식별자, keyword = 키워드) %&gt;% slice_sample(prop = 0.01) %&gt;% # 5145 중 1% 51개 mutate(keyword = map(keyword, str_split, pattern = &quot;,&quot;)) %&gt;% unnest(keyword) %&gt;% unnest(keyword) ai_news_sparse &lt;- ai_news_tidy %&gt;% count(news_id, keyword) %&gt;% cast_sparse(news_id, keyword, n) dim(ai_news_sparse) ## [1] 51 4869 ai_news_sparse[1:10, 100:110] ## 10 x 11 sparse Matrix of class \"dgCMatrix\" ## ## 01100101.20200211155933001 1 1 1 3 1 2 1 1 1 3 1 ## 01100101.20200311210122001 . . . . . . . . . . . ## 01100101.20200323091817001 . . . . . . . . . 1 . ## 01100101.20200727214230001 . . . . . . . . . 1 . ## 01100801.20200114104535002 . . . . . . . . . 1 . ## 01100801.20200128103005001 . . 1 . . 1 . . 2 1 . ## 01100801.20200618091524001 . . . . . . . . . 2 . ## 01100801.20200731103017001 . . . . . 1 9 1 1 . . ## 01100801.20200905145115001 . 1 1 . . 1 1 . 1 . . ## 01100901.20200114054513001 . . . . . 1 . . 1 1 . ai_news_tidy 데이터프레임은 탐색적 데이터 분석에 적합한 자료형태라 이를 cast_sparse() 함수를 사용해서 주제모형 개발에 적합한 형태로 변환시킨다. cast_sparse() 함수를 사용하는 이유는 동일한 정보를 저장하는데 cast_dtm()보다 저장공간을 절약하고 효율적이기 때문이다. 주제모형 개발작업흐름에 있어 개념적으로 봤을 때 큰 차이는 없다. 10.2.3.2 순차 말뭉치 방식 토큰화한 데이터프레임을 stm패키지 형식의 말뭉치로 변환한다. 이를 위해 먼저 분리된 토큰을 원래 문장에 속한 하나의 열로 저장한다. str_flatten()은 str_c()함수와 달리, 문자열을 결합해 단일 요소로 산출한다. str_c()함수에 collapse =인자를 사용한 경우에 해당한다. str_c()는 R기본함수 paste0()와 비슷하다. 결측값 처리방법이 서로 다르다. # install.packages(&quot;stm&quot;, dependencies = T) library(stm) combined_df &lt;- ai2_tk %&gt;% group_by(ID) %&gt;% summarise(text2 = str_flatten(word, &quot; &quot;)) %&gt;% ungroup() %&gt;% inner_join(ai2_df, by = &quot;ID&quot;) combined_df %&gt;% glimpse() ## Rows: 3,791 ## Columns: 8 ## $ ID &lt;fct&gt; 1, 2, 4, 5, 6, 7, 8, 9, 10, 12, 14, 15, 16,… ## $ text2 &lt;chr&gt; \"사모 펀드 책임 새해 금융 은성 금융 위원장 … ## $ 일자 &lt;chr&gt; \"20201231\", \"20201231\", \"20201231\", \"202012… ## $ text &lt;chr&gt; \"`사모펀드 책임` 놓고 새해부터 금융위 탓한 … ## $ 언론사 &lt;chr&gt; \"매일경제\", \"경향신문\", \"매일경제\", \"매일경… ## $ cat &lt;chr&gt; \"경제\", \"IT_과학\", \"경제\", \"경제\", \"IT_과학… ## $ month &lt;dbl&gt; 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,… ## $ press &lt;chr&gt; \"경제지\", \"종합지\", \"경제지\", \"경제지\", \"경… textProcessor()함수로 리스트 형식의 stm말뭉치로 변환한다. ‘documents’ ‘vacab’ ’meta’등의 하부요소가 생성된다. ’meta’에 텍스트데이터가 저장돼 있다. 영문문서의 경우, textProcessor()함수로 정제과정을 수행하므로, 한글문서처럼 별도의 형태소 추출과정 바로 영문데이터프레임을 투입하면 된다. ’text2’열에 토큰화한 단어가 저장돼 있다. 만일 tm패키지나 SnowballC패키지가 설치돼 있지 않으면 library(stm) processed &lt;- ai2_df %&gt;% textProcessor(documents = combined_df$text2, metadata = .) ## Building corpus... ## Converting to Lower Case... ## Removing punctuation... ## Removing stopwords... ## Removing numbers... ## Stemming... ## Creating Output... prepDocuments()함수로 주제모형에 사용할 데이터의 인덱스(wordcounts)를 만든다. out &lt;- prepDocuments(processed$documents, processed$vocab, processed$meta) ## Removing 1354 of 2677 terms (1354 of 18122 tokens) due to frequency ## Removing 16 Documents with No Words ## Your corpus now has 3747 documents, 1323 terms and 16768 tokens. 제거할수 있는 단어와 문서의 수를 plotRemoved()함수로 확인할 수 있다. plotRemoved(processed$documents, lower.thresh = seq(0, 100, by = 5)) lower.thresh =로 최소값 설정하면 빈도가 낮아 제거할 용어의 수를 설정할 수 있다. 설정값을 너무 높게 잡으면 분석의 정확도가 떨어진다. 여기서는 계산 편의를 위해 설정값을 높게 잡았다. out &lt;- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 15) ## Removing 2487 of 2677 terms (6204 of 18122 tokens) due to frequency ## Removing 179 Documents with No Words ## Your corpus now has 3584 documents, 190 terms and 11918 tokens. 산출결과를 개별 객체로 저장한다. 이 객체들은 이후 모형구축에 사용된다. docs &lt;- out$documents vocab &lt;- out$vocab meta &lt;- out$meta 10.3 분석 10.3.1 주제의 수(K) 설정 주제를 몇개로 설정할지 탐색한다. 7개와 10개를 놓고 비교해보자. searchK()함수는 주제의 수에 따라 4가지 계수를 제공한다. 배타성(exclusivity): 특정 주제에 등장한 단어가 다른 주제에는 나오지 않는 정도. 확산타당도에 해당. 의미 일관성(semantic coherence): 특정 주제에 높은 확률로 나타나는 단어가 동시에 등장하는 정도. 수렴타당도에 해당. 지속성(heldout likelihood): 데이터 일부가 존재하지 않을 때의 모형 예측 지속 정도. 잔차(residual): 투입한 데이터로 모형을 설명하지 못하는 오차. 배타성, 의미 일관성, 지속성이 높을 수록, 그리고 잔차가 작을수록 모형의 적절성 증가. 보통 10개부터 100개까지 10개 단위로 주제의 수를 구분해 연구자들이 정성적으로 최종 주제의 수 판단한다. 학습 상황이므로 계산시간을 줄이기 위해 주제의 수를 3개와 10개의 결과만 비교한다. (iteration을 200회 이상 수행하므로 계산시간이 오래 걸린다.) 백그라운드에서 계산하도록 하면, RStudio에서 다른 작업을 병행할 수 있다. RStudio 코드 백그라운드에서 돌리기 # topicN &lt;- seq(from = 10, to = 100, by = 10) topicN &lt;- c(3, 10) topicN_storage &lt;- searchK(out$documents, out$vocab, K = topicN) topicN_storage %&gt;% write_rds(&quot;data/topicN_storage.rds&quot;) 작업 시간이 오래 걸리기 때문에 작업결과를 로컬 디스크에 저장하고 나서 이를 다시 가져와서 적절한 주제 선정에 대한 작업결과를 시각적으로 확인한다. topicN_storage &lt;- read_rds(&quot;data/topicN_storage.rds&quot;) plot(topicN_storage) 배타성, 지속성, 잔차 등 3개 지표에서 모두 주제의 수가 10개인 모형이 3개인 모형보다 우수하고, 3개인 모형은 의미일관성만 높다. 따라서 이미 10개로 분석한 모형을 그대로 이용한다. 10.3.2 기계학습 방식 적절한 주제수 K를 선정하는 다른 방식을 기계학습모형 개발에 사용하는 하이퍼 패러미터(hyper parameter) 를 격자탐색, 무작위 탐색 등 다양한 방식이 있지만 기본 개념은 패러미터를 달리하여, 즉 주제수(K)를 달리하여 데이터에 주제모형을 적합시켜 가장 좋은 주제모형 특성을 갖는 K를 선정하는 방식이다. 개념상 단순하며 직관적인데 이에 대한 단점은 시간이 오래 걸린다는 점이다. 이를 보완하기 위해서 최근 컴퓨터가 멀티코어 병렬처리가 가능하기 때문에 furrr 패키지를 사용하여 병렬처리를 통해 이런 단점을 보완한다. 따라서, 주제수 K 를 선정하기 위해서 기계학습 방식을 도입하여 지정된 K에 대해 모두 주제모형 적합을 수행하고 학습 시간을 단축하기 위해서 병렬컴퓨팅을 통해 신속히 결과를 취합하고 이를 후속 시각화 작업을 통해 최적의 K 를 선정한다. library(stm) # stm() 토픽모형 기본 # ai_topic_model &lt;- stm(ai_news_sparse, K = 10, verbose = FALSE) # 병렬처리 준비 library(furrr) plan(multisession) # 주제수 선정: 1 ~ 10 topic_tbl &lt;- tibble(K = c(2:10)) # 주제수를 달리하여 모형 적합 many_models &lt;- topic_tbl %&gt;% mutate(topic_model = future_map(K, ~stm(ai_news_sparse, K = ., verbose = FALSE))) heldout &lt;- make.heldout(ai_news_sparse) k_result &lt;- many_models %&gt;% mutate(exclusivity = map(topic_model, exclusivity), semantic_coherence = map(topic_model, semanticCoherence, ai_news_sparse), eval_heldout = map(topic_model, eval.heldout, heldout$missing), residual = map(topic_model, checkResiduals, ai_news_sparse), bound = map_dbl(topic_model, function(x) max(x$convergence$bound)), lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)), lbound = bound + lfact, iterations = map_dbl(topic_model, function(x) length(x$convergence$bound))) k_result %&gt;% write_rds(&quot;data/k_result.rds&quot;) Held-out likelihood, Residuals, Semantic coherence 값이 일치된 주제수 K에 대한 정보를 제공하지 않지만 대략 6~9 주제가 적절한 주제로 사료된다. 이를 바탕으로 주제수를 선정하여 텍스트 데이터에 모형을 적합시키고 해석하는 과정을 거쳐 최종적으로 소통에 필요한 후속 작업을 수행한다. k_result &lt;- read_rds(&quot;data/k_result.rds&quot;) k_result %&gt;% transmute(K, `Lower bound` = lbound, Residuals = map_dbl(residual, &quot;dispersion&quot;), `Semantic coherence` = map_dbl(semantic_coherence, mean), `Held-out likelihood` = map_dbl(eval_heldout, &quot;expected.heldout&quot;)) %&gt;% gather(Metric, Value, -K) %&gt;% ggplot(aes(K, Value, color = Metric)) + geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) + facet_wrap(~Metric, scales = &quot;free_y&quot;) + labs(x = &quot;K (주제 수)&quot;, y = NULL, title = &quot;주제 수(K)를 달리한 모형 검진 통계량&quot;, subtitle = &quot;각 검진 통계량이 서로 다른 정보를 제공하고 있지만 대략 6 주제로 선정&quot;) 10.3.3 주제모형 구성 docs, vocab, meta에 저장된 문서와 텍스트정보를 이용해 주제모형을 구성한다. 추출한 주제의 수는 K =인자로 설정한다. 처음에는 임의의 값을 투입한다. 이후 적절한 주제의 수를 다시 추정하는 단계가 있다. 모형 초기값은 init.type =인자에 “Spectral”을 투입한다. 같은 결과가 나오도록 하려면 seed =인자를 지정한다. 투입하는 값은 개인의 선호대로 한다. stm_fit &lt;- stm( documents = docs, vocab = vocab, K = 10, # 토픽의 수 data = meta, init.type = &quot;Spectral&quot;, seed = 37 # 반복실행해도 같은 결과가 나오게 난수 고정 ) summary(stm_fit) %&gt;% glimpse() summary(stm_fit) stm패키지는 4종의 가중치를 이용해 주제별로 주요 단어를 제시한다. Highest probability: 각 주제별로 단어가 등장할 확률이 높은 정도. 베타() 값. FREX: 전반적인 단어빈도의 가중 평균. 해당 주제에 배타적으로 포함된 정도. Lift: 다른 주제에 덜 등장하는 정도. 해당 주제에 특정된 정도. score: 다른 주제에 등장하는 로그 빈도. 해당 주제에 특정된 정도. FREX와 Lift는 드문 빈도의 단어도 분류하는 경향이 있으므로, 주로 Highest probability(베타)를 이용한다. stm패키지의 자세한 내용은 패키지 저자 Roberts의 stm홈페이지와 해설 논문을 참조한다. stm: R Package for Structural Topic Models Roberts, M. E., Stewart, B. M., &amp; Airoldi, E. M. (2016). A model of text for experimentation in the social sciences. Journal of the American Statistical Association, 111(515), 988-1003. 10.4 소통 분석한 모형을 통해 말뭉치에 포함된 주제와 주제별 단어의 의미가 무엇인지 전달하기 위해서는 우선 모형에 대한 해석을 제시할 수 있는 시각화가 필요하다. 이를 위해서는 먼저 주요 계수의 의미를 이해할 필요가 있다. 주제모형에서 주제별 확률분포를 나타내는 베타와 감마다. 베타 \\(\\beta\\): 단어가 각 주제에 등장할 확률. 각 단어별로 베타 값 부여. stm모형 요약에서 제시한 Highest probability의 지표다. 감마 \\(\\gamma\\): 문서가 각 주제에 등장할 확률. 각 문서별로 감마 값 부여. 즉, 베타와 감마 계수를 이용해 시각화하면 주제와 주제단어의 의미를 간명하게 나타낼 수 있다. 먼저 tidy()함수를 이용해 stm()함수로 주제모형을 계산한 결과를 정돈텍스트 형식으로 변환한다(줄리아 실기의 시각화 참고) 학습편의를 위해 주제의 수롤 6개로 조정해 다시 모형을 구성하자. stm_fit &lt;- stm( documents = docs, vocab = vocab, K = 6, # 토픽의 수 data = meta, init.type = &quot;Spectral&quot;, seed = 37, # 반복실행해도 같은 결과가 나오게 난수 고정 verbose = F ) summary(stm_fit) %&gt;% glimpse() ## A topic model with 6 topics, 3584 documents and a 190 word dictionary. ## List of 5 ## $ prob : chr [1:6, 1:7] \"플랫폼\" \"시스템\" \"글로벌\" \"서비스\" ... ## $ frex : chr [1:6, 1:7] \"대학교\" \"시스템\" \"건조기\" \"스마트\" ... ## $ lift : chr [1:6, 1:7] \"에어컨\" \"청소기\" \"건조기\" \"어르신\" ... ## $ score : chr [1:6, 1:7] \"에어컨\" \"청소기\" \"건조기\" \"서비스\" ... ## $ topicnums: int [1:6] 1 2 3 4 5 6 ## - attr(*, \"class\")= chr \"labelTopics\" summary(stm_fit) ## A topic model with 6 topics, 3584 documents and a 190 word dictionary. ## Topic 1 Top Words: ## Highest Prob: 플랫폼, 대학교, 지난해, 자동차, 브랜드, 위원회, 통신부 ## FREX: 대학교, 코스닥, 사이버, 플랫폼, 위원회, 브랜드, 스피커 ## Lift: 에어컨, 사이버, 위원장, 코스닥, 대학교, 전시관, 플러스 ## Score: 에어컨, 플랫폼, 대학교, 자동차, 지난해, 브랜드, 학년도 ## Topic 2 Top Words: ## Highest Prob: 시스템, 투자자, 일자리, 모빌리티, 실시간, 경쟁력, 소프트 ## FREX: 시스템, 디자인, 일자리, 투자자, 청소기, 알고리즘, 중공업 ## Lift: 청소기, 디자인, 중공업, 베스트, 시스템, 알고리즘, 자동화 ## Score: 청소기, 시스템, 투자자, 디자인, 일자리, 모빌리티, 소프트 ## Topic 3 Top Words: ## Highest Prob: 글로벌, 건조기, 인터넷, 세탁기, 콘텐츠, 소비자, 컴퓨터 ## FREX: 건조기, 세탁기, 콘텐츠, 소비자, 그랑데, 핀테크, 글로벌 ## Lift: 건조기, 개개인, 고등학교, 세탁기, 그랑데, 콘텐츠, 한국어 ## Score: 건조기, 세탁기, 글로벌, 그랑데, 콘텐츠, 소비자, 컴퓨터 ## Topic 4 Top Words: ## Highest Prob: 서비스, 스마트, 온라인, 마케팅, 에너지, 중소기업, 전시회 ## FREX: 스마트, 에너지, 목소리, 이벤트, 중소기업, 마케팅, 컴퍼니 ## Lift: 어르신, 운전자, 이벤트, 목소리, 스마트, 컴퍼니, 에너지 ## Score: 서비스, 운전자, 스마트, 온라인, 에너지, 마케팅, 목소리 ## Topic 5 Top Words: ## Highest Prob: 코로나, 디지털, 바이러스, 대통령, 감염증, 소프트웨어, 가운데 ## FREX: 바이러스, 감염증, 코로나, 디지털, 대통령, 가운데, 컨퍼런스 ## Lift: 대유행, 바이러스, 어려움, 감염증, 과학자, 학술지, 포스트 ## Score: 코로나, 디지털, 대유행, 바이러스, 감염증, 대통령, 소프트웨어 ## Topic 6 Top Words: ## Highest Prob: 데이터, 스타트업, 반도체, 연구원, 프로그램, 대학원, 카카오 ## FREX: 반도체, 스타트업, 카카오, 대학원, 데이터, 차세대, 비디아 ## Lift: 반도체, 메모리, 비디아, 배터리, 부회장, 스타트업, 차세대 ## Score: 반도체, 데이터, 스타트업, 대학원, 연구원, 프로그램, 부동산 10.4.1 주제별 단어 분포 베타 값을 이용해 주제별로 단어의 분포를 막대도표로 시각화하자. td_beta &lt;- stm_fit %&gt;% tidy(matrix = &#39;beta&#39;) td_beta %&gt;% group_by(topic) %&gt;% slice_max(beta, n = 7) %&gt;% ungroup() %&gt;% mutate(topic = str_c(&quot;주제&quot;, topic)) %&gt;% ggplot(aes(x = beta, y = reorder_within(term, beta, topic), fill = topic)) + geom_col(show.legend = F) + scale_y_reordered() + facet_wrap(~topic, scales = &quot;free&quot;) + labs(x = expression(&quot;단어 확률분포: &quot;~beta), y = NULL, title = &quot;주제별 단어 확률 분포&quot;, subtitle = &quot;각 주제별로 다른 단어들로 군집&quot;) + theme(plot.title = element_text(size = 20)) 10.4.2 주제별 문서 분포 감마 값을 이용해 주제별로 문서의 분포를 히스토그램으로 시각화한다. x축과 y축에 각각 변수를 투입하는 막대도표와 달리, 히스토그램은 x축에만 변수를 투입하고, y축에는 x축 값을 구간(bin)별로 요약해 표시한다. 각 문서가 각 주제별로 등장할 확률인 감마(\\(\\gamma\\))의 분포가 어떻게 히스토그램으로 표시되는지 살펴보자. td_gamma &lt;- stm_fit %&gt;% tidy(matrix = &quot;gamma&quot;) td_gamma %&gt;% glimpse() ## Rows: 21,504 ## Columns: 3 ## $ document &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13… ## $ topic &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ gamma &lt;dbl&gt; 0.4677, 0.2027, 0.2440, 0.1080, 0.1209, 0… td_gamma %&gt;% mutate(max = max(gamma), min = min(gamma), median = median(gamma)) ## # A tibble: 21,504 × 6 ## document topic gamma max min median ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0.468 0.835 0.0225 0.120 ## 2 2 1 0.203 0.835 0.0225 0.120 ## 3 3 1 0.244 0.835 0.0225 0.120 ## 4 4 1 0.108 0.835 0.0225 0.120 ## 5 5 1 0.121 0.835 0.0225 0.120 ## 6 6 1 0.149 0.835 0.0225 0.120 ## # … with 21,498 more rows td_gamma에는 문서별로 감마 값이 부여돼 있다. 1번 문서는 주제1에 포함될 확률(감마)이 0.4이고, 2번 문서는 주제1에 포함될 확률이 0.2다. 감마 값은 최저 0.04에서 최고 0.74까지 있다. 감마는 연속적인 값이므로 이 감마의 값을 일정한 구간(bin)으로 나누면, 각 감마의 구간에 문서(document)가 몇개 있는지 계산해, 감마 값에 따른 각 문서의 분포를 구할 수 있다. 연속하는 값을 구간(bin)으로 구분해 분포를 표시한 도표가 히스토그램이다. 주제별로 문서의 분포를 감마 값에 따라 히스토그램으로 시각해하자. geom_histogram()함수에서 bins =인자의 기본값은 30이다. 즉, bin을 30개로 나눠 분포를 그린다. td_gamma %&gt;% ggplot(aes(x = gamma, fill = as.factor(topic))) + geom_histogram(bins = 100, show.legend = F) + facet_wrap(~topic) + labs(title = &quot;주제별 문서 확률 분포&quot;, y = &quot;문서(기사)의 수&quot;, x = expression(&quot;문서 확률분포: &quot;~(gamma))) + theme(plot.title = element_text(size = 20)) 감마가 높은 문서(기사)가 많지 않고, 대부분 낮은 값에 치우쳐 있다. ‘인공지능’ 단일 검색어로 추출한 말뭉치이기 때문이다. 10.4.3 주제별 단어-문서 분포 주제별로 감마의 평균값을 구하면 비교적 각 주제와 특정 문서와 관련성이 높은 순서로 주제를 구분해 표시할 수 있다. 또한 각 주제 별로 대표 단어를 표시할 수 있다. 가장 간단하게 주제별로 단어와 문서의 분포를 표시하는 방법은 stm패키지에서 제공하는 plot()함수다. stm plot()함수는 type =인자에 ‘summary’ ‘labels’ ‘perspective’ ‘hist’ 등을 투입해 다양한 방식으로 결과를 탐색할 수 있다. 기본적인 정보는 ’summary’를 통해 제시한다. plot(stm_fit, type = &quot;summary&quot;, n = 5) 위 결과를 ggplot2 패키지로 시각화하는 방법은 다음과 같다. 주제별 상위 5개 단어 추출해 데이터프레임에 저장. 문서의 감마 평균값 주제별로 계산해 주제별 상위 단어 데이터프레임과 결합 10.4.3.1 주제별 상위 5개 단어 추출 td_beta에서 주제별로 상위 5개 단어 추출해 top_terms에 할당한다. 각 주제별로 그룹을 묶어 list형식으로 각 상위단어 5개를 각 주제에 리스트로 묶어준 다음, 다시 데이터프레임의 열로 바꿔준다. top_terms &lt;- td_beta %&gt;% group_by(topic) %&gt;% slice_max(beta, n = 5) %&gt;% select(topic, term) %&gt;% summarise(terms = str_flatten(term, collapse = &quot;, &quot;)) 10.4.3.2 주제별 감마 평균 계산 td_gamma에서 각 주제별 감마 평균값 계산해 top_terms(주제별로 추출한 상위 5개 단어 데이터프레임)와 결합해 gamma_terms에 할당한다. gamma_terms &lt;- td_gamma %&gt;% group_by(topic) %&gt;% summarise(gamma = mean(gamma)) %&gt;% left_join(top_terms, by = &#39;topic&#39;) %&gt;% mutate(topic = str_c(&quot;주제&quot;, topic), topic = reorder(topic, gamma)) 결합한 데이터프레임을 막대도표에 표시한다. 문서 확률분포 평균값과 주제별로 기여도가 높은 단어를 표시한다. 주제별로 문서의 확률분포와 단어의 확률분포를 한눈에 볼수 있다. X축을 0에서 1까지 설정한 이유는 구간을 국소로 설정할 경우, 막대도표가 크기가 상대적으로 크게 보여 결과적으로 데이터의 왜곡이 되기 때문이다. gamma_terms %&gt;% ggplot(aes(x = gamma, y = topic, fill = topic)) + geom_col(width = 0.5, show.legend = F) + geom_text(aes(label = round(gamma, 2)), # 소수점 2자리 hjust = 1.4) + # 라벨을 막대도표 안쪽으로 이동 geom_text(aes(label = terms), hjust = -0.05) + # 단어를 막대도표 바깥으로 이동 scale_x_continuous(expand = c(0, 0), # x축 막대 위치를 Y축쪽으로 조정 limit = c(0, 1)) + # x축 범위 설정 labs(x = expression(&quot;문서 확률분포&quot;~(gamma)), y = NULL, title = &quot;인공지능 관련보도 상위 주제어&quot;, subtitle = &quot;주제별로 기여도가 높은 단어 중심&quot;) + theme(plot.title = element_text(size = 20)) + theme_light() 10.5 과제 관심있는 검색어를 이용해 빅카인즈에서 기사를 검색해 수집한 기사의 주제모형을 구축한다. 구축한 주제 모형에 대해 시각화한다(주제별 단어분포, 주제별 문서분포, 주제별 단어-문서 분포) "],["anal4topic.html", "11 . 주제모형(공변인) 11.1 주제 명명과 공변인 주제모형 11.2 자료 준비 11.3 분석 11.4 주제 이름짓기 11.5 공변인 분석 11.6 과제", " 11 . 주제모형(공변인) 11.1 주제 명명과 공변인 주제모형 11.1.1 개관 주제모형은 기계학습의 비지도학습에 해당한다. 기계가 인간의 ‘지도’를 받지 않고 ’스스로’ 자료에서 일정한 규칙을 찾아 비슷한 유형끼리 군집하는 학습방식이다. 인간이 데이터셋을 미리 분류한 정보를 투입하지 않기 때문에, 기계가 분류한 군집에 대해 인간이 사후적으로 의미를 추론해야 한다. 이번 장에서는 기계가 도출한 각 주제의 주요 단어와 문서를 통해 주제의 의미를 추론하는 방법으로서의 주제 명명에 대해 학습한다. 이와 함께, 메타데이터를 이용한 공변인(covariate) 주제모형 분석에 대해서도 학습한다. 메타데이터는 데이터에 대한 데이터다. 예를 들어, 말뭉치에 포함된 문서의 유형(예: 소설, 논설), 분류(예: 사회면, 정치면), 소속(예: 언론사), 시기(예: 연, 월, 주)에 대한 정보가 메타데이터다. 이 메타데이터를 변수로서 투입해 분석하면 말뭉치의 주제에 대해 보다 의미있는 분석이 가능하다. 예를 들어, 기간대별로 말뭉치의 주제가 어떻게 변하는지, 혹은 문서의 분류에 따라 주제가 어떻게 다른지 등을 분석할 수 있다. 주제의 구조적인 측면은 다룬다고 해서 구조적 주제모형(structural topic models)이라고 한다. 공변인을 투입한 주제모형 분석이므로 여기서는 동적 주제모형과 구조적 주제모형을 모두 공변인 주제모형(Covariates topic models)이라고 하겠다. stm패키지와 keyATM패키지는 말뭉치의 메타데이터를 공변인으로 투입해 말뭉치의 주제에 대한 회귀분석 기능을 제공한다. keyATM이 보다 최근의 패키지라 보다 다양한 기능이 있지만, 윈도에서 멀티바이트문자를 지원하지 않아 윈도에서는 한글문서를 분석할 수 없다는 단점이 있다. pkg_v &lt;- c(&quot;tidyverse&quot;, &quot;tidytext&quot;, &quot;stm&quot;, &quot;lubridate&quot;) purrr::map(pkg_v, require, ch = TRUE) ## [[1]] ## [1] TRUE ## ## [[2]] ## [1] TRUE ## ## [[3]] ## [1] TRUE ## ## [[4]] ## [1] TRUE 11.2 자료 준비 11.2.1 수집 빅카인즈의 ‘뉴스분석’ 메뉴에서 ‘뉴스검색·분석’을 선택한 다음, ’상세검색’을 클릭한다. 상세검색은 다양한 기준으로 검색할 수 있다. 검색유형 기본값은 ’뉴스’, 검색어처리 기본값은 ‘형태소분석’, 검색어범위 기본값은 ’제목+본문’이다. 모두 기본값으로 검색한다. 기간: 2021-01-01 - 2021-03-31 검색어: ‘백신’, ‘코로나19’, ‘신종 코로나’, ‘신종코로나’, ‘우한 폐렴’, ‘우한폐렴’, ‘바이러스’ (쉼표로 분리하면 각 검색어를 ‘OR’ 연산자로 검색이 된다.) 언론사: 경향신문, 조선일보, 중앙일보, 한겨레 통합분류: 정치, 경제, 사회, 국제, 지역, IT_과학 분석: 분석기사 (분석기사를 선택하면 중복(반복되는 유사도 높은 기사)과 예외(인사 부고 동정 포토)가 검색에서 제외된다. 모두 14,097건이다. 다운로드한 엑셀파일을 작업디렉토리 아래 data폴더로 복사한다. data폴더에서 ‘News’로 시작해서’.xlsx’로 끝나는 파일명만 표시해 보자. list.files(path = &#39;data&#39;, pattern = &#39;^News.*20210101.*\\\\.xlsx$&#39;) ## [1] \"NewsResult_20210101-20210330.xlsx\" 데이터셋의 파일명이 ’NewsResult_20210101-20210330.xlsx’이다. readxl::read_excel(&quot;data/NewsResult_20210101-20210330.xlsx&quot;) %&gt;% names() ## [1] \"뉴스 식별자\" ## [2] \"일자\" ## [3] \"언론사\" ## [4] \"기고자\" ## [5] \"제목\" ## [6] \"통합 분류1\" ## [7] \"통합 분류2\" ## [8] \"통합 분류3\" ## [9] \"사건/사고 분류1\" ## [10] \"사건/사고 분류2\" ## [11] \"사건/사고 분류3\" ## [12] \"인물\" ## [13] \"위치\" ## [14] \"기관\" ## [15] \"키워드\" ## [16] \"특성추출(가중치순 상위 50개)\" ## [17] \"본문\" ## [18] \"URL\" ## [19] \"분석제외 여부\" 분석에 필요한 열을 선택해 데이터프레임으로 저장한다. 분석 텍스트는 제목과 본문이다. 제목은 본문의 핵심 내용을 반영하므로, 제목과 본문을 모두 주제모형 분석에 투입한다. 시간별, 언론사별, 분류별로 분석할 계획이므로, 해당 열을 모두 선택한다. 키워드 열은 빅카인즈가 본문에서 추출한 키워드 중 단순 숫자, 이메일주소, 시간이 아닌 단어 등이다. 빅카인즈의 형태소분석결과를 이용할 계획이므로 키워드 열도 선택한다. 빅카인즈는 본문을 200자까지만 무료로 제공하지만, 빅카인즈에서 형태소분석을 통해 추출한 키워드는 기사 전문에서 추출한 결과다. 키워드를 이용하면 기사 전문을 이용하는 효과가 있다. vac_df &lt;- readxl::read_excel(&quot;data/NewsResult_20210101-20210330.xlsx&quot;) %&gt;% select(일자, 제목, 본문, 언론사, cat = `통합 분류1`, 키워드) vac_df %&gt;% head(3) ## # A tibble: 3 × 6 ## 일자 제목 본문 언론사 cat 키워드 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 20210330 美 국채 금리 상승에 뉴… \"정… 조선… 경제… 국채,… ## 2 20210330 코로나 변이 바이러스 … \"전… 경향… IT_… 코로… ## 3 20210330 “숲 통해 국민 심신 치… \"ㆍ… 경향… 지역… 치유,… 11.2.2 (선택) 표집 LDA 모형은 베이지언 모형이므로 사후확률의 근사치를 주어진 자료로부터 반복적으로 계산해 주제를 추론한다. 복잡한 계산을 반복적으로 수행하기 때문에 컴퓨터 사양이 낮은 경우 분석이 매우 느릴 수 있다. 데이터 크기와 컴퓨터 서능에 따라 수십분 혹은 수시간 이상 소요될 수 있다. 주제모형 분석 방법 학습 맥락에서 시간 절약을 위해 데이터셋의 일부만 추려 분석에 활용한다. 아래 코드는 1만4천개 행에서 3천개행을 무작위로 표집했다. 다만, set.seed(37)을 지정하여 무작위 기사추출에 대한 재현성을 확보했다. set.seed(37) vac_sample_df &lt;- vac_df %&gt;% sample_n(size = 3000) vac_df %&gt;% glimpse() 기사 본문과 키워드를 비교해보자. vac_df %&gt;% pull(키워드) %&gt;% head(1) ## [1] \"국채,금리,상승,출발,뉴욕,증시,소폭,하락,정체중,미국,국채,금리,상승세,미국,증시,소폭,약세,출발,마무리,트레이더들,미국,대통령,인프라,투자,계획,매도,국채,30일,현지시간,기준,미국,뉴욕,증시,지수,하락,거래,다우평균,0.24%,S&amp;P,0.35%,나스닥,0.81%,거래,0.81%,채권시장,이날,뉴욕,채권,시장,개장,전자상,거래,만기,국채,금리,하루,0.06%,포인트,여파,코로나,바이러스,1월,수준,기록,미국,정부,사회,인프라,투자,패키지,법안,공개,예정,법안,사회,인프라,투자,패키지,3조,달러,사회,인프라,투자,전망,인플레이션,가능,부각,국채,금리,상승,압력,작용,가능성,국채,금리,상승,이외,매니저,한국,해지펀드,Bill,Hwang,운용,펀드,이케고스,투자,주식,하락,여파,이케고스,지난주,마진콜,계약,가격,변화,부족,증거금,추가,납부,요구,블록딜,일반,주식,거래,양측,거래,상대방,대량,거래,시장,영향,가격,합의,정규장,거래,비아콤,CBS,디스커버리,주식,주식,낙차,발생,전문가들,증시,예상,예측,야후,파이낸스,30일,시황,보도,고용,동향,금요일,전망,이달,63만,일자리,창출,예상,수치,시작,코로나,펜데믹,수치\" vac_df %&gt;% pull(제목) %&gt;% head(1) ## [1] \"美 국채 금리 상승에 뉴욕 증시 소폭 하락 출발\" vac_df %&gt;% pull(본문) %&gt;% head(1) ## [1] \"정체중이었던 미국 국채금리가 다시 상승세를 보이면서 미국 증시가 소폭 약세를 보이며 출발했다. 1분기 마무리를 앞둔 트레이더들이 조 바이든 미국 대통령의 대규모 인프라 투자 계획 발표 전에 미 국채 매도를 늘리고 있다는 분석이 나온다. \\n \\n30일 오전 9시 40분(현지시간) 기준 미국 뉴욕 증시 3대 지수는 하락한채 거래 중이다. 다우평균은 전날보다 ..\" 분석 목적에 맞게 열을 재구성한다. 언론사는 ’여당지’와 ’야당지’로 구분한다. 분류는 ’사회면’와 ’비사회면’로 나눈다. vac2_df &lt;- vac_df %&gt;% # 중복기사 제거 distinct(제목, .keep_all = TRUE) %&gt;% # 기사별 ID부여 mutate(ID = factor(row_number())) %&gt;% # 월별로 구분한 열 추가(lubridate 패키지) mutate(week = week(ymd(일자))) %&gt;% # 기사 제목과 본문 결합 unite(제목, 본문, col = &quot;text&quot;, sep = &quot; &quot;) %&gt;% # 중복 공백 제거 mutate(text = str_squish(text)) %&gt;% # 언론사 구분: 야당지, 여당지 %&gt;% mutate(press = case_when( 언론사 == &quot;조선일보&quot; ~ &quot;야당지&quot;, 언론사 == &quot;중앙일보&quot; ~ &quot;야당지&quot;, 언론사 == &quot;경향신문&quot; ~ &quot;여당지&quot;, TRUE ~ &quot;여당지&quot;) ) %&gt;% # 기사 분류 구분 separate(cat, sep = &quot;&gt;&quot;, into = c(&quot;cat&quot;, &quot;cat2&quot;)) %&gt;% # IT_과학, 경제, 사회 만 선택 select(-cat2) %&gt;% # 분류 구분: 사회, 비사회 mutate(catSoc = case_when( cat == &quot;사회&quot; ~ &quot;사회면&quot;, cat == &quot;지역&quot; ~ &quot;사회면&quot;, TRUE ~ &quot;비사회면&quot;) ) vac2_df %&gt;% glimpse() ## Rows: 14,090 ## Columns: 9 ## $ 일자 &lt;chr&gt; \"20210330\", \"20210330\", \"20210330\", \"202103… ## $ text &lt;chr&gt; \"美 국채 금리 상승에 뉴욕 증시 소폭 하락 출… ## $ 언론사 &lt;chr&gt; \"조선일보\", \"경향신문\", \"경향신문\", \"경향신… ## $ cat &lt;chr&gt; \"경제\", \"IT_과학\", \"지역\", \"IT_과학\", \"지역… ## $ 키워드 &lt;chr&gt; \"국채,금리,상승,출발,뉴욕,증시,소폭,하락,정… ## $ ID &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, … ## $ week &lt;dbl&gt; 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,… ## $ press &lt;chr&gt; \"야당지\", \"여당지\", \"여당지\", \"여당지\", \"여… ## $ catSoc &lt;chr&gt; \"비사회면\", \"비사회면\", \"사회면\", \"비사회면… 새로 생성된 열의 기사 양을 계산해보자. vac2_df %&gt;% count(cat, sort = TRUE) ## # A tibble: 8 × 2 ## cat n ## &lt;chr&gt; &lt;int&gt; ## 1 사회 3733 ## 2 경제 2891 ## 3 국제 2164 ## 4 정치 2088 ## 5 지역 1921 ## 6 IT_과학 915 ## # … with 2 more rows 문화와 스포츠를 검색단계서 선택하지 않았음에도 데이터셋에 포함된 이유는 하부 분류에 포함돼 있었기 때문이다. vac2_df %&gt;% count(catSoc, sort = TRUE) ## # A tibble: 2 × 2 ## catSoc n ## &lt;chr&gt; &lt;int&gt; ## 1 비사회면 8436 ## 2 사회면 5654 비사회면 8,436건, 사회면 5,653건으로 문서 수에서 큰 차이가 나지 않는다. vac2_df %&gt;% count(week) ## # A tibble: 13 × 2 ## week n ## &lt;dbl&gt; &lt;int&gt; ## 1 1 1491 ## 2 2 1299 ## 3 3 1229 ## 4 4 1379 ## 5 5 1219 ## 6 6 927 ## # … with 7 more rows vac2_df %&gt;% count(press, sort = TRUE) ## # A tibble: 2 × 2 ## press n ## &lt;chr&gt; &lt;int&gt; ## 1 야당지 8043 ## 2 여당지 6047 월 및 언론사 구분에서도 문서 수에서 큰 차이가 나지 않는다. 11.2.3 정제 11.2.3.1 토큰화 빅카인즈의 형태소분석이 된 키워드를 이용하므로 이미 토큰화가 된 상태이나, 본 분석에 앞서 단어의 빈도 등을 검토하기 위해 정돈텍스트 형식으로 변경하기 위해 토큰화를 진행한다. ‘text’열이 아니라 ’키워드’열의’,’를 기준으로 토큰화한다. 토큰화하기 전 문자, 숫자, 쉼표 이외의 요소를 제거한다. 전각문자는 문자가 아님에도 정규표현식으로 걸리지지 않으므로 추가로 제거한다. 추가로 지워야 하는 주요 기호 : ㆍㅣ‘’“” ○ ● ◎ ◇ ◆ □ ■ △ ▲ ▽ ▼ 〓 ◁ ◀ ▷ ▶ ♤ ♠ ♡ ♥ ♧ ♣ ⊙ ◈ ▣ 이외에도 빈도 분석을 통해 정규표현식으로 걸러지지 않은 기호가 나오면 추가로 제거한다. &quot;!@#$... 전각ㆍㅣ문자 %^&amp;*()&quot; %&gt;% str_remove(&quot;\\\\w+&quot;) ## [1] \"!@#$... %^&amp;*()\" fullchar_v &lt;- &quot;ㆍ|ㅣ|‘|’|“|”|○|●|◎|◇|◆|□|■|△|▲|▽|▼|〓|◁|◀|▷|▶|♤|♠|♡|♥|♧|♣|⊙|◈|▣&quot; vac_tk &lt;- vac2_df %&gt;% mutate(키워드 = str_remove_all(키워드, &quot;[^(\\\\w+|\\\\d+|,)]&quot;)) %&gt;% mutate(키워드 = str_remove_all(키워드, fullchar_v)) %&gt;% unnest_tokens(word, 키워드, token = &quot;regex&quot;, pattern = &quot;,&quot;) vac_tk %&gt;% arrange(ID) %&gt;% head(30) ## # A tibble: 30 × 9 ## 일자 text 언론사 cat ID week press catSoc word ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 20210330 美 … 조선… 경제 1 13 야당… 비사… 국채 ## 2 20210330 美 … 조선… 경제 1 13 야당… 비사… 금리 ## 3 20210330 美 … 조선… 경제 1 13 야당… 비사… 상승 ## 4 20210330 美 … 조선… 경제 1 13 야당… 비사… 출발 ## 5 20210330 美 … 조선… 경제 1 13 야당… 비사… 뉴욕 ## 6 20210330 美 … 조선… 경제 1 13 야당… 비사… 증시 ## # … with 24 more rows vac_tk %&gt;% arrange(ID) %&gt;% tail(30) ## # A tibble: 30 × 9 ## 일자 text 언론사 cat ID week press catSoc word ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 20210101 \"\\\"… 경향… 사회 14090 1 여당… 사회면 온라… ## 2 20210101 \"\\\"… 경향… 사회 14090 1 여당… 사회면 자가 ## 3 20210101 \"\\\"… 경향… 사회 14090 1 여당… 사회면 진단 ## 4 20210101 \"\\\"… 경향… 사회 14090 1 여당… 사회면 제출 ## 5 20210101 \"\\\"… 경향… 사회 14090 1 여당… 사회면 선생… ## 6 20210101 \"\\\"… 경향… 사회 14090 1 여당… 사회면 부재… ## # … with 24 more rows 11.2.3.2 불용어 처리 빅카인즈의 형태소분석 결과를 이용하므로 별도의 불용어처리는 불필요하나, 의미없는 고빈도 단어를 선별할 필요가 있다. 단어의 총빈도를 계산해본다. ’백신’과 ’코로나19’의 빈도가 높다. 어느 한 단어가 압도적인 비중을 차지하는 것이 아니므로 제거하지 않고 그대로 둔다. ’코로나’는 ’코로나19’라는 질병의 이름으로 사용됐을수 있고, ’코로나바이러스’라는 병인으로 사용됐을수도 있으므로, ’코로나19’와 병합하지 말고 그대로 둔다. count_df &lt;- vac_tk %&gt;% count(word, sort = T) count_df %&gt;% head(40) ## # A tibble: 40 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 백신 34189 ## 2 코로나19 32204 ## 3 접종 24660 ## 4 정부 14813 ## 5 미국 12322 ## 6 코로나 12120 ## # … with 34 more rows count_df %&gt;% tail(40) ## # A tibble: 40 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 히팅 1 ## 2 힉스 1 ## 3 힌덴버그리서치 1 ## 4 힌두경전 1 ## 5 힌두교식 1 ## 6 힌두트바 1 ## # … with 34 more rows 11.2.4 stm말뭉치 stm()함수에서 처리하는 데이터는 각 기사의 토큰이 하나의 열에 함께 있어야 한다. 정돈텍스트형식은 한개의 열에 하나의 토큰만 있으므로 str_flatten()함수로 하나의 열에 결합한다. combined_df &lt;- vac_tk %&gt;% group_by(ID) %&gt;% summarise(text2 = str_flatten(word, &quot; &quot;)) %&gt;% ungroup() %&gt;% inner_join(vac2_df, by = &quot;ID&quot;) combined_df %&gt;% glimpse() ## Rows: 14,090 ## Columns: 10 ## $ ID &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, … ## $ text2 &lt;chr&gt; \"국채 금리 상승 출발 뉴욕 증시 소폭 하락 정… ## $ 일자 &lt;chr&gt; \"20210330\", \"20210330\", \"20210330\", \"202103… ## $ text &lt;chr&gt; \"美 국채 금리 상승에 뉴욕 증시 소폭 하락 출… ## $ 언론사 &lt;chr&gt; \"조선일보\", \"경향신문\", \"경향신문\", \"경향신… ## $ cat &lt;chr&gt; \"경제\", \"IT_과학\", \"지역\", \"IT_과학\", \"지역… ## $ 키워드 &lt;chr&gt; \"국채,금리,상승,출발,뉴욕,증시,소폭,하락,정… ## $ week &lt;dbl&gt; 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,… ## $ press &lt;chr&gt; \"야당지\", \"여당지\", \"여당지\", \"여당지\", \"여… ## $ catSoc &lt;chr&gt; \"비사회면\", \"비사회면\", \"사회면\", \"비사회면… textProcessor()함수는 영문처리를 기본값으로 하고 있다. 영문은 두 글자 단어가 거의 없기 때문에 기본값이 세글자 이상만 분석에 투입하도록 기본값이 설정돼 있다. 국문은 두 글자 단어도 의미있는 단어가 많기 때문에, 단어의 길이를 두 글자 이상으로 설정한다. processed &lt;- combined_df %&gt;% textProcessor( documents = combined_df$text2, metadata = ., wordLengths = c(2, Inf) ) ## Building corpus... ## Converting to Lower Case... ## Removing punctuation... ## Removing stopwords... ## Removing numbers... ## Stemming... ## Creating Output... summary(processed) ## A text corpus with 14090 documents, and an 121938 word dictionary. Use str() to inspect object or see documentation prepDocuments()함수로 주제모형에 사용할 데이터의 인덱스(wordcounts)를 만든다. 이후 stm말뭉치와 기사 본문을 연결해 확인해야 하므로, 단어와 문서를 제거하지 않는다. out &lt;- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 0) summary(out) ## Length Class Mode ## documents 14090 -none- list ## vocab 121938 -none- character ## meta 10 data.frame list ## words.removed 0 -none- character ## docs.removed 0 -none- NULL ## tokens.removed 1 -none- numeric ## wordcounts 121938 -none- numeric 산출결과를 개별 객체로 저장한다. 이 객체들은 이후 모형구축에 사용된다. docs &lt;- out$documents vocab &lt;- out$vocab meta &lt;- out$meta 11.3 분석 주제모형 구축에 앞서 먼저 도출한 주제의 수를 정한다. 11.3.1 주제(topic)의 수 설정 보통 10개부터 100개까지 10개 단위로 주제의 수를 구분해 연구자들이 정성적으로 최종 주제의 수 판단한다. 학습 상황이므로 계산시간을 줄이기 위해 주제의 수를 3개, 9개, 100개의 결과를 비교해보자. 주제모형 분석은 사후 확률의 근사치를 주어진 자료로부터 반복적으로 최적화하는 계산을 수행하기 때문에 자료가 크면 계산시간이 오래 걸린다. 컴퓨터 성능에 따라 수십분 이상 소요될 수 있다. topicN &lt;- c(3, 9, 100) covariate_storage &lt;- searchK(docs, vocab, K = topicN) covariate_storage &lt;- storage covariate_storage %&gt;% write_rds(&quot;data/covariate_storage.rds&quot;) covariate_storage &lt;- read_rds(&quot;data/covariate_storage.rds&quot;) plot(covariate_storage) 11.3.2 모형 구성 stm패키지가 추출한 주제에 대하여 메타데이터를 변수로 투입하는 방식은 2가지다. ‘topical prevalence’와 ’topical content’를 이용하는 방식이다. ’topical prevalence’ 공변인은 prevalence =인자를 통해 투입하고, ’topical content’는 cotent =인자를 통해 투입한다. 모형에 따라 prevalence와 content 중 하나만 투입하기도 하고 둘다 투입하기도 한다. Topical prevalence: 공변인에 따른 문서별 주제 분포의 비율 Topical content: 공변인에 따른 단어별 주제 분포 연속변수를 투입할 때는 s()함수를 이용해 구간의 수를 지정한다. s()함수는 공변인을 연속변수로 투입할때 spline으로 추정하도록 한다. 즉, 구간을 지정해 각 구간별로 따로 회귀식을 구하면서 각 구간을 연속적인 형태로 만들어준다. 구간의 수는 df =인자를 통해 투입한다. 기본값은 10이다. 주제모형분석을 위해서는 주제어 선정과 분포계산을 반복적으로 수행한다. 이 과정을 화면에 출력되지 않게 verbose = 인자를 FALSE로 설정할 수 있다. 주의: 메타데이터 인자(prevalence =~ 와 content =~)에 투입할 때 =가 아니라 =~ !!! 아래 모형에서는 언론사의 정치성향과 시간을 공변인으로 투입했다. t1 &lt;- Sys.time() meta_fit &lt;- stm( documents = docs, vocab = vocab, data = meta, K = 9, prevalence =~ press + s(week, 6), # 투입하는 공변인 max.em.its = 75, # 최대 반복계산 회수 verbose = F, # 반복계산결과 화면출력 여부 init.type = &quot;Spectral&quot;, seed = 37 ) t2 &lt;- Sys.time() t2-t1 meta_fit %&gt;% write_rds(&quot;data/meta_fit.rds&quot;) meta_fit &lt;- read_rds(&quot;data/meta_fit.rds&quot;) summary(meta_fit) ## A topic model with 9 topics, 14090 documents and a 121938 word dictionary. ## Topic 1 Top Words: ## Highest Prob: 백신, 접종, 코로나, 아스트라제네카, 바이러스, 예방, 영국 ## FREX: 접종, 아스트라제네카, 임상, az, 면역, 화이자, 혈전 ## Lift: aah, abv, acip, acut, adapt, adcov, ade ## Score: 접종, 백신, 아스트라제네카, 접종자, az, 임상, 혈전 ## Topic 2 Top Words: ## Highest Prob: 코로나, 지원, 지급, 정부, 지원금, 재난, 매출 ## FREX: 추경안, 가액, 예비비, 지원대상, 부가세, 본예산, 하이트진로 ## Lift: bgf, blt, carrier, cj오쇼핑, covideigokr, db손해보험, e영업제한 ## Score: 지원금, 매출, 지급, 소상공인, 소득, 대출, 자영업자 ## Topic 3 Top Words: ## Highest Prob: 달러, 투자, 기업, 코로나, 시장, 미국, 경제 ## FREX: 주식, 반도체, 주가, 증시, 투자자, 코스피, 공매도 ## Lift: a기업, bev, carbon, cbi, ceo스코어, cpng, c쇼크 ## Score: 금리, 증시, 달러, 주식, 반도체, 코스피, 공매도 ## Topic 4 Top Words: ## Highest Prob: 코로나, 병원, 경찰, 환자, a씨, 의료, 법무부 ## FREX: 교도소, 서울동부구치소, 의료법, 재판부, 재소자, 서울구치소, 정인이 ## Lift: 교도소, 서울구치소, 총회장, abus, admit, aids감염인연합회, aids인권활동가네트워크 ## Score: 수용자, 구치소, 경찰, 병원, 법무부, 동부구치소, 혐의 ## Topic 5 Top Words: ## Highest Prob: 미국, 중국, 일본, 한국, 코로나, 북한, 정부 ## FREX: 올림픽, 도쿄, 미얀마, 스가, 쿼드, 군부, 도쿄올림픽 ## Lift: 관영, 도쿄, aaaj, aaip, aapi, aapp, aaron ## Score: 북한, 중국, 바이든, 외교, 트럼프, 이란, 미국 ## Topic 6 Top Words: ## Highest Prob: 온라인, 교육, 코로나, 서비스, 지원, 사업, 학교 ## FREX: 클럽하우스, 학년도, 학급, 메타버스, vr, 특강, 명예의 ## Lift: 학급, aadhaar, aapex, abet, abf, abf제도, abl생명보험 ## Score: 브랜드, 수업, 학생, 등교, 학년, 고객, 학교 ## Topic 7 Top Words: ## Highest Prob: 대통령, 국민, 의원, 후보, 대표, 정부, 민주당 ## FREX: 선거, 지지율, 출마, 탄핵, 사면, 보궐, 경선 ## Lift: sica, 고전역학, 공정사단, 기록관, 기초의원, 김해신공항, 난타전 ## Score: 대통령, 민주당, 후보, 선거, 대선, 출마, 의원 ## Topic 8 Top Words: ## Highest Prob: 확진자, 코로나, 감염, 검사, 방역, 확진, 발생 ## FREX: 진단검사, 비수도권, 입국자, 열방, btj, 선교회, 다중이용시설 ## Lift: abrimosomorimo, accept, agejspnurl, antibodi, anvisa, a관세법, a관세법인 ## Score: 확진자, 확진, 판정, 검사, 감염, 방대본, 집단감염 ## Topic 9 Top Words: ## Highest Prob: 코로나, 여성, 사람, 사회, 생각, 사람들, 서울 ## FREX: 출생아, 층간소음, 천사, 빙어, 칫솔, 장미, 빨대 ## Lift: a무리, bj파이, brt, cesd, cmip, connectus, contact ## Score: 플라스틱, 쓰레기, 결혼, 축제, 일자리, 가구, 여성 11.4 주제 이름짓기 11.4.1 주제별 단어와 원문 결합 주제 단어가 추출된 원문을 살펴보면 해당 주제를 보다 명확하게 파악할 수 있다. 모형 구성에 투입한 데이터와 문서(이 경우 개별 기사) 본문이 포함된 데이터를 결합해야 한다. stm패키지는 findThoughts()함수를 통해 각 모형별로 전형적인 문서를 확인할 수 있도록 한다. findThoughts( model = meta_fit, # 구성한 주제모형 texts = vac2_df$text, # 문서 본문 문자 벡터 topics = c(1, 2), # 찾고자 하는 주제의 값. 기본값은 모든 주제 n = 3 # 찾고자 하는 문서의 수 ) ## ## Topic 1: ## 770만명 맞을 아스트라, 유럽선 중단 EU(유럽연합) 4대 회원국인 독일 프랑스 이탈리아 스페인이 15일(현지 시각) 아스트라제네카(AZ) 코로나 백신의 접종을 일시 중단하겠다고 선언했다. 접종 후 혈전(血栓 핏덩이)이 생기는 부작용이 발생했다는 보고가 잇따르자 추가 조사 결과가 나올 때까지 접종을 멈추겠다는 것이다. 이날 옌스 슈판 독일 보건부 장관은 “부작용이 백신 접종의 효과를 .. ## 아스트라 백신 1차 검증서 \"예방 효과 62% 고령층 접종 가능\" 아스트라제네카 백신이 국내 접종을 앞두고 허가 심사 첫 관문을 통과했다. 식품의약품안전처 외부 자문단이 아스트라제네카 백신에 대해 조건부 허가할 수 있다는 결론을 내렸다. 해외에서 논란이 일고 있는 65세 이상 고령자 접종에 대해서도 자문단 다수가 “투여할 수 있다”고 의견을 냈다. 식약처 허가를 통과하면 아스트라제네카 백신은 이달 말께 공급돼 요양병원.. ## 65세 이상, 아스트라 백신 접종 연기 아스트라제네카 코로나 백신의 최우선 접종 대상에서 ‘65세 이상’ 고령층이 제외됐다. 코로나19 예방접종 대응 추진단(단장 정은경 질병관리청장)은 당초 2월 접종 예정이었던 요양병원 요양시설의 65세 이상 고령자 접종을 2분기(4~6월) 이후로 미룬다고 15일 밝혔다. 최근 유럽에서 ‘효과 검증이 부족하다’는 이유로 65세 이상에 대한 아스트라제네.. ## Topic 2: ## 매출 근로자 수 기준 재검토 실질 피해 따져 ‘사각’ 없앤다 기재부, 선별 방식 연구용역 발주 업종 간 형평성도 보완 ‘직격탄’ 업종 더 주고, 특고 프리랜서 제한 기준 개선할 듯 4차 재난지원금 ‘선별지원’을 주장해온 정부는 ‘선별 속 선별’을 강조하고 있다. 지급 대상을 더 정교하게 구분해서 피해에 걸맞은 지원액을 최대한 많이 주자는 것이다. 앞선 지급과정에서 문제로 드러난 매출 근로자 수 기준, 업종 구분.. ## 분배 악화일로 근로 사업소득 사상 첫 3분기째 동반 감소 코로나19 3차 유행이 시작된 지난해 4분기 상 하위 계층 간 소득 격차가 벌어지며 두 분기 연속 분배 상황이 악화했다. 코로나19로 인한 경기 부진과 고용 충격이 저소득층에 집중되면서 소득 불균형을 키웠다. 코로나19 사태 장기화로 일해서 버는 돈인 근로소득과 사업소득이 사상 처음으로 세 분기 연속 동반 감소했다. 18일 통계청이 발표한 ‘2020년.. ## 소상공인 자영업자 코로나 피해지원금 최대 500만원 검토 정부가 4차 재난지원금에 포함할 소상공인 자영업자 피해지원금을 매출 감소에 따라 차등 지원하는 방안을 추진 중이다. 1인당 지원금은 최대 500만원까지 지급하는 방안을 검토 중이다. 21일 더불어민주당과 정부 설명을 종합하면, 기획재정부는 다음달 초 국회에 제출할 예정인 추가경정예산(추경)안에 이런 내용을 담은 소상공인 피해지원대책을 논의 중이다. .. 구성한 stm모형과 구성전 데이터프레임을 결합하면 보다 다양한 방식으로 문서의 원문을 탐색할 수 있다. 결합하는 두 데이터프레임의 기준이 되는 열에 포함된 자료의 유형을 통일시킨다. 여기서는 정수(integer)로 통일시켰다. td_gamma &lt;- meta_fit %&gt;% tidy(matrix = &quot;gamma&quot;) td_gamma$document &lt;- as.integer(td_gamma$document) combined_df$ID &lt;- as.integer(combined_df$ID) 각 주제는 독립된 열로 분리한다. text_gamma &lt;- combined_df %&gt;% select(ID, text2, text, 키워드) %&gt;% left_join(td_gamma, by = c(&quot;ID&quot; = &quot;document&quot;)) %&gt;% pivot_wider( names_from = topic, values_from = gamma, names_prefix = &quot;tGamma&quot;, values_fill = 0 ) text_gamma %&gt;% glimpse() ## Rows: 14,090 ## Columns: 13 ## $ ID &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,… ## $ text2 &lt;chr&gt; \"국채 금리 상승 출발 뉴욕 증시 소폭 하락 … ## $ text &lt;chr&gt; \"美 국채 금리 상승에 뉴욕 증시 소폭 하락 … ## $ 키워드 &lt;chr&gt; \"국채,금리,상승,출발,뉴욕,증시,소폭,하락,… ## $ tGamma1 &lt;dbl&gt; 0.006021, 0.944370, 0.058890, 0.362034, 0.… ## $ tGamma2 &lt;dbl&gt; 0.011376, 0.001862, 0.009169, 0.063926, 0.… ## $ tGamma3 &lt;dbl&gt; 0.936021, 0.003908, 0.018304, 0.007786, 0.… ## $ tGamma4 &lt;dbl&gt; 0.000581, 0.004401, 0.003925, 0.004451, 0.… ## $ tGamma5 &lt;dbl&gt; 0.007778, 0.009563, 0.012495, 0.001949, 0.… ## $ tGamma6 &lt;dbl&gt; 0.003105, 0.002289, 0.369640, 0.516974, 0.… ## $ tGamma7 &lt;dbl&gt; 0.033440, 0.011748, 0.210739, 0.025828, 0.… ## $ tGamma8 &lt;dbl&gt; 0.000197, 0.007103, 0.003683, 0.012100, 0.… ## $ tGamma9 &lt;dbl&gt; 0.001481, 0.014757, 0.313156, 0.004952, 0.… 각 주제별로 확률분포가 높은 문서를 확인해 보자. 각 문서에서 감마가 높은 순서로 정열하면, 해당 주제에 속할 확률이 높은 문서 순서대로 볼수 있다. pull()함수를 이용하면 해당 열의 모든 내용을 볼수 있다. text_gamma %&gt;% arrange(-tGamma7) %&gt;% pull(text) %&gt;% head(9) ## [1] \"이낙연의 사면론 묘수일까 자충수일까 당대표 임기 두달 앞두고 전격 제기 하필이면 왜, 지금일까 이적수. 바둑용어다. 바둑에서 이적수는 둘이다. 이적수(利敵手)와 이적수(耳赤手). 한글발음은 같지만, 뜻은 정반대다. 이적수(利敵手)는 상대방에게 유리한 결과를 두는 수다. 자충수가 대표적이다. 이적수(耳赤手)는 상대방의 귀가 빨갛게 변하는 수다. 형세가 불리할 때 역전의 발판이 되..\" ## [2] \"2021 서울시장 선거, 단일화 전쟁이 시작됐다 오는 4월7일 서울시장과 부산시장을 다시 뽑는다. 내년 3월 대선을 앞둔 전초전이어서 여야 모두 물러설 수 없는 승부다. 대한민국 ‘민심의 풍향계’인 수도 서울에선 더더욱 그렇다. 여당은 수성을, 야당은 수복을 노리고 있다. 선거 한 달을 앞두고 주요 정당의 서울시장 후보가 모습을 드러내고 있지만 아직 대진표는 확정되지 않았다. 후보 단일화 과정이 남..\" ## [3] \"금태섭 \\\"1대1 경선하자\\\" 안철수 \\\"국민의힘 논의 먼저\\\" 금태섭 전 더불어민주당 의원이 31일 서울시장 보궐선거 출마선언을 하면서 안철수 국민의당 대표를 향해 “제3지대 경선을 하자”고 제안했다. 이날 조정훈 시대전환 대표도 보선 출사표를 던지면서 범야권 단일화 시계가 본격적으로 움직이기 시작했다. 이날 오전 서울 마포구의 한 공연장에서 출마선언을 한 금 전 의원은 “서울 시민의 삶을 바꾸고 변화의 ..\" ## [4] \"사면론 이틀만에 막히자...이낙연 \\\"오랜 충정에서 말한 것뿐\\\" 이낙연 더불어민주당 대표가 꺼낸 이명박 박근혜 전 대통령에 대한 사면(赦免) 구상이 당원과 지지층의 반발에 부딪히면서 제동이 걸렸다. 이 대표가 새해 첫날 언론 인터뷰에서 ‘사면론’을 꺼낸 지 이틀 만이다. 이 대표는 3일 오후 국회 의원회관 사무실에서 긴급 최고위원 간담회를 소집해 의견 수렴에 나섰다. 당 최고위원회는 회의 직후 입장문(최인호 ..\" ## [5] \"서울시장 선거 여권 재역전 가능할까 오세훈 강세 지속 선거전 1주일 유권자 선택 중요 “오해하면 안 되는 것이 LH 사태라는 돌발변수 때문에 뒤집힌 것이 아니라는 점이다.” 민주당 측 당 전략전문가의 말이다. “교육부 서기관 나향욱의 개돼지 발언을 보라. 그게 박근혜가 시켜 한 발언인가. 박근혜나 당시 새누리당과 아무 관련 없이 터져나온 것이다. 그렇지 않아도 불만이 쌓여..\" ## [6] \"文 교감 속 작품? 반전 노린 3위 이낙연의 '사면' 승부수 이낙연 더불어민주당 대표가 “이명박 박근혜 전 대통령 사면 건의” 뜻을 1일 언론 인터뷰에서 밝히면서 정국을 흔들었다. 평소 신중한 성격의 이 대표가 대통령 고유 권한인 사면 문제를 먼저 꺼낸 걸 두고 “사면에 대한 대통령의 정치적 부담을 줄이고, 한 편으론 대선 후보 지지율 3위로 주저앉은 자신을 위한 반전 계기를 만들려 했다”는 분석이 나온다. 이날..\" ## [7] \"\\\"대선 포기\\\" 안철수가 치고나간 서울시장 선거 여야 딜레마 4 7 재보선의 하이라이트로 꼽히는 서울시장 보궐선거전은 지난달 20일 출렁댔다. 주요 예상 후보들이 출마 선언을 미루는 가운데 안철수 국민의당 대표가 “대권 포기, 야권 단일화”를 외치며 치고 나갔다. 여야는 과거 중도층 지지를 받았던 안 대표의 등장에 촉각을 곤두세웠다. ━ 경선에 주목하는 與 민주당에선 우상호 의원이 처음 출마를..\" ## [8] \"금 “새정치만 10년째, 성과 뭔가?” 안 “정치개혁 초심 여전히 굳다” “10년 전 ‘새정치’라는 기치를 들고 나오셨다. 그런데 10년 동안 어떤 성과가 있었나?”(금태섭 후보) “금 후보나 저나 정치를 같은 시기에 시작했다. 정치를 개혁하겠단 초심, 의지는 여전히 굳고 똑같다.”(안철수 후보) 4월 서울시장 재보궐 선거에 출마한 안철수 국민의당 예비후보와 무소속 금태섭 예비후보가 18일 ‘문재인 정부의 4년 평가와..\" ## [9] \"범야권 ‘단일화 시계’ 빨라지나 금태섭, 안철수에 “1:1 경선하자” 금태섭 전 더불어민주당 의원이 31일 서울시장 보궐선거 출마선언을 하면서 안철수 국민의당 대표를 향해 “제3지대 경선을 하자”고 제안했다. 금 전 의원은 이날 오전 서울 마포구 공연장에서 출마선언을 하면서 “서울시민의 삶을 바꾸고 변화의 새판을 열어야 하는 선거지만 정치권은 오래된 싸움만 하고 있다”며 “엄중한 시기를 오래되고 낡은 정치에 맡길 ..\" text_gamma %&gt;% arrange(-tGamma7) %&gt;% pull(키워드) %&gt;% .[6] ## [1] \"교감,작품,반전,이낙연,사면,승부수,대표,이낙연,더불어민주당,이명박,박근혜,대통령,건의,인터뷰,언론,정국,평소,신중,성격,대표,권한,대통령,고유,사면,사면,대통령,정치,부담,대선,후보,지지율,자신,반전,계기,이날,민주당,사면,주장,모습,추락,승부수,관계자,민주당,핵심,중앙일보,메시지,대표,모습,통합,원래,이낙연,본모습,국가,좌고우면,설명,취임,문파,文派,강성,지지,비판,대표,목소리,신호,의미,이날,여론조사,각종,신년,여론,조사,대표,이재명,윤석열,지지,대선,주자,3위,대표,혼자,주장,여권,대통령,대표,사이,임기,마지막,화두,국민,통합,의기투합,4,모종,그림,발언,기획,대표,26일,차례,대통령,차례,독대,대표,당내,의견,청와대,국민,소통,수석,윤영찬,의원,참여,의원,중앙일보,통화,조언,대표,의견,교환,생각,임기,총대,멨나,대표,임기,마지막,대통령,구속,전직,대통령,정치,부담,총대,가능성,거론,친문,분류,의원,대통령,고민,대표,이슈,당직,의원,대표,신중,성격,대통령,영역,사람,혼자,독단,대통령,권한,이야기,임기,대표,지지,반발,각오,결단,추론,당내,부정,여론,김종민,최고,위원,통화,결정,국민,수용,대통령,사면권,국민,위임,권한,여야,국정농단,정치,상황,고민,극복,개선,방안,모색,논의,우상호,의원,페이스북,사람,반성,사과,박근혜,심판,사법,반대,공개적,가시화,내부,반발,가시,대표,명분,신년사,언급,사회,갈등,완화,국민,통합,비판,분출,의원,친문,재선,통합,이명박,박근혜,대통령,사면,국민,논의,당대표,대통령,압박,비판,상의,초선,최고,위원,본인,결단,전략통,의원,사면,수도,재선,당내,반발,확산,양상,게시판,민주당,권리당원,대통령,도전,생각,사퇴,촛불민심,뒤통수,비난,도배,대표,건의,해피엔딩,대통령,수용,여부,대통령,통합,이미지,대선후보,유력,대선,후보,청와대,반대,좌초,대표,오점,대표,모두발언,신년인,사회,발언,김대중,외환위기,노무현,안보,위기,문재인,코로나19,현직,대통령,위기,극복,거론,전진,통합,동시,통합,강조,주변,대표,윤석열,탄핵,배제,친문세력들,문자,폭탄,와중,반응,삼척동자,친문,초선,대표,비난,승부수,의미\" 각 주제별로 대표 단어를 선택해 원문을 살펴보자. text_gamma %&gt;% arrange(-tGamma2) %&gt;% filter(str_detect(text, &quot;지원금&quot;)) %&gt;% mutate(text = str_replace_all(text, &quot;지원금&quot;, &quot;**지원금**&quot;)) %&gt;% pull(text) %&gt;% head(5) ## [1] \"매출 근로자 수 기준 재검토 실질 피해 따져 ‘사각’ 없앤다 기재부, 선별 방식 연구용역 발주 업종 간 형평성도 보완 ‘직격탄’ 업종 더 주고, 특고 프리랜서 제한 기준 개선할 듯 4차 재난**지원금** ‘선별지원’을 주장해온 정부는 ‘선별 속 선별’을 강조하고 있다. 지급 대상을 더 정교하게 구분해서 피해에 걸맞은 지원액을 최대한 많이 주자는 것이다. 앞선 지급과정에서 문제로 드러난 매출 근로자 수 기준, 업종 구분..\" ## [2] \"소상공인 자영업자 코로나 피해**지원금** 최대 500만원 검토 정부가 4차 재난**지원금**에 포함할 소상공인 자영업자 피해**지원금**을 매출 감소에 따라 차등 지원하는 방안을 추진 중이다. 1인당 **지원금**은 최대 500만원까지 지급하는 방안을 검토 중이다. 21일 더불어민주당과 정부 설명을 종합하면, 기획재정부는 다음달 초 국회에 제출할 예정인 추가경정예산(추경)안에 이런 내용을 담은 소상공인 피해지원대책을 논의 중이다. ..\" ## [3] \"피해지원? 경기부양? “재난**지원금** 목표따라 규모 정해야” 지난해 전국민에게 지급한 14조3천억원 규모의 긴급재난**지원금**이 일으킨 소비 효과를 두고 학계가 뜨겁게 논쟁하고 있다. 소비 진작 효과가 투입 금액의 24%였다는 연구에서 최대 78%에 이른다는 분석까지 나온다. 연구 결과들은 공통으로 전체적인 소비 진작 효과뿐만 아니라 지원방식, 지급대상 등을 더욱 정교하게 설계해 **지원금** 지급 효율성을 높여야 한다고 강..\" ## [4] \"코로나 고용 한파에 소득 불평등 커졌다 ㆍ임시 일용직 많은 저소득층 타격 ㆍ하위 20% 근로소득 급감 적자살림 ㆍ4분기 ‘5분위 배율’ 4.72로 더 악화 ㆍ정부 지원 효과로 양극화 폭 줄여 코로나19에 따른 경제충격으로 지난해 4분기에 소득양극화가 심화된 것으로 나타났다. 정부가 2차 긴급재난**지원금**으로 취약계층을 집중 지원했지만 물리적(사회적) 거리 두기에 따른 일자리 쇼크 해소..\" ## [5] \"29일 소상공인, 30일 특고 프리랜서 4차 재난**지원금** 지급 29일부터 소상공인 특수형태근로종사자 등 코로나19 피해계층을 대상으로 한 4차 재난**지원금** 지급이 시작된다. 28일 기획재정부 등에 따르면 정부는 6조7천억원 규모의 소상공인 버팀목자금 플러스와 1조원 규모의 고용 취약계층 피해**지원금**을 29일부터 순차 지급한다. 소상공인 버팀목자금 플러스는 집합금지 제한업종 및 국세청 자료에서 매출 감소가 확인되는..\" 11.4.2 주제 이름 목록 각 주제별로 주요 주제어와 해당 문서의 본문을 비교해 주제별로 주요 문서를 살펴보고 주제에 대한 이름을 짓는다. 각 주제별 주요 단어는 labelTopics()함수를 통해 주요단어를 찾을 수 있다. 주제별 이름은 목록을 만들어 데이터프레임에 저장한다. labelTopics(meta_fit) ## Topic 1 Top Words: ## Highest Prob: 백신, 접종, 코로나, 아스트라제네카, 바이러스, 예방, 영국 ## FREX: 접종, 아스트라제네카, 임상, az, 면역, 화이자, 혈전 ## Lift: aah, abv, acip, acut, adapt, adcov, ade ## Score: 접종, 백신, 아스트라제네카, 접종자, az, 임상, 혈전 ## Topic 2 Top Words: ## Highest Prob: 코로나, 지원, 지급, 정부, 지원금, 재난, 매출 ## FREX: 추경안, 가액, 예비비, 지원대상, 부가세, 본예산, 하이트진로 ## Lift: bgf, blt, carrier, cj오쇼핑, covideigokr, db손해보험, e영업제한 ## Score: 지원금, 매출, 지급, 소상공인, 소득, 대출, 자영업자 ## Topic 3 Top Words: ## Highest Prob: 달러, 투자, 기업, 코로나, 시장, 미국, 경제 ## FREX: 주식, 반도체, 주가, 증시, 투자자, 코스피, 공매도 ## Lift: a기업, bev, carbon, cbi, ceo스코어, cpng, c쇼크 ## Score: 금리, 증시, 달러, 주식, 반도체, 코스피, 공매도 ## Topic 4 Top Words: ## Highest Prob: 코로나, 병원, 경찰, 환자, a씨, 의료, 법무부 ## FREX: 교도소, 서울동부구치소, 의료법, 재판부, 재소자, 서울구치소, 정인이 ## Lift: 교도소, 서울구치소, 총회장, abus, admit, aids감염인연합회, aids인권활동가네트워크 ## Score: 수용자, 구치소, 경찰, 병원, 법무부, 동부구치소, 혐의 ## Topic 5 Top Words: ## Highest Prob: 미국, 중국, 일본, 한국, 코로나, 북한, 정부 ## FREX: 올림픽, 도쿄, 미얀마, 스가, 쿼드, 군부, 도쿄올림픽 ## Lift: 관영, 도쿄, aaaj, aaip, aapi, aapp, aaron ## Score: 북한, 중국, 바이든, 외교, 트럼프, 이란, 미국 ## Topic 6 Top Words: ## Highest Prob: 온라인, 교육, 코로나, 서비스, 지원, 사업, 학교 ## FREX: 클럽하우스, 학년도, 학급, 메타버스, vr, 특강, 명예의 ## Lift: 학급, aadhaar, aapex, abet, abf, abf제도, abl생명보험 ## Score: 브랜드, 수업, 학생, 등교, 학년, 고객, 학교 ## Topic 7 Top Words: ## Highest Prob: 대통령, 국민, 의원, 후보, 대표, 정부, 민주당 ## FREX: 선거, 지지율, 출마, 탄핵, 사면, 보궐, 경선 ## Lift: sica, 고전역학, 공정사단, 기록관, 기초의원, 김해신공항, 난타전 ## Score: 대통령, 민주당, 후보, 선거, 대선, 출마, 의원 ## Topic 8 Top Words: ## Highest Prob: 확진자, 코로나, 감염, 검사, 방역, 확진, 발생 ## FREX: 진단검사, 비수도권, 입국자, 열방, btj, 선교회, 다중이용시설 ## Lift: abrimosomorimo, accept, agejspnurl, antibodi, anvisa, a관세법, a관세법인 ## Score: 확진자, 확진, 판정, 검사, 감염, 방대본, 집단감염 ## Topic 9 Top Words: ## Highest Prob: 코로나, 여성, 사람, 사회, 생각, 사람들, 서울 ## FREX: 출생아, 층간소음, 천사, 빙어, 칫솔, 장미, 빨대 ## Lift: a무리, bj파이, brt, cesd, cmip, connectus, contact ## Score: 플라스틱, 쓰레기, 결혼, 축제, 일자리, 가구, 여성 주제별 이름 목록을 데이터프레임에 저장한다. topic_name &lt;- tibble(topic = 1:9, name = c(&quot;1. 백신 접종자&quot;, &quot;2. 코로나 지원금&quot;, &quot;3. 경제 영향&quot;, &quot;4. 집단 수용자&quot;, &quot;5. 국제 관계&quot;, &quot;6. 교육 온라인&quot;, &quot;7. 정치권 동향&quot;, &quot;8. 확진자 검진&quot;, &quot;9. 사회 영향&quot;) ) 주제별 상위 7개 단어목록을 데이터프레임에 저장한 다음, 이름 목록과 결합한다. td_beta &lt;- meta_fit %&gt;% tidy(matrix = &#39;beta&#39;) term_topic_name &lt;- td_beta %&gt;% group_by(topic) %&gt;% slice_max(beta, n = 7) %&gt;% left_join(topic_name, by = &quot;topic&quot;) term_topic_name ## # A tibble: 63 × 4 ## # Groups: topic [9] ## topic term beta name ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 백신 0.0842 1. 백신 접종자 ## 2 1 접종 0.0612 1. 백신 접종자 ## 3 1 코로나 0.0249 1. 백신 접종자 ## 4 1 아스트라제네카 0.0115 1. 백신 접종자 ## 5 1 바이러스 0.00924 1. 백신 접종자 ## 6 1 예방 0.00745 1. 백신 접종자 ## # … with 57 more rows 11.4.3 주제별 단어 분포도 단어별로 부여된 베타 값을 이용해 주제별 단어 분포도를 각 주제의 이름과 함께 시각화한다. term_topic_name %&gt;% ggplot(aes(x = beta, y = reorder_within(term, beta, name), # 각 주제별로 재정렬 fill = name)) + geom_col(show.legend = F) + facet_wrap(~name, scales = &quot;free&quot;) + scale_y_reordered() + # 재정렬한 y축의 값 설정 labs(x = expression(&quot;단어 확률분포: &quot;~beta), y = NULL, title = &quot;주제별 단어 확률 분포&quot;, subtitle = &quot;주제별로 다른 단어들로 군집&quot;) + theme(plot.title = element_text(size = 20)) 11.4.4 주제별 문서 분포도 문서별로 부여된 감마 값을 이용한 주제별로 문서의 분포도를 각 주제의 이름과 함께 시각화한다. td_gamma &lt;- meta_fit %&gt;% tidy(matrix = &#39;gamma&#39;) doc_topic_name &lt;- td_gamma %&gt;% group_by(topic) %&gt;% left_join(topic_name, by = &quot;topic&quot;) doc_topic_name ## # A tibble: 126,810 × 4 ## # Groups: topic [9] ## document topic gamma name ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 1 0.00602 1. 백신 접종자 ## 2 2 1 0.944 1. 백신 접종자 ## 3 3 1 0.0589 1. 백신 접종자 ## 4 4 1 0.362 1. 백신 접종자 ## 5 5 1 0.000923 1. 백신 접종자 ## 6 6 1 0.00141 1. 백신 접종자 ## # … with 126,804 more rows doc_topic_name %&gt;% ggplot(aes(x = gamma, fill = name)) + geom_histogram(bins = 50, show.legend = F) + facet_wrap(~name) + labs(title = &quot;주제별 문서 확률 분포&quot;, y = &quot;문서(기사)의 수&quot;, x = expression(&quot;문서 확률분포&quot;~(gamma))) + theme(plot.title = element_text(size = 20)) 11.4.5 주제별 단어-문서 분포 # 주제별 상위 7개 단어 추출 top_terms &lt;- td_beta %&gt;% group_by(topic) %&gt;% slice_max(beta, n = 7) %&gt;% select(topic, term) %&gt;% summarise(terms = str_flatten(term, collapse = &quot;, &quot;)) # 주제별 감마 평균 계산 gamma_terms &lt;- td_gamma %&gt;% group_by(topic) %&gt;% summarise(gamma = mean(gamma)) %&gt;% left_join(top_terms, by = &#39;topic&#39;) %&gt;% # 주제별 단어 데이터프레임과 결합 left_join(topic_name, by = &#39;topic&#39;) # 주제 이름 데이터프레임과 결합 gamma_terms ## # A tibble: 9 × 4 ## topic gamma terms name ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 0.152 백신, 접종, 코로나, 아스트라제네카, 바… 1. … ## 2 2 0.106 코로나, 지원, 지급, 정부, 지원금, 재난… 2. … ## 3 3 0.100 달러, 투자, 기업, 코로나, 시장, 미국, … 3. … ## 4 4 0.0838 코로나, 병원, 경찰, 환자, a씨, 의료, … 4. … ## 5 5 0.0996 미국, 중국, 일본, 한국, 코로나, 북한, … 5. … ## 6 6 0.111 온라인, 교육, 코로나, 서비스, 지원, 사… 6. … ## # … with 3 more rows 결합한 데이터프레임을 막대도표로 시각화. gamma_terms %&gt;% ggplot(aes(x = gamma, y = reorder(name, gamma), fill = name)) + geom_col(width = 0.7, show.legend = F) + geom_text(aes(label = round(gamma, 2)), # 소수점 2자리 hjust = 1.15) + # 라벨을 막대도표 안쪽으로 이동 geom_text(aes(label = terms), hjust = -0.05) + # 단어를 막대도표 바깥으로 이동 scale_x_continuous(expand = c(0, 0), # x축 막대 위치를 Y축쪽으로 조정 limit = c(0, .8)) + # x축 범위 설정 labs(x = expression(&quot;문서 확률분포&quot;~(gamma)), y = NULL, title = &quot;코로나19와 백신 관련보도 상위 주제어&quot;, subtitle = &quot;주제별로 기여도가 높은 단어 중심&quot;) + theme(plot.title = element_text(size = 20)) + theme_light() 11.5 공변인 분석 stm패키지는 메타데이터와 주제 사이의 관계 탐색을 위해 estimateEffect()함수를 제공한다. 각 주제를 종속변수(산출요소)로 설정하고, 메타데이터를 독립변수(투입요소)로 설정해 회귀분석으로 수행한다. 분석결과는 회귀계수(estimate), 표준오차, t값으로 요약해 제시한다. 여기서 사용한 자료에서 메타데이터로 투입한 독립변수는 press(언론사의 정치성향)와 시간(week)이다. 즉, 언론사 정치성향과 시간은 독립변수로서 종속변수인 추촐한 주제를 예측하는 변인이 된다. 회귀계수는 독립변수가 종속변수를 설명하는 정도다. 예를 들어, press(언론사 정치성향)가 각 주제를 예측하는 정도가 회귀계수이므로, 이 회귀계수가 야당지에 비해 여당지에서 해당 주제에 대해 평균적으로 더 많이 언급한 정도를 나타낸다. 회귀계수가 음수면 야당지에서 언급이 더 많은 주제이고, 양수면 여당지에서 언급이 더 많은 주제다. t값과 p값은 그 효과(회귀계수)가 0과 유의하게 다른지를 나타낸다. 분석 결과는 plot.estimateEffect()함수로 시각화한다. 주제 분포의 불확실성은 uncertainty =인자를 통해 다양한 방법으로 계산하는데, 기본값은 모든 경우를 고려하는 ’Global’이다. 만일 계산속도를 높이고 싶으면 추가적인 불확실성 계산을 생략하는 ’None’으로 투입한다. 언론사의 정치성향(press)과 보도시점을 주 단위를 6개로 구분해 독립변수로 투입하고, 추출한 9개 주제를 종속변수로 투입해 분석해보자. out$meta$rating &lt;- as.factor(out$meta$press) prep &lt;- estimateEffect(formula = 1:9 ~ press + s(week, 6), stmobj = meta_fit, metadata = out$meta, uncertainty = &quot;Global&quot;) summary(prep, topics= 1:9) ## ## Call: ## estimateEffect(formula = 1:9 ~ press + s(week, 6), stmobj = meta_fit, ## metadata = out$meta, uncertainty = \"Global\") ## ## ## Topic 1: ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.13292 0.00836 15.90 &lt; 2e-16 *** ## press여당지 -0.06927 0.00518 -13.36 &lt; 2e-16 *** ## s(week, 6)1 -0.00787 0.01797 -0.44 0.6615 ## s(week, 6)2 0.01004 0.01744 0.58 0.5648 ## s(week, 6)3 0.02105 0.01823 1.15 0.2483 ## s(week, 6)4 0.13472 0.01943 6.93 4.2e-12 *** ## s(week, 6)5 0.08363 0.02002 4.18 3.0e-05 *** ## s(week, 6)6 0.04645 0.01492 3.11 0.0019 ** ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## ## Topic 2: ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.075411 0.006971 10.82 &lt;2e-16 *** ## press여당지 0.033215 0.004549 7.30 3e-13 *** ## s(week, 6)1 0.026761 0.015302 1.75 0.0803 . ## s(week, 6)2 0.041134 0.014805 2.78 0.0055 ** ## s(week, 6)3 0.027896 0.015051 1.85 0.0638 . ## s(week, 6)4 0.000922 0.016436 0.06 0.9553 ## s(week, 6)5 -0.008214 0.017042 -0.48 0.6298 ## s(week, 6)6 0.016163 0.011890 1.36 0.1741 ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## ## Topic 3: ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.12245 0.00709 17.27 &lt;2e-16 *** ## press여당지 -0.01048 0.00471 -2.22 0.026 * ## s(week, 6)1 -0.01928 0.01529 -1.26 0.207 ## s(week, 6)2 -0.01191 0.01551 -0.77 0.442 ## s(week, 6)3 -0.03370 0.01586 -2.13 0.034 * ## s(week, 6)4 -0.02218 0.01644 -1.35 0.177 ## s(week, 6)5 0.00103 0.01695 0.06 0.951 ## s(week, 6)6 -0.00978 0.01217 -0.80 0.422 ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## ## Topic 4: ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.11652 0.00659 17.67 &lt; 2e-16 *** ## press여당지 0.00411 0.00388 1.06 0.289 ## s(week, 6)1 0.02654 0.01381 1.92 0.055 . ## s(week, 6)2 -0.07887 0.01353 -5.83 5.7e-09 *** ## s(week, 6)3 -0.01425 0.01263 -1.13 0.259 ## s(week, 6)4 -0.03679 0.01461 -2.52 0.012 * ## s(week, 6)5 -0.05692 0.01367 -4.16 3.2e-05 *** ## s(week, 6)6 -0.04658 0.01139 -4.09 4.3e-05 *** ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## ## Topic 5: ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.09544 0.00674 14.16 &lt; 2e-16 *** ## press여당지 -0.01034 0.00449 -2.30 0.021 * ## s(week, 6)1 -0.00683 0.01457 -0.47 0.639 ## s(week, 6)2 0.03850 0.01517 2.54 0.011 * ## s(week, 6)3 -0.00535 0.01422 -0.38 0.707 ## s(week, 6)4 -0.03021 0.01592 -1.90 0.058 . ## s(week, 6)5 0.07686 0.01555 4.94 7.8e-07 *** ## s(week, 6)6 -0.01430 0.01203 -1.19 0.235 ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## ## Topic 6: ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.087814 0.007130 12.32 &lt; 2e-16 *** ## press여당지 -0.011102 0.004793 -2.32 0.0206 * ## s(week, 6)1 0.000218 0.015118 0.01 0.9885 ## s(week, 6)2 0.042055 0.015102 2.78 0.0054 ** ## s(week, 6)3 0.005794 0.015508 0.37 0.7087 ## s(week, 6)4 0.051091 0.016810 3.04 0.0024 ** ## s(week, 6)5 0.020497 0.018240 1.12 0.2611 ## s(week, 6)6 0.069777 0.012326 5.66 1.5e-08 *** ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## ## Topic 7: ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.12688 0.00791 16.04 &lt; 2e-16 *** ## press여당지 0.00781 0.00439 1.78 0.075 . ## s(week, 6)1 -0.01200 0.01661 -0.72 0.470 ## s(week, 6)2 0.02591 0.01587 1.63 0.102 ## s(week, 6)3 -0.06810 0.01597 -4.27 2.0e-05 *** ## s(week, 6)4 0.01741 0.01689 1.03 0.303 ## s(week, 6)5 -0.10097 0.01746 -5.78 7.4e-09 *** ## s(week, 6)6 -0.02197 0.01282 -1.71 0.087 . ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## ## Topic 8: ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.17429 0.00849 20.53 &lt; 2e-16 *** ## press여당지 0.02968 0.00521 5.69 1.3e-08 *** ## s(week, 6)1 -0.01232 0.01928 -0.64 0.52275 ## s(week, 6)2 -0.05192 0.01936 -2.68 0.00734 ** ## s(week, 6)3 0.03165 0.01875 1.69 0.09145 . ## s(week, 6)4 -0.09281 0.02112 -4.39 1.1e-05 *** ## s(week, 6)5 -0.05503 0.01992 -2.76 0.00574 ** ## s(week, 6)6 -0.05245 0.01520 -3.45 0.00056 *** ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## ## Topic 9: ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.06851 0.00589 11.63 &lt; 2e-16 *** ## press여당지 0.02653 0.00375 7.07 1.6e-12 *** ## s(week, 6)1 0.00463 0.01325 0.35 0.7269 ## s(week, 6)2 -0.01549 0.01295 -1.20 0.2317 ## s(week, 6)3 0.03523 0.01274 2.76 0.0057 ** ## s(week, 6)4 -0.02312 0.01444 -1.60 0.1094 ## s(week, 6)5 0.03956 0.01470 2.69 0.0071 ** ## s(week, 6)6 0.01209 0.01101 1.10 0.2721 ## --- ## Signif. codes: ## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 주제4와 주제7을 제외하고 모두 언론사의 정치성향에 따라 기사의 주제가 다른 경향을 보이고 있다. 주제3과 주제5를 제외하고 모두 3월에 기사의 주제가 바뀌었다. 언론사의 정치성향을 ’여당지’와 ’야당지’로 구분했는데, 분석결과에 ’여당지’만 나오는 이유는 야당지를 기준으로 여당지의 분포를 계산했기 때문이다. 즉, 계수가 음수면 야당지를 기준으로 여당지에 해당 주제를 구성하는 단어의 빈도가 상대적으로 적게 나타났다는 의미이다. 반대로 계수가 양수면 여당지에 해당 주제를 구성하는 단어의 빈도가 더 크다는 의미다. 11.5.1 문서 내용 확인 주제1은 ’백신접종’에 관련된 내용이다. 언론사 별로 각 주제에 대해 전형적인 기사가 무엇인지 확인해보자. 원문과 언론사 정보가 포함된 데이터프레임과 감마 계수 데이터프레임을 결합해 전형적인 기사를 찾을 수 있다. combined_df %&gt;% names() ## [1] \"ID\" \"text2\" \"일자\" \"text\" \"언론사\" \"cat\" ## [7] \"키워드\" \"week\" \"press\" \"catSoc\" combined_df %&gt;% left_join(td_gamma, by = c(&quot;ID&quot; = &quot;document&quot;)) %&gt;% pivot_wider( names_from = topic, values_from = gamma, names_prefix = &quot;tGamma&quot;, values_fill = 0 ) %&gt;% arrange(-tGamma1) %&gt;% filter(str_detect(text, &quot;백신&quot;)) %&gt;% mutate(text = str_replace_all(text, &quot;백신&quot;, &quot;**백신**&quot;)) %&gt;% head(30) ## # A tibble: 30 × 19 ## ID text2 일자 text 언론사 cat 키워드 week press ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1847 770만 … 2021… \"770… 조선… 국제 770만… 11 야당… ## 2 8183 62 아스… 2021… \"아… 중앙… 사회 62%,… 5 야당… ## 3 6092 65세 아… 2021… \"65… 조선… 사회 65세,… 7 야당… ## 4 3822 유럽 접… 2021… \"유… 중앙… 사회 유럽,… 9 야당… ## 5 7362 az 고령… 2021… \"AZ … 중앙… 사회 AZ,고… 6 야당… ## 6 1852 백신 혈… 2021… \"**… 조선… 국제 백신,… 11 야당… ## # … with 24 more rows, and 10 more variables: catSoc &lt;chr&gt;, ## # tGamma1 &lt;dbl&gt;, tGamma2 &lt;dbl&gt;, tGamma3 &lt;dbl&gt;, ## # tGamma4 &lt;dbl&gt;, tGamma5 &lt;dbl&gt;, tGamma6 &lt;dbl&gt;, ## # tGamma7 &lt;dbl&gt;, tGamma8 &lt;dbl&gt;, tGamma9 &lt;dbl&gt; ’백신 접종’을 주제로 보도한 기사에서 주제1의 감마 계수 상위 10대 기사 중 9건이 야당지 보도다. 기사의 제목과 본문을 살펴보자. 4번째가 여당지 기사이고, 나머지는 모두 야당지 기사다. combined_df %&gt;% names() ## [1] \"ID\" \"text2\" \"일자\" \"text\" \"언론사\" \"cat\" ## [7] \"키워드\" \"week\" \"press\" \"catSoc\" combined_df %&gt;% left_join(td_gamma, by = c(&quot;ID&quot; = &quot;document&quot;)) %&gt;% pivot_wider( names_from = topic, values_from = gamma, names_prefix = &quot;tGamma&quot;, values_fill = 0 ) %&gt;% arrange(-tGamma1) %&gt;% filter(str_detect(text, &quot;백신&quot;)) %&gt;% mutate(text = str_replace_all(text, &quot;백신&quot;, &quot;**백신**&quot;)) %&gt;% pull(text) %&gt;% .[1:10] ## [1] \"770만명 맞을 아스트라, 유럽선 중단 EU(유럽연합) 4대 회원국인 독일 프랑스 이탈리아 스페인이 15일(현지 시각) 아스트라제네카(AZ) 코로나 **백신**의 접종을 일시 중단하겠다고 선언했다. 접종 후 혈전(血栓 핏덩이)이 생기는 부작용이 발생했다는 보고가 잇따르자 추가 조사 결과가 나올 때까지 접종을 멈추겠다는 것이다. 이날 옌스 슈판 독일 보건부 장관은 “부작용이 **백신** 접종의 효과를 ..\" ## [2] \"아스트라 **백신** 1차 검증서 \\\"예방 효과 62% 고령층 접종 가능\\\" 아스트라제네카 **백신**이 국내 접종을 앞두고 허가 심사 첫 관문을 통과했다. 식품의약품안전처 외부 자문단이 아스트라제네카 **백신**에 대해 조건부 허가할 수 있다는 결론을 내렸다. 해외에서 논란이 일고 있는 65세 이상 고령자 접종에 대해서도 자문단 다수가 “투여할 수 있다”고 의견을 냈다. 식약처 허가를 통과하면 아스트라제네카 **백신**은 이달 말께 공급돼 요양병원..\" ## [3] \"65세 이상, 아스트라 **백신** 접종 연기 아스트라제네카 코로나 **백신**의 최우선 접종 대상에서 ‘65세 이상’ 고령층이 제외됐다. 코로나19 예방접종 대응 추진단(단장 정은경 질병관리청장)은 당초 2월 접종 예정이었던 요양병원 요양시설의 65세 이상 고령자 접종을 2분기(4~6월) 이후로 미룬다고 15일 밝혔다. 최근 유럽에서 ‘효과 검증이 부족하다’는 이유로 65세 이상에 대한 아스트라제네..\" ## [4] \"유럽도 AZ 고령층 접종한다는데 국내선 결론나도 2분기에나 아스트라제네카(AZ) **백신**의 고령층 접종을 제한했던 유럽 국가 일부가 접종을 다시 허용하는 쪽으로 입장을 바꾸는 분위기다. 국내에서도 고령층 접종을 재검토해야 한다는 목소리가 나오고 있다. 2일 영국 BBC 등 외신에 따르면 프랑스 정부는 1일(현지시간) 아스트라제네카 **백신**의 접종 권고 연령을 기존 65세 미만에서 74세까지로 확대했다. 올리비에..\" ## [5] \"AZ 고령층 접종 신중하게...질병청 예방접종위로 공 넘겨 아스트라제네카사(社)의 **백신** 허가를 위한 식품의약품안전처의 중앙약사심의위원회 회의에서 ‘18세 이상에게 접종할 수 있다’는 조건부 허가 권고가 나왔다. 현재 진행 중인 3상 임상결과를 제출하는 조건이다. 다만 중앙약심은 핵심 쟁점인 ‘65세 이상 고령자’에 대해서는 애매한 판단을 내렸다. 그간 나온 임상시험 자료가 충분하지 않다는 이유에서다. 허용은 하..\" ## [6] \"**백신**이 혈전 원인? 아스트라에 대한 3가지 의문 유럽 국가들이 줄줄이 아스트라제네카(AZ) **백신** 접종을 잠정 중단하면서 국내서도 불안감이 커지고 있다. 전문가들은 대부분 “현재까지 AZ **백신**과 혈전(핏덩이) 등과의 인과성은 확인된 바 없다”며 “고령층과 기저 질환자도 코로나 감염 위험을 감안하면 접종을 하는 게 더 이득”이란 입장이다. 하지만 정부가 국민 불안에 지금처럼 소극적 수동적으로 대처하면 백..\" ## [7] \"아스트라 효과 62% 자문단 다수 “고령자 배제 이유 없다” 식품의약품안전처의 전문가 자문기구가 아스트라제네카의 코로나19 **백신**을 고령층에게 접종해도 된다는 판단을 내렸다. 해당 **백신**은 유럽에서 고령층 접종 효과를 둘러싼 논란이 불거지면서 일부 국가는 고령층을 제외하고 접종하도록 권고했다. 자문기구는 현재 진행 중인 임상시험 결과를 제출하는 것을 전제로 아스트라제네카 **백신**에 대해 ‘조건부 허가’를 권고했다. ..\" ## [8] \"화이자 **백신**, 국내 두 번째 허가 \\\"16세 이상 접종 가능\\\" 화이자 **백신**이 아스트라제네카에 이어 국내에서 두 번째 코로나19 **백신**으로 정식 허가됐다. 허가 접종 연령이 만 16세 이상으로 결론 난 데 따라 원칙적으로는 미성년자인 고등학생도 **백신**을 접종할 수 있게 됐다. 식품의약품안전처는 5일 최종점검위원회를 열어 한국화이자제약의 코로나19 **백신**인 ‘코미나티주’를 허가했다고 밝혔다. 식약처는 “앞서 실시된 ..\" ## [9] \"AZ **백신**, 고령층 중증 위험 84% 낮춰...이상반응 0.4%는? 26일 아스트라제네카(AZ) **백신** 접종이 시작된 가운데 접종 후 나타날 수 있는 '이상 반응'에 관심이 높다. 우리나라보다 먼저 AZ **백신**을 맞은 영국에서는 접종 인구 가운데 약 0.4%가 이상 반응을 신고한 것으로 나타났다. 질별관리청 중앙방역대책본부(방대본)는 지난 24일 열린 코로나19 예방접종 특집브리핑에서 “\\\"각 국가에서 보고되는 코..\" ## [10] \"대상자 54만명 줄고, 접종시기 밀리고 스텝 꼬인 **백신**플랜 방역 당국이 15일 발표한 ‘1분기(2~3월) **백신** 접종 계획’은 요양병원에 입원한 65세 환자 등의 접종을 2월 말에서 4월 이후로 미루는 내용이 골자다. 각국이 아스트라제네카 **백신**의 ‘고령자 예방 효과’에 대한 근거 부족을 이유로 잇따라 ‘고령자 접종 제한’ 권고를 내렸던 게 고려됐다. 또 국내 1분기 도입 물량 100만명분 중 94만명분이 이 **백신**..\" 11.5.2 공변인 분석 시각화 공변인이 주제를 어떻게 예측하는지에 대해 도표로 시각화할 수 있다. 간단한 방법은 stm패키지에서 제공하는 plot.estimate()함수를 이용하는 방식이다. ggplot2패키지를 이용하면 보다 다양한 방식으로 독립변수와 종속변수의 관계를 시각화할 수 있다. 11.5.2.1 정치성향에 따른 주제분포 plot.estimateEffect( prep, covariate = &quot;press&quot;, topics = c(1, 2, 4), model = meta_fit, method = &quot;difference&quot;, cov.value1 = &quot;여당지&quot;, cov.value2 = &quot;야당지&quot;, xlab = &quot;문서당 주제 분포 비율(야당지 대 여당지)&quot;, main = &quot;언론사 정치성향에 따른 문서별 주제 분포&quot;, xlim = c(-.1, .1), labeltype = &quot;custom&quot;, custom.labels = c(&quot;주제1&quot;, &quot;주제2&quot;, &quot;주제4&quot;) ) ggplot2패키지를 이용하면 모다 다양한 방식으로 시각화할 수 있다. 앞서 명명한 주제의 이름이 막대도표에 표시되도록 먼저 데이터프레임을 결합한다. # 주제 이름 topic_name ## # A tibble: 9 × 2 ## topic name ## &lt;int&gt; &lt;chr&gt; ## 1 1 1. 백신 접종자 ## 2 2 2. 코로나 지원금 ## 3 3 3. 경제 영향 ## 4 4 4. 집단 수용자 ## 5 5 5. 국제 관계 ## 6 6 6. 교육 온라인 ## # … with 3 more rows # 공변인 계수 coef_df &lt;- prep %&gt;% tidy() %&gt;% filter(term == &quot;press여당지&quot;) coef_df ## # A tibble: 9 × 6 ## topic term estimate std.error statistic p.value ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 press여당지 -0.0693 0.00521 -13.3 4.30e-40 ## 2 2 press여당지 0.0332 0.00454 7.31 2.90e-13 ## 3 3 press여당지 -0.0105 0.00467 -2.25 2.47e- 2 ## 4 4 press여당지 0.00413 0.00393 1.05 2.93e- 1 ## 5 5 press여당지 -0.0103 0.00448 -2.31 2.08e- 2 ## 6 6 press여당지 -0.0111 0.00481 -2.31 2.10e- 2 ## # … with 3 more rows # 주제별 상위 10개 단어 추출 top_terms &lt;- meta_fit %&gt;% tidy(matrix = &quot;beta&quot;) %&gt;% group_by(topic) %&gt;% slice_max(beta, n = 7) %&gt;% select(topic, term) %&gt;% summarise(terms = str_flatten(term, &quot; &quot;)) top_terms ## # A tibble: 9 × 2 ## topic terms ## &lt;int&gt; &lt;chr&gt; ## 1 1 백신 접종 코로나 아스트라제네카 바이러스 예방 영국 ## 2 2 코로나 지원 지급 정부 지원금 재난 매출 ## 3 3 달러 투자 기업 코로나 시장 미국 경제 ## 4 4 코로나 병원 경찰 환자 a씨 의료 법무부 ## 5 5 미국 중국 일본 한국 코로나 북한 정부 ## 6 6 온라인 교육 코로나 서비스 지원 사업 학교 ## # … with 3 more rows # 데이터프레임 결합 term_coef_name &lt;- top_terms %&gt;% left_join(topic_name, by = &quot;topic&quot;) %&gt;% left_join(coef_df, by = &quot;topic&quot;) term_coef_name %&gt;% glimpse() ## Rows: 9 ## Columns: 8 ## $ topic &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9 ## $ terms &lt;chr&gt; \"백신 접종 코로나 아스트라제네카 바이러… ## $ name &lt;chr&gt; \"1. 백신 접종자\", \"2. 코로나 지원금\", \"3… ## $ term &lt;chr&gt; \"press여당지\", \"press여당지\", \"press여당… ## $ estimate &lt;dbl&gt; -0.06931, 0.03317, -0.01049, 0.00413, -0… ## $ std.error &lt;dbl&gt; 0.00521, 0.00454, 0.00467, 0.00393, 0.00… ## $ statistic &lt;dbl&gt; -13.30, 7.31, -2.25, 1.05, -2.31, -2.31,… ## $ p.value &lt;dbl&gt; 4.30e-40, 2.90e-13, 2.47e-02, 2.93e-01, … 데이터셋이 마련됐으면 막대도표에 시각화한다. term_coef_name %&gt;% ggplot(aes(x = estimate, y = reorder(name, estimate), fill = name)) + geom_col(show.legend = F, width = 0.7) + geom_errorbar(aes(xmin = estimate - std.error, xmax = estimate + std.error), width = .9, size = .4, color = &quot;grey10&quot;, show.legend = F) + scale_x_continuous(expand = c(0, 0), limits = c(-.75, .15), breaks = 0) + geom_text(aes(x =-.4, label = terms), show.legend = F) + geom_text(aes(label = round(estimate, 3)), hjust = -.2) + labs(x = &quot;문서당 주제 분포 비율(야당지 대 여당지)&quot;, y = NULL, title = &quot;언론사 정치성향에 따른 문서별 주제 분포&quot;) + theme(plot.title = element_text(size = 20)) + theme_light() 11.5.2.2 시간대별 주제 변화 plot.estimateEffect( prep, covariate = &quot;week&quot;, topics = c(1, 8), model = meta_fit, method = &quot;continuous&quot;, # 시간대 연속적으로 표시 xlab = &quot;기간 (1월 ~ 3월)&quot;, main = &quot;시간대별 주제 분포&quot; ) ggplot2패키지로 시각화하기 위해 먼저 데이터프레임을 결합한다. # 주제 이름 topic_name ## # A tibble: 9 × 2 ## topic name ## &lt;int&gt; &lt;chr&gt; ## 1 1 1. 백신 접종자 ## 2 2 2. 코로나 지원금 ## 3 3 3. 경제 영향 ## 4 4 4. 집단 수용자 ## 5 5 5. 국제 관계 ## 6 6 6. 교육 온라인 ## # … with 3 more rows # 공변인 계수 coef_time &lt;- prep %&gt;% tidy() %&gt;% filter(str_detect(term, &quot;^s&quot;)) coef_time ## # A tibble: 54 × 6 ## topic term estimate std.error statistic p.value ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 s(week, 6)1 -0.00808 0.0180 -0.449 6.54e- 1 ## 2 1 s(week, 6)2 0.00992 0.0174 0.571 5.68e- 1 ## 3 1 s(week, 6)3 0.0206 0.0181 1.14 2.54e- 1 ## 4 1 s(week, 6)4 0.135 0.0194 6.96 3.54e-12 ## 5 1 s(week, 6)5 0.0830 0.0201 4.12 3.74e- 5 ## 6 1 s(week, 6)6 0.0462 0.0147 3.16 1.60e- 3 ## # … with 48 more rows # 데이터프레임 결합 term_coef_time &lt;- coef_time %&gt;% left_join(topic_name, by = &quot;topic&quot;) term_coef_time %&gt;% glimpse() ## Rows: 54 ## Columns: 7 ## $ topic &lt;int&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3… ## $ term &lt;chr&gt; \"s(week, 6)1\", \"s(week, 6)2\", \"s(week, 6… ## $ estimate &lt;dbl&gt; -0.00808, 0.00992, 0.02061, 0.13494, 0.0… ## $ std.error &lt;dbl&gt; 0.0180, 0.0174, 0.0181, 0.0194, 0.0201, … ## $ statistic &lt;dbl&gt; -0.4485, 0.5706, 1.1401, 6.9606, 4.1245,… ## $ p.value &lt;dbl&gt; 6.54e-01, 5.68e-01, 2.54e-01, 3.54e-12, … ## $ name &lt;chr&gt; \"1. 백신 접종자\", \"1. 백신 접종자\", \"1. … term_coef_time %&gt;% mutate(term = str_extract(term, &quot;\\\\d$&quot;)) %&gt;% mutate(term = as.integer(term)) %&gt;% mutate(term = term * 2 - 1) %&gt;% mutate(term = as.factor(term)) %&gt;% filter(str_detect(name, &quot;^1|^2|^8&quot;)) %&gt;% ggplot(aes(x = term, y = estimate, color = name)) + geom_line(aes(group = name), size = 1.2) + geom_point(aes(shape = name), size = 3,) + geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate + std.error), width = .4, size = 1, position = position_dodge(.01)) + labs(x = &quot;기간(1월 ~ 3월)&quot;, y = &quot;문서당 주제 분포 비율&quot;, title = &quot;시간대별 주제 분포&quot;) + theme(plot.title = element_text(size = 20)) 11.5.3 주제 사이 상관성 주제사이의 상관성을 표시할 수 있다. library(reshape2) get_lower_tri &lt;- function(x){ x[upper.tri(x)] &lt;- NA return(x) } topicCorr(meta_fit) %&gt;% .$cor %&gt;% get_lower_tri() %&gt;% melt(na.rm = T) %&gt;% ggplot(aes(x = factor(Var1), y = factor(Var2), fill = value)) + geom_tile(color = &quot;white&quot;) + scale_fill_gradient2(low = &quot;blue&quot;, high = &quot;red&quot;, mid = &quot;white&quot;, midpoint = 0, limit = c(-1, 1), space = &quot;Lab&quot;) + geom_text(aes(Var1, Var2, label = round(value, 3)), color = &quot;black&quot;, size = 3) + theme_minimal() 11.6 과제 관심있는 검색어를 이용해 빅카인즈에서 기사를 검색해 수집한 기사의 주제모형을 구축한다. 주요 주제어에 대하여 시각화한다. 공변인(메타데이터)를 설정해, 공변인과 주제와의 관계를 탐색한다. 공변인은 시간, 언론사, 기사분류 등 선택. "],["참고문헌.html", "참고문헌", " 참고문헌 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
