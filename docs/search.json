[{"path":"index.html","id":"들어가며","chapter":"들어가며","heading":"들어가며","text":"","code":""},{"path":"index.html","id":"머리말","chapter":"들어가며","heading":"0.1 머리말","text":"","code":""},{"path":"index.html","id":"구성","chapter":"들어가며","heading":"0.2 구성","text":"텍스트마이닝에 필요한 R의 기초, 분석의 전단계, 그리고 분석 등으로 구성돼 있다. R에 익숙하면 곧바로 분석 전단계인 “4. 틀잡기”부터 시작해도 된다.","code":""},{"path":"index.html","id":"r기초","chapter":"들어가며","heading":"0.2.0.1 R기초","text":"R의 설치, 데이터유형과 구조, 시각화의 기초적인 내용과 R과 RStudio를 이용하는 과정에서 겪을 수 있는 문제해결 방법에 대해 다뤘다.","code":""},{"path":"index.html","id":"분석-전단계","chapter":"들어가며","heading":"0.2.0.2 분석 전단계","text":"텍스트마이닝의 전반적인 구조와 자료 수집과 불러오기, 정제(전처리)에 필요한 다양한 도구(stringr, dplyr, tidyr, purrr, regex 등)의 학습 및 정제(전처리)하는 방법에 대해 학습한다.","code":""},{"path":"index.html","id":"분석-i","chapter":"들어가며","heading":"0.2.0.3 분석 I","text":"단어의 빈도를 계산해 텍스트에서 의미를 추론하는 방식을 학습한다. 사전(감정사전)을 이용하는 방법, 상대적인 빈도(tf-idf, 가중로그승산비 등)를 계산하는 방법, 기계학습의 비지도학습(주제모형: topic models)으로 계산하는 방법 등을 학습한다.","code":""},{"path":"index.html","id":"분석-ii-예정","chapter":"들어가며","heading":"0.2.0.4 분석 II (예정)","text":"기계학습의 지도학습 방식에 대해 학습한다.online version book licensed Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.","code":""},{"path":"저작에-기여하신-분.html","id":"저작에-기여하신-분","chapter":"저작에 기여하신 분","heading":"저작에 기여하신 분","text":"“R 텍스트 마이닝” 전자책은 한국 R 사용자회와 제주대학교 제주대학교 언론홍보학과\n안도현 교수님이 공동 작업한 저작물로 건강한 R 생태계 확대에 크게 기여할 것으로 기대됩니다.","code":""},{"path":"install-setup.html","id":"install-setup","chapter":"1 .  설치 및 환경설정","heading":"1 .  설치 및 환경설정","text":"즐거운 텍스트 마이닝(Text Mining) 작업환경을 구축하기 위해서는 몇가지\n환경이 구비되어야만 한다. 먼저 작업할 데이터가 텍스트이기 때문에 텍스트에서\n특정 단어 색상을 달리하는 것은 추후 딥러닝 질의응답 인공지능 시스템을 구축할 때\n딥러닝 시스템이 질의에 대한 답변을 전체 텍스트의 일부를 색상을 달리하여\n시각적으로 표현하게 되면 사용자 편의성이 크게 개선시킬 수 있다.텍스트 색상을 달리할 경우 크게 두가지 부분이 이슈가 된다. 첫번째는 텍스트 마이닝\n콘솔 작업할 때 코드와 R 코드로 작업한 결과물을 출력할 때 색상을 차별화하는 것이고,\n다른 하나는 텍스트 마이닝 결과를 데이터 과학 제품으로 출력할 때 색상을 달리하여\n웹상으로 표현하는 것이다.","code":""},{"path":"install-setup.html","id":"tm-colors","chapter":"1 .  설치 및 환경설정","heading":"1.1 색상","text":"","code":""},{"path":"install-setup.html","id":"console-colors","chapter":"1 .  설치 및 환경설정","heading":"1.1.1 콘솔 색상","text":"glue 패키지 glue_col() 함수를 사용하게 되면\n텍스트에 색상을 입히는 작업을 간단하게 실행에 옮길 수 있다.","code":"\nlibrary(glue)\nlibrary(crayon)\n\nglue_col(\"{blue 1 + 2 = {red 1 + 2}}\")## 1 + 2 = 1 + 2\n"},{"path":"install-setup.html","id":"rmarkdown-colors","chapter":"1 .  설치 및 환경설정","heading":"1.1.2 R마크다운 색상","text":".Rmd R마크다운 파일 작업결과에 색상을 입히기 위해서는\nfansi 패키지가 필요하다.\nR마크다운 코드 덩어리에 다음 사항을 추가하고 R마크다운 작업을 수행하면\n자동으로 해당 색상을 .html, .pdf, shiny 결과물에 반영할 수 있다.R마크다운 색상 적용에 대한 자세한 사항은 rmarkdown terminal colors를\n참조한다.","code":"\n\n```r\nknitr::knit_hooks$set(output = function(x, options){\n  paste0(\n    '',\n    fansi::sgr_to_html(x = htmltools::htmlEscape(x), warn = FALSE),\n    ''\n  )\n})\n```\n',\n    fansi::sgr_to_html(x = htmltools::htmlEscape(x), warn = FALSE),\n    '"},{"path":"install-setup.html","id":"dataframe-color","chapter":"1 .  설치 및 환경설정","heading":"1.1.3 데이터프레임","text":"이를 확장하여 콘솔, R마크다운, 그리고 데이터프레임에도 색상을 적용하여\n반영시킬 수 있다.","code":""},{"path":"install-setup.html","id":"install-stacks","chapter":"1 .  설치 및 환경설정","heading":"1.2 메카브(MeCab) 설치","text":"빠르면서 성능이 좋다고 알려진 메카드(MeCab) 형태소 분석기를 설치한다.MeCab 설치과정","code":""},{"path":"install-setup.html","id":"install-mecab-mac","chapter":"1 .  설치 및 환경설정","heading":"1.2.1 맥","text":"MeCab 설치과정은 가장먼저 MeCab 설치부터 시작한다. 일본에서 제작했기 때문에\nRMeCaB 패키지를 설치하면 일본어 형태소 분석 작업을 바로 시작할 수 있다.\n한글을 형태소 분석하기 위해서는 은전한닢(mecab-ko)를 설치한 후에 R에서 사용할 수 있도록\n개발중인 bitTA 패키지를 설치하면 된다.","code":""},{"path":"install-setup.html","id":"install-mecab-mac-mecab","chapter":"1 .  설치 및 환경설정","heading":"1.2.1.1 MeCab 설치","text":"GitHub Installation RMeCab 1.07 M1 Mac #13 에 자세한 사항이 나와 있지만 간략하게 정리하면 다음과 같다.","code":"## xcode 설치되면 생략 ----- \n$ xcode-select --install\n\n## MeCab 설치 --------------\n$ cd ~/Downloads\n$ curl -fsSL 'https://drive.google.com/uc?export=download&id=0B4y35FiV1wh7cENtOXlicTFaRUE'  -o mecab-0.996.tar.gz\n$ tar xf mecab-0.996.tar.gz\n$ cd mecab-0.996\n$ ./configure --with-charset=utf8\n$ make\n$ sudo make install\n\n## MeCab 사전 설치 --------------\n$ cd ~/Downloads\n$ curl -fsSL 'https://drive.google.com/uc?export=download&id=0B4y35FiV1wh7MWVlSDBCSXZMTXM'  -o mecab-ipadic-2.7.0-20070801.tar.gz\n$ tar zvxf mecab-ipadic-2.7.0-20070801.tar.gz\n$ tar xf mecab-ipadic-2.7.0-20070801.tar.gz\n$ cd mecab-ipadic-2.7.0-20070801\n$ ./configure --with-charset=utf-8\n$ make\n$ sudo make install\n\n## MeCab 설치 테스트 --------------\n$ mecab\nすもももももももものうち"},{"path":"install-setup.html","id":"install-mecab-mac-rmecab","chapter":"1 .  설치 및 환경설정","heading":"1.2.1.2 RMeCab 설치 (생략)","text":"RMeCab GitHub 저장소에 설치사항을\n정리하여 보면 MeCab와 사전을 설치한 후에 install.packages() 에 RMeCab 패키지\n저장소를 달리 지정하여 설치하면 된다.","code":"\ninstall.packages(\"RMeCab\", repos = \"https://rmecab.jp/R\", type = \"source\") \n\nlibrary(RMeCab)\nres <- RMeCabC(\"すもももももももものうち\")\nunlist (res)\n## 名詞     助詞     名詞     助詞     名詞     助詞     名詞 \n## \"すもも\"     \"も\"   \"もも\"     \"も\"   \"もも\"     \"の\"   \"うち\" "},{"path":"install-setup.html","id":"install-mecab-mac-mecab-ko","chapter":"1 .  설치 및 환경설정","heading":"1.2.1.3 MeCab-ko 설치","text":"일본어 MeCab 설치과정과 동일하게 한국어 MeCab-ko를 설치한다.Bitbucket eunjeon/mecab-ko 저장소에서 mecab-ko 최신버전을 다운로드 한다.Bitbucket eunjeon/mecab-ko-dic 저장소에서 mecab-ko-dic 사전 최신버전을 다운로드 한다.","code":"## MeCab-ko 설치 ------------\n$ cd ~/Downloads\n$ curl -fsSL 'https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz' -o mecab-0.996-ko-0.9.2.tar.gz\n$ tar xzvf mecab-0.996-ko-0.9.2.tar.gz\n$ cd mecab-0.996-ko-0.9.2\n$ ./configure --with-charset=utf-8\n$ make\n$ sudo make install\n\n## MeCab-ko-dic 사전 설치 ------------\n$ cd ~/Downloads\n$ curl -fsSL 'https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz' -o mecab-ko-dic-2.1.1-20180720.tar.gz\n$ cd mecab-ko-dic-2.1.1-20180720\n$ ./configure --with-charset=utf-8\n$ make\n$ sudo make install"},{"path":"install-setup.html","id":"install-mecab-bitTA","chapter":"1 .  설치 및 환경설정","heading":"1.2.2 bitTA 설치","text":"","code":"\n## remotes::install_github(\"bit2r/bitTA\")\n## \n## library(bitTA)\n## \n## morpho_mecab(\"아버지가 방에 들어가신다.\")\n## #>      NNG      NNG \n## #> \"아버지\"     \"방\""},{"path":"install-setup.html","id":"리눅스","chapter":"1 .  설치 및 환경설정","heading":"1.2.3 리눅스","text":"","code":""},{"path":"install-setup.html","id":"윈도우","chapter":"1 .  설치 및 환경설정","heading":"1.2.4 윈도우","text":"MeCab 설치 이슈 참조","code":"\n## library(bitTA)\n## \n## morpho_mecab(\"아버지가 방에 들어가신다.\")"},{"path":"install-setup.html","id":"ggplot-font","chapter":"1 .  설치 및 환경설정","heading":"1.3 ggplot 글꼴","text":"단어구름(worldcloud)를 사용해서 텍스트 시각화를 많이 한다.\nggwordcloud 패키지는\nggplot에서 텍스트 단어구름을 자연스럽게 구현했다.\nggwordcloud에 내장된 전세계 사랑 이라는 단어가 love_words_small 데이터프레임으로 내장되어 있다. 이를 기본 글꼴을 사용해서 단어구름 시각화를 구현해보자.글꼴을 다양한 방식으로 구현하면 좀더 미려한 워드 클라우드를 뽑아낼 수 있다. 가장 최근에 네이버에서 공개한 마루부리 글꼴을 워드 클라우드에 반영해보자.마루 부리 글꼴 다운로드압축을 풀어 해당 글꼴을 운영체제에 설치sysfonts 패키지를 사용해서 R 글꼴로 등록showtext 패키지 showtext_auto() 함수로 ggplot에 사용할 수 있도록 설정다음 워드 클라우드를 통해 마루부리 글꼴이 잘 반영된 것이 확인되지만 다른 언어로\n표현된 글꼴을 마루부리 글꼴이 적절히 반영하지 않는 것도 확인된다.상기 문제를 풀고자 글꼴이 필요한데 구글 폰트에서 웹폰트를 가져와서 이를 워드 클라우드 작성에 활용해보자. sysfonts 패키지 font_add_google() 함수를 사용하면 R에서 바로 사용할 수 있는 글꼴을 바로 설치해주기 때문에 매우 편리하다.\n다만, Noto Serif KR 글꼴은 한글과 한자, 영어는 문제가 없어 보이지만 다른 언어를 표현하는데 문제가 있음을 알 수 있다.","code":"\nlibrary(ggwordcloud)\nlibrary(tidyverse)\n\ndata(\"love_words_small\")\n\nlove_words_small %>% \n  mutate(color = ifelse(word == \"사랑\", \"blue\", \"gray50\")) %>% \n  ggplot(aes(label = word, size = speakers, color = color)) +\n    geom_text_wordcloud() +\n    scale_size_area(max_size = 40) +\n    theme_minimal() +\n    scale_color_manual(values = c(\"blue\", \"gray50\"))\nlibrary(systemfonts)\nlibrary(sysfonts)\nlibrary(showtext)\n\n# 글꼴이 설치된 경로 표시\nfont_paths() ## [1] \"C:\\\\Windows\\\\Fonts\"\n\n# 운영체제 등록된 글꼴을 R 글꼴로 등록\nsysfonts::font_add(family = \"MaruBuri\", regular = 'MaruBuri-Regular.ttf')\n\n# 마루부리 글꼴이 설치되었는지 확인\nfont_files() %>% tibble() %>% filter(str_detect(family, \"Maru\"))## # A tibble: 4 × 6\n##   path             file         family face  version ps_name\n##   <chr>            <chr>        <chr>  <chr> <chr>   <chr>  \n## 1 C:/Windows/Fonts MaruBuri-Bo… MaruB… Regu… Versio… MaruBu…\n## 2 C:/Windows/Fonts MaruBuri-Li… MaruB… Regu… Versio… MaruBu…\n## 3 C:/Windows/Fonts MaruBuri-Re… MaruB… Regu… Versio… MaruBu…\n## 4 C:/Windows/Fonts MaruBuri-Se… MaruB… Regu… Versio… MaruBu…\n\n# ggplot에서 사용할 수 있도록 설정\nshowtext::showtext_auto()\n\nlove_words_small %>% \n  mutate(color = ifelse(word == \"사랑\", \"blue\", \"gray50\")) %>% \n  ggplot(aes(label = word, size = speakers, color = color)) +\n    geom_text_wordcloud(family = \"MaruBuri\") +\n    scale_size_area(max_size = 40) +\n    theme_minimal() +\n    scale_color_manual(values = c(\"blue\", \"gray50\"))\nsysfonts::font_add_google(name = \"Noto Serif KR\", family = \"noto_serif\")\n\nshowtext::showtext_auto()\n\nlove_words_small %>% \n  mutate(color = ifelse(word == \"사랑\", \"blue\", \"gray50\")) %>% \n  ggplot(aes(label = word, size = speakers, color = color)) +\n    geom_text_wordcloud(family = \"noto_serif\") +\n    scale_size_area(max_size = 40) +\n    theme_minimal() +\n    scale_color_manual(values = c(\"blue\", \"gray50\"))"},{"path":"browse.html","id":"browse","chapter":"2 .  헬로 월드","heading":"2 .  헬로 월드","text":"인식론의 틀에서 우리가 사는 세상은 세상 그 자체가 아니라, 우리 마음에 속에 비친 그림자이다. 따라서 세상은 “인식전 세상”과 “인식된 세상”으로 구분할 수 있다.인식의 과정은 자료-정보-지식-지혜(DIKW: Data, Information, Knowledge, Wisdom) 위계론을 통해 4단계 모형으로 구성할 수 있다.1단계: 인식전 세계에서 원자료(raw data) 수집1단계: 인식전 세계에서 원자료(raw data) 수집2단계: 원자료 정제해 자료(data)로 1차부호화2단계: 원자료 정제해 자료(data)로 1차부호화3단계: 자료를 분석해 정보(information)로 2차부호화3단계: 자료를 분석해 정보(information)로 2차부호화4단계: 정보를 지식(knowledge)으로 해석하고 지혜(wisdom)로 내면화4단계: 정보를 지식(knowledge)으로 해석하고 지혜(wisdom)로 내면화이를 순서대로 제시하면 다음과 같다.[인식전세계]-(수집)-원자료-(정제)-자료-(분석)-정보-(해석)-지식-(내면화)-지혜이를 소통의 관점에서 재구성하면 해석과 내면화의 과정을 의미공유 과정인 소통(communication)으로 대체할 수 있다.[인식전세계]-(수집)-원자료-(정제)-자료-(분석)-정보-(소통)-메시지이를 재정리하면 다음과 같다.바깥세상 -(수집)- 원자료 -(정제)-자료-(분석)-정보-(소통)-메시지텍스트마이닝에도 이 위계 과정을 적용해, 수집 -> 정제 -> 분석 - 소통 등의 구조로 이뤄진다. 각 단계별로 자세하게 다루기 이전에 전반적인 구조를 먼저 이해하자.수집수집정제(전처리)정제(전처리)분석분석소통소통","code":""},{"path":"browse.html","id":"수집","chapter":"2 .  헬로 월드","heading":"2.1 수집","text":"텍스트마이닝의 첫 단계다. 원자룔 수집한 다음, 정제하고 분석하기 위해 R환경에 탑재하는 단계다.텍스트데이터가 저장된 데이터구조는 다양하다. 벡터(vector), 데이터프레임(data frame), 리스트(list) 등의 구조에 저장해 사용한다. 가장 기본적인 텍스트데이터 구조는 문자형으로 구성된 문자벡터다. 학이편 1장의 텍스트데이터를 문자벡터에 담아 불러와 보자.\\n은 행바꿈을 의미하는 정규표현식이다.","code":"배우며 제때에 실행하면 진실로 즐겁지 않겠는가?\n벗이 먼 곳에서부터 온다면 참으로 즐겁지 않겠는가?\n남이 알아주지 않아도 성내지 않는다면 참으로 군자답지 않겠는가?\n\ntext_v <- c(\"배우며 제때에 실행하면 진실로 즐겁지 않겠는가?\n            벗이 먼 곳에서부터 온다면 참으로 즐겁지 않겠는가?\n            남이 알아주지 않아도 성내지 않는다면 참으로 군자답지 않겠는가?\")\ntext_v## [1] \"배우며 제때에 실행하면 진실로 즐겁지 않겠는가?\\n            벗이 먼 곳에서부터 온다면 참으로 즐겁지 않겠는가?\\n            남이 알아주지 않아도 성내지 않는다면 참으로 군자답지 않겠는가?\"\n"},{"path":"browse.html","id":"정제전처리","chapter":"2 .  헬로 월드","heading":"2.2 정제(전처리)","text":"","code":""},{"path":"browse.html","id":"정돈텍스트tidy-text","chapter":"2 .  헬로 월드","heading":"2.2.1 정돈텍스트(tidy text)","text":"문자형, 숫자형, 논리형 등의 데이터 유형 중 텍스트데이터유형은 문자형이다. 그런데 데이터 유형이 문자형이어서는 컴퓨터로 분석할 수 없다. 양화(quantification)시켜 컴퓨터가 계산할 수 있도록 바꿔줘야 한다.\nf\n다양한 방법이 있는데, 데이터프레임을 “정돈된 데이터 원리(tidy data principle)”에 따라 만든 정돈텍스트(tidy text)구조부터 시작하자.정돈텍스트(tidy text)는 데이터프레임을 텍스트마이닝에 적합하도록 만든 데이터구조다. 일반적으로 데이터프레임은 복수의 열(column)로 이뤄져 있는데, 정돈텍스트 구조에서는 열을 단 하나로 고정시켰다. 즉, 행(row) 하나에 토큰(token)이 하나만 할당돼 있다(one-token-per-row). 토큰은 텍스트분석의 기본 단위다. 단어 하나를 토큰으로 이용하기도 하고, 복수의 단어(n-gram)를 묶어 하나의 토큰으로 이용하기도 한다.","code":""},{"path":"browse.html","id":"토큰화-unnest_tokens-함수","chapter":"2 .  헬로 월드","heading":"2.2.2 토큰화: unnest_tokens 함수","text":"데이터프레임 구성 tibble데이터프레임 구성 tibble토큰화 unnest_tokens토큰화 unnest_tokens정돈된 세계(tidyverse)에서 텍스트데이터를 정돈텍스트 구조에 담는 함수가 tidytext패키지에서 제공하는 unnest_tokens다. (철자에 주의하자. 복수 s가 붙어 있다.)tidytext의 자세한 사용법은 아래 사용설명서를 참조한다.tidytext https://cran.r-project.org/web/packages/tidytext/unnest_tokens은 데이트프레임 구조의 데이터를 받아 처리한다. 앞서 만든 문자벡터 text_v를 정돈텍스트로 바꾸기 위해서는 먼저 데이터프레임으로 바꿔야 한다. tibble패키지의 tibble함수를 이용해 text_v의 내용을 데이터프레임에 담아 보자. tibble은 tidyverse에 부착돼 있다.티블(tibble)은 현대적인 데이터프레임의 형식이다. 문자열을 요인(factor)형식으로 로 바꾸지 않는 등 텍스트분석에 사용하기 좋다.데이터프레임의 열 이름을 무엇으로 설정했는지에 주의하자. 여기서는 “text”로 설정했다.이제 생성된 데이터프레임을 tidytext패키지의 unnest_tokens함수를 이용해 정돈텍스트 구조로 바꿔보자. (철자에 주의. 복수 s)앞서 만든 데이터프레임의 열 이름(“text”)을 input에 투입한 것을 잘 기억하자.input : 입력한 데이터프레임의 열 이름output : 출력할 정돈텍스트의 열 이름이처럼 텍스트데이터를 컴퓨터가 분석할 수 있도록 양화할 수 있는 단위로 나누는 것은 토큰화(tkoenization)이라고 한다.이번에는 토큰을 단어 2개로 묶은 ngram으로 정돈텍스트를 만들어 보자.unnest_tokens함수의 자세한 사용법은 ?unnest_tokens의 도움말을 참고한다.","code":"\ninstall.packages(\"tidytext\")\nlibrary(tidyverse)\ntext_df <- tibble(text = text_v)\ntext_df## # A tibble: 1 × 1\n##   text                                                      \n##   <chr>                                                     \n## 1 \"배우며 제때에 실행하면 진실로 즐겁지 않겠는가?\\n        …\n\nlibrary(tidytext)\ntext_df %>% unnest_tokens(output = word, input = text)## # A tibble: 21 × 1\n##   word    \n##   <chr>   \n## 1 배우며  \n## 2 제때에  \n## 3 실행하면\n## 4 진실로  \n## 5 즐겁지  \n## 6 않겠는가\n## # … with 15 more rows\n\ntext_df %>% unnest_tokens(output = word, input = text, \n                          token = \"ngrams\", n = 2 )## # A tibble: 20 × 1\n##   word           \n##   <chr>          \n## 1 배우며 제때에  \n## 2 제때에 실행하면\n## 3 실행하면 진실로\n## 4 진실로 즐겁지  \n## 5 즐겁지 않겠는가\n## 6 않겠는가 벗이  \n## # … with 14 more rows\n"},{"path":"browse.html","id":"분석","chapter":"2 .  헬로 월드","heading":"2.3 분석","text":"","code":""},{"path":"browse.html","id":"빈도-count-함수","chapter":"2 .  헬로 월드","heading":"2.3.1 빈도: count 함수","text":"이제 텍스트마이닝을 해보자. 어떤 문서에서 많이 사용하는 단어가 있다면, 그 문서는 그 단어가 나타내는 의미에 의해 규정된다고 할 수 있다. “사랑”이란 단어를 많이 사용한 문서면 사랑에 대한 문서이고, “학습”이란 단어를 많이 사용했으면 학습에 대한 문서일 가능성이 높다.문서에 등장하는 단어를 세어 주는 함수가 count다. count함수를 앞서 정돈텍스트 구조에 저장된 텍스트에 많이 등장한 단어가 무멋인지 찾아보자.“않겠는가”가 3회, “즐겁지”와 “참으로”로가 각각 2회 사용됐다. 즐거움에 관한 문서라고 가늠할 수 있다.","code":"\ntext_tk <- text_df %>% unnest_tokens(output = word, input = text)\ntext_tk %>% \n  count(word, sort = TRUE)## # A tibble: 17 × 2\n##   word           n\n##   <chr>      <int>\n## 1 않겠는가       3\n## 2 즐겁지         2\n## 3 참으로         2\n## 4 곳에서부터     1\n## 5 군자답지       1\n## 6 남이           1\n## # … with 11 more rows\n"},{"path":"browse.html","id":"소통","chapter":"2 .  헬로 월드","heading":"2.4 소통","text":"분석결과를 해석해 의미를 공유하는 과정이다. 분석결과를 표에 정리하거나 도표로 시각화한다. 물론 글로도 분석결과의 의미를 정리한다. 여기서는 먼저 분석결과를 도표에 시각화하는 작업부터 해보자.","code":""},{"path":"browse.html","id":"시각화","chapter":"2 .  헬로 월드","heading":"2.4.1 시각화","text":"분석결과가 데이터프레임에 저장돼 있으므로 ggplot2패키지로 직접 파이핑(piping)할 수 있다. ggplot2는 파이프(%>%)가 아니라 +로 레이어를 더하는 방식을 이용한다는 점에 주의하자.ggplot2는 tidyverse에 부착돼 있다.다음 코드는 2회 이상 등장filter(n > 1)한 단어를 등장빈도 순서대로 정렬mutate(word = reorder(word, n))해 막대도포geom_col()로 시각화했다.","code":"\ntext_tk %>% \n  count(word, sort = TRUE) %>% \n  filter(n > 1) %>% \n  mutate(word = reorder(word, n)) %>% \n  ggplot(aes(word, n)) +\n  geom_col() +\n  coord_flip()"},{"path":"browse.html","id":"연습","chapter":"2 .  헬로 월드","heading":"2.5 연습","text":"다음은 소설가가 이상의 오감도다. 오감도에서 자주 사용된 단어의 빈도를 계산하시오.작업순서는 다음과 같다.자료준비\n문자벡터에 저장 c\n데이터프레임으로 변환 tibble\n문자벡터에 저장 c데이터프레임으로 변환 tibble정제\n토큰화 unnest_tokens\n토큰화 unnest_tokens분석 count소통(시각화) ggplot텍스트 양이 많은 편이니 텍스트 객체를 별도로 마련하자.이제 분석결과를 ggplot2의 막대도표로 시각화하자. 4회 이상 등장한 단어만 포함시키자.","code":" 13인의 아해가 도로로 질주하오.\n(길은 막다른 골목이 적당하오.)\n\n제1의 아해가 무섭다고 그리오.\n제2의 아해도 무섭다고 그리오.\n제3의 아해도 무섭다고 그리오.\n제4의 아해도 무섭다고 그리오.\n제5의 아해도 무섭다고 그리오.\n제6의 아해도 무섭다고 그리오.\n제7의 아해도 무섭다고 그리오.\n제8의 아해도 무섭다고 그리오.\n제9의 아해도 무섭다고 그리오.\n제10의 아해도 무섭다고 그리오.\n제11의 아해가 무섭다고 그리오.\n제12의 아해도 무섭다고 그리오.\n제13의 아해도 무섭다고 그리오.\n13인의 아해는 무서운 아해와 무서워하는 아해와 그렇게뿐이 모였소.\n(다른 사정은 없는 것이 차라리 나았소)\n\n그중에 1인의 아해가 무서운 아해라도 좋소.\n그중에 2인의 아해가 무서운 아해라도 좋소.\n그중에 2인의 아해가 무서워하는 아해라도 좋소.\n그중에 1인의 아해가 무서워하는 아해라도 좋소.\n\n(길은 뚫린 골목이라도 적당하오.)\n13인의 아해가 도로로 질주하지 아니하여도 좋소.\nogamdo_txt <- \"13인의 아해가 도로로 질주하오.\n(길은 막다른 골목이 적당하오.)\n\n제1의 아해가 무섭다고 그리오.\n제2의 아해도 무섭다고 그리오.\n제3의 아해도 무섭다고 그리오.\n제4의 아해도 무섭다고 그리오.\n제5의 아해도 무섭다고 그리오.\n제6의 아해도 무섭다고 그리오.\n제7의 아해도 무섭다고 그리오.\n제8의 아해도 무섭다고 그리오.\n제9의 아해도 무섭다고 그리오.\n제10의 아해도 무섭다고 그리오.\n제11의 아해가 무섭다고 그리오.\n제12의 아해도 무섭다고 그리오.\n제13의 아해도 무섭다고 그리오.\n13인의 아해는 무서운 아해와 무서워하는 아해와 그렇게뿐이 모였소.(다른 사정은 없는 것이 차라리 나았소)\n\n그중에 1인의 아해가 무서운 아해라도 좋소.\n그중에 2인의 아해가 무서운 아해라도 좋소.\n그중에 2인의 아해가 무서워하는 아해라도 좋소.\n그중에 1인의 아해가 무서워하는 아해라도 좋소.\n\n(길은 뚫린 골목이라도 적당하오.)\n13인의 아해가 도로로 질주하지 아니하여도 좋소.\"\n# 자료준비\ntxt_df <- tibble(text = ogamdo_txt)\n\n# 정제(토큰화) 및 분석\ntxt_tk <- txt_df %>% \n  unnest_tokens(output = word,\n                input = text) %>% \n  count(word, sort = T)\ntxt_tk## # A tibble: 45 × 2\n##   word         n\n##   <chr>    <int>\n## 1 그리오      13\n## 2 무섭다고    13\n## 3 의          13\n## 4 제          13\n## 5 아해도      11\n## 6 아해가       8\n## # … with 39 more rows\n\ntxt_tk %>%\n  filter(n >= 4) %>% \n  mutate(word = reorder(word, n)) %>% \n  ggplot(aes(word, n)) +\n  geom_col() +\n  coord_flip()"},{"path":"browse.html","id":"과제","chapter":"2 .  헬로 월드","heading":"2.5.1 과제","text":"소설이나 신문기사 등 마음에 드는 텍스트를 한 건 골라, 단어빈도를 계산하시오. 분석결과를 표와 막대도표로 제시하시오.","code":""},{"path":"view-text.html","id":"view-text","chapter":"3 .  텍스트 살펴보기","heading":"3 .  텍스트 살펴보기","text":"","code":""},{"path":"view-text.html","id":"text-highlight","chapter":"3 .  텍스트 살펴보기","heading":"3.1 특정 단어 강조","text":"윤석열 대통령 취임사 텍스트를 취임사_윤석열.txt 파일로 저장한 후에\n특정 단어 자유 를 탐색하여 색상을 달리하여 출력해보자.","code":"\n## 기본 텍스트 패키지\nlibrary(tidyverse)\nlibrary(tidytext)\n## 한국 텍스트 처리 패키지\n# library(RMeCab)\n# library(bitTA)\n## 글 색상\nlibrary(glue)\nlibrary(crayon)\nlibrary(fansi)\noptions(crayon.enabled = TRUE)\n\n\nyoon_raw <- read_lines(\"data/취임사_윤석열.txt\")\n\nyoon_txt <- yoon_raw[yoon_raw !=\"\"]\n\ncrayon_words <- function(input_text, word = \"자유\") {\n\n  replaced_text <- str_replace_all(input_text, word, \"{red {word}}\")\n\n  for(i in 1:length(replaced_text)) {\n    crayon_text <- glue::glue_col(deparse(replaced_text[[i]]))\n    print(crayon_text)\n  }\n}\n\ncrayon_words(input_text = yoon_txt, \"자유\")## \"존경하고 사랑하는 국민 여러분,\"\n## \"750만 재외동포 여러분,\"\n## \"그리고 자유를 사랑하는 세계 시민 여러분,\"\n## \"저는 이 나라를 자유민주주의와 시장경제 체제를 기반으로 국민이 진정한 주인인 나라로 재건하고, 국제사회에서 책임과 역할을 다하는 나라로 만들어야 하는 시대적 소명을 갖고 오늘 이 자리에 섰습니다.\"\n## \"역사적인 자리에 함께해 주신 국민 여러분께 감사드립니다.\"\n## \"문재인, 박근혜 전 대통령, 그리고 할리마 야콥 싱가포르 대통령, 포스탱 아르샹쥬 투아데라 중앙아프리카공화국 대통령, 왕치산 중국 국가부주석, 메가와티 수카르노푸트리 인도네시아 전 대통령, 더글러스 엠호프 해리스 미국 부통령 부군, 조지 퓨리 캐나다 상원의장, 하야시 요시마사 일본 외무상을 비롯한 세계 각국의 경축 사절과 내외 귀빈 여러분께도 깊이 감사드립니다.\"\n## \"이 자리를 빌려 지난 2년간 코로나 팬데믹을 극복하는 과정에서 큰 고통을 감내해주신 국민 여러분께 경의를 표합니다.\"\n## \"그리고 헌신해주신 의료진 여러분께도 감사드립니다.\"\n## \"존경하는 국민 여러분,\"\n## \"세계 시민 여러분,\"\n## \"지금 전 세계는 팬데믹 위기, 교역 질서의 변화와 공급망의 재편, 기후 변화, 식량과 에너지 위기, 분쟁의 평화적 해결의 후퇴 등 어느 한 나라가 독자적으로,  또는 몇몇 나라만 참여해서 해결하기 어려운 난제들에 직면해 있습니다.\"\n## \"다양한 위기가 복합적으로 인류 사회에 어두운 그림자를 드리우고 있는 것입니다.\"\n## \"또한 우리나라를 비롯한 많은 나라들이 국내적으로 초저성장과 대규모 실업, 양극화의 심화와 다양한 사회적 갈등으로 인해 공동체의 결속력이 흔들리고 와해되고 있습니다.\"\n## \"한편, 이러한 문제들을 해결해야 하는 정치는 이른바 민주주의의 위기로 인해 제 기능을 하지 못하고 있습니다.\"\n## \"가장 큰 원인으로 지목되는 것이 바로 반지성주의입니다.\"\n## \"견해가 다른 사람들이 서로의 입장을 조정하고 타협하기 위해서는 과학과 진실이 전제되어야 합니다.\"\n## \"그것이 민주주의를 지탱하는 합리주의와 지성주의입니다.\"\n## \"국가 간, 국가 내부의 지나친 집단적 갈등에 의해 진실이 왜곡되고, 각자가 보고 듣고 싶은 사실만을 선택하거나 다수의 힘으로 상대의 의견을 억압하는 반지성주의가 민주주의를 위기에 빠뜨리고 민주주의에 대한 믿음을 해치고 있습니다.\"\n## \"이러한 상황이 우리가 처해있는 문제의 해결을 더 어렵게 만들고 있습니다.\"\n## \"그러나 우리는 할 수 있습니다.\"\n## \"역사를 돌이켜 보면 우리 국민은 많은 위기에 처했지만 그럴 때마다 국민 모두 힘을 합쳐 지혜롭게, 또 용기있게 극복해 왔습니다.\"\n## \"저는 이 순간 이러한 위기를 극복하는 책임을 부여받게 된 것을 감사한 마음으로 받아들이고, 우리 위대한 국민과 함께 당당하게 헤쳐 나갈 수 있다고 확신합니다.\"\n## \"또 세계 시민과 힘을 합쳐 국내외적인 위기와 난제들을 해결해 나갈 수 있다고 믿습니다.\"\n## \"존경하는 국민 여러분,\"\n## \"세계 시민 여러분,\"\n## \"저는 이 어려움을 해결해 나가기 위해서 우리가 보편적 가치를 공유하는 것이 매우 중요하다고 생각합니다.\"\n## \"그것은 바로 ‘자유’입니다.\"\n## \"우리는 자유의 가치를 제대로, 그리고 정확하게 인식해야 합니다.\"\n## \"자유의 가치를 재발견해야 합니다.\"\n## \"인류 역사를 돌이켜보면 자유로운 정치적 권리, 자유로운 시장이 숨 쉬고 있던 곳은 언제나 번영과 풍요가 꽃 피었습니다.\"\n## \"번영과 풍요, 경제적 성장은 바로 자유의 확대입니다.\"\n## \"자유는 보편적 가치입니다.\"\n## \"우리 사회 모든 구성원이 자유 시민이 되어야 하는 것입니다.\"\n## \"어떤 개인의 자유가 침해되는 것이 방치된다면 우리 공동체 구성원 모두의 자유마저 위협받게 됩니다.\"\n## \"자유는 결코 승자독식이 아닙니다.\"\n## \"자유 시민이 되기 위해서는 일정한 수준의 경제적 기초, 그리고 공정한 교육과 문화의 접근 기회가 보장되어야 합니다.\"\n## \"이런 것 없이 자유 시민이라고 할 수 없습니다.\"\n## \"어떤 사람의 자유가 유린되거나 자유 시민이 되는데 필요한 조건을 충족하지 못한다면 모든 자유 시민은 연대해서 도와야 합니다.\"\n## \"그리고 개별 국가뿐 아니라 국제적으로도 기아와 빈곤, 공권력과 군사력에 의한 불법 행위로 개인의 자유가 침해되고 자유 시민으로서의 존엄한 삶이 유지되지 않는다면 모든 세계 시민이 자유 시민으로서 연대하여 도와야 하는 것입니다.\"\n## \"모두가 자유 시민이 되기 위해서는 공정한 규칙을 지켜야 하고, 연대와 박애의 정신을 가져야 합니다.\"\n## \"존경하는 국민 여러분,\"\n## \"국내 문제로 눈을 돌려 제가 중요하게 생각하는 방향에 대해 말씀드리겠습니다.\"\n## \"우리나라는 지나친 양극화와 사회 갈등이 자유와 민주주의를 위협할 뿐 아니라 사회 발전의 발목을 잡고 있습니다.\"\n## \"저는 이 문제를 도약과 빠른 성장을 이룩하지 않고는 해결하기 어렵다고 생각합니다.\"\n## \"빠른 성장 과정에서 많은 국민이 새로운 기회를 찾을 수 있고, 사회 이동성을 제고함으로써 양극화와 갈등의 근원을 제거할 수 있습니다.\"\n## \"도약과 빠른 성장은 오로지 과학과 기술, 그리고 혁신에 의해서만 이뤄낼 수 있는 것입니다.\"\n## \"과학과 기술, 그리고 혁신은 우리의 자유민주주의를 지키고 우리의 자유를 확대하며 우리의 존엄한 삶을 지속 가능하게 할 것입니다.\"\n## \"과학과 기술, 그리고 혁신은 우리나라 혼자만의 노력으로는 달성하기 어렵습니다.\"\n## \"자유와 창의를 존중함으로써 과학 기술의 진보와 혁신을 이뤄낸 많은 나라들과 협력하고 연대해야만 합니다.\"\n## \"존경하는 국민 여러분,\"\n## \"세계 시민 여러분,\"\n## \"자유민주주의는 평화를 만들어내고, 평화는 자유를 지켜줍니다.\"\n## \"그리고 평화는 자유와 인권의 가치를 존중하는 국제사회와의 연대에 의해 보장이 됩니다.\"\n## \"일시적으로 전쟁을 회피하는 취약한 평화가 아니라 자유와 번영을 꽃피우는 지속 가능한 평화를 추구해야 합니다.\"\n## \"전 세계 어떤 곳도 자유와 평화에 대한 위협에서 자유롭지 못합니다.\"\n## \"지금 한반도와 동북아의 평화도 마찬가지입니다.\"\n## \"저는 한반도뿐 아니라 아시아와 세계의 평화를 위협하는 북한의 핵 개발에 대해서도 그 평화적 해결을 위해 대화의 문을 열어놓겠습니다.\"\n## \"그리고 북한이 핵 개발을 중단하고 실질적인 비핵화로 전환한다면 국제사회와 협력하여 북한 경제와 북한 주민의 삶의 질을 획기적으로 개선할 수 있는 담대한 계획을 준비하겠습니다.\"\n## \"북한의 비핵화는 한반도에 지속 가능한 평화를 가져올 뿐 아니라 아시아와 전 세계의 평화와 번영에도 크게 기여할 것입니다.\"\n## \"사랑하고 존경하는 국민 여러분,\"\n## \"지금 우리는 세계 10위권의 경제 대국 그룹에 들어가 있습니다.\"\n## \"그러므로 우리는 자유와 인권의 가치에 기반한 보편적 국제 규범을 적극 지지하고 수호하는데 글로벌 리더 국가로서의 자세를 가져야 합니다.\"\n## \"우리나라뿐 아니라 세계 시민 모두의 자유와 인권을 지키고 확대하는데 더욱 주도적인 역할을 해야 합니다.\"\n## \"국제사회도 대한민국에 더욱 큰 역할을 기대하고 있음이 분명합니다.\"\n## \"지금 우리나라는 국내 문제와 국제 문제를 분리할 수 없습니다.\"\n## \"국제사회가 우리에게 기대하는 역할을 주도적으로 수행할 때 국내 문제도 올바른 해결 방향을\"\n## \"찾을 수 있는 것입니다.\"\n## \"저는 자유, 인권, 공정, 연대의 가치를 기반으로 국민이 진정한 주인인 나라, 국제사회에서 책임을 다하고 존경받는 나라를 위대한 국민 여러분과 함께 반드시 만들어 나가겠습니다.\"\n## \"감사합니다.\"\n"},{"path":"view-text.html","id":"text-highlight-locatin","chapter":"3 .  텍스트 살펴보기","heading":"3.2 단어 위치","text":"윤석열 대통령 취임사에서 가장 빈도수가 높은 명사 5개를 찾아낸다.\n이를 위해서 먼저 텍스트를 각행별로 텍스트를 데이터프레임으로 변환시킨다.\n그리고 나서 메카브(MeCab) 형태소 분석기를 사용해서 연설문 형태소 분석을 수행하고\n명사만 추출한 후에 가장 빈도수가 높은 단어 3개를 뽑아낸다.윤 대통령 취임사에서 가장 많이 언급된 명사 3개(자유, 국민, 시민)가 취임사 어느\n부분에 위치하는지 시각화를 한다. 이를 위해서 ggpage\n패키지를 활용하여 ggpage_build() 함수와 ggpage_plot() 함수를 사용하여 깔끔하게 시각화한다.","code":"\nlibrary(RcppMeCab)\nlibrary(tidytext)\n\nyoon_tbl <- yoon_txt %>% \n  enframe(name = \"행\", value = \"text\") %>% \n  filter(text != \"\") \n\n\nyoun_noun <- yoon_tbl %>% \n  unnest_tokens( output = 분석_텍스트,\n                 input = text, \n                 token = RcppMeCab::pos) %>% \n  separate(분석_텍스트, c(\"명사\", \"형태소\"), sep = \"/\") %>% \n  filter(형태소 == \"nng\") %>% \n  count(명사, sort = TRUE, name = \"빈도수\")\n\nyoun_top_three <- youn_noun %>% \n  slice_max(빈도수, n = 3) %>% \n  pull(명사)\n\nyoun_top_three## [1] \"자유\" \"국민\" \"시민\"\n\nlibrary(ggpage)\n\nyoon_tbl %>%\n  ggpage_build(wtl = TRUE, lpp = 30, x_space_pages =10, y_space_pages = 0, nrow = 1) %>%   \n  unnest_tokens( output = 분석_텍스트,\n                 input = word, \n                 token = RcppMeCab::pos,\n                 drop = FALSE) %>% \n  separate(분석_텍스트, c(\"명사\", \"형태소\"), sep = \"/\") %>% \n  mutate(highlight = case_when(명사 %in% c(\"자유\") ~ \"자유\",\n                               명사 %in% c(\"국민\") ~ \"국민\",\n                               명사 %in% c(\"시민\") ~ \"시민\",\n                               TRUE ~ \"기타\"))  %>% \n  mutate(highlight = factor(highlight, levels=c(\"자유\", \"국민\", \"시민\", \"기타\"))) %>% \n  ggpage_plot(aes(fill = highlight),\n              paper.show = TRUE, page.number = \"top\", paper.limits = 3) +\n  scale_fill_manual(values = c(\"blue\", \"red\", \"green\", \"darkgray\")) +\n  labs(title = \"2022 윤석열 대통령 취임사\",\n       fill = NULL) +\n  theme_void(base_family = \"NanumGothic\")  "},{"path":"view-text.html","id":"취임사-요약","chapter":"3 .  텍스트 살펴보기","heading":"3.3 취임사 요약","text":"Ainize Teachable-NLP를 사용한 kobart 문서요약 텍스트/신문기사를 사용하여 윤 대통령 취임사를 요약한다.","code":"from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n#  사전 훈련 토큰, 모형 다운로드\ntokenizer = PreTrainedTokenizerFast.from_pretrained(\"ainize/kobart-news\")\nmodel = BartForConditionalGeneration.from_pretrained(\"ainize/kobart-news\")\n# 입력 텍스트\nwith open('data/취임사_윤석열.txt') as f:\n  input_text = f.read()\n\nimport re\n\ninput_text = re.sub(r\"\\n\", \" \", input_text)\n\n\n\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n# Generate Summary Text Ids\nsummary_text_ids = model.generate(\n    input_ids=input_ids,\n    bos_token_id=model.config.bos_token_id,\n    eos_token_id=model.config.eos_token_id,\n    length_penalty=2.0,\n    max_length=142,\n    min_length=56,\n    num_beams=4,\n)\n# Decoding Text\nprint(tokenizer.decode(summary_text_ids[0], skip_special_tokens=True))"},{"path":"text-processing.html","id":"text-processing","chapter":"4 .  텍스트 전처리","heading":"4 .  텍스트 전처리","text":"텍스트 원데이터(Raw data)를 R에서 처리하는 방식은 오래전부터 많은 함수와 도구가\n개발되어 현재도 사용가능하다. 하지만 tidyverse 생태계를 구성하고 있는\nstringr은 문자열(string)을\n다루기 위해서 개발된 전용 패키지로 티블(tibble) 데이터프레임과 결합할 경우\n텍스트 전처리 작업에 생산성을 높일 수 있다. stringr 패키지는 stringi 패키지를\n기반으로 하여 str_* 방식으로 문자열 처리 함수 API 잘 문서화 되어 되어\n한번 기억하면 기억을 되새기며 코딩하기 적합하다.텍스트 처리 tibble과 stringr 조합","code":""},{"path":"text-processing.html","id":"text-repex","chapter":"4 .  텍스트 전처리","heading":"4.1 정규표현식","text":"텍스트 데이터는 신문기사, 책, 블로그, 채팅로그 등 다양한 형태로 나타난다. 이런 텍스트를 다루는데 별도 언어가 필요한데\n이것이 정규표현식(regular expression)이다. 강력한 정규표현식을 사용하기 위해서 기본적으로 R에서 문자열을 불러오고 패턴을 매칭하고,\n문자열 일부를 떼어내고 등등 다양한 기능을 수행하는 팩키지가 있다. 물론 R Base의 기본 기능함수도 있지만, stringi, stringr, rebus 팩키지를 조합하여\n사용하는 것이 생산성을 최대한 끌어올릴 수 있다.텍스트/문자열 툴체인stringr: RStudio 개발환경에서 str_ + 탭완성 기능을 조합하여 일관성을 갖고 가장 많이 사용되는 문자열 처리 기능을 함수로 제공하는 팩키지stringi: 속도 및 확장성, 기능에서 R 환경에서 타의 추종을 불허하는 기본 문자열 처리 R 팩키지rebus: Regular Expression Builder, Um, Something: 사람이 읽고 사용하기 쉬운 형태로 바꾼 정규식 구현 팩키지텍스트/문자열 처리작업을 수행할 때 tidyverse 팩키지와 마찬가지로 복잡하고 난잡한 부분을 가능하면 숨기고 나서\n가장 많이 활용되는 기능만 뽑아서 가장 생산성 높게 사용하는 툴체인으로 활용가능하다.\n정규표현식의 보다 자세한 사항은 데이터 과학 프로그래밍 정규표현식을 참고한다.","code":""},{"path":"text-processing.html","id":"nlp-stringr-basic-operation","chapter":"4 .  텍스트 전처리","heading":"4.2 문자열 기본 작업","text":"기본적으로 작업하는 기본 문자열 작업은 문자열을 찾고, 문자열을 쪼개고, 문자열을 치환하는 작업이다.\nstringr에서 문자열 매칭에 문자열을 탐지하고, 탐지된 문자열을 뽑아내고, 매칭된 문자열 갯수를 찾아내는 기본작업을 지원한다.\n이런 기본기 작업에 활용되는 다음과 같다.str_detect()str_subset()str_count()문자열을 특정 패터에 맞춰 쪼개고 나면 str_split() 함수를 통해 반환되는 객체는 리스트 자료형이 된다. 왜냐하면 고정된 길이를 갖지 않을 수 있기 때문에\n반환되는 리스트를 lapply() 함수와 연동하여 길이를 바로 구하는 것도 많이 작어되는 기본 작업 패턴이다.\n그리고 str_replace(), str_replace_all() 함수를 활용하여 문자열을 치환하는 것도 많이 사용되는 패턴이다.","code":"\nlibrary(stringr)\nlibrary(magrittr)\n\n# 1. 문자열 매칭하기 -------------------------\nhangul <- c(\"자동차\", \"한국\", \"한국산 자동차와 손수레\")\n\nstr_detect(string = hangul, pattern = \"자동차\")## [1]  TRUE FALSE  TRUE\n\nstr_detect(string = hangul, pattern = fixed(\"자동차\"))## [1]  TRUE FALSE  TRUE\n\nstr_subset(string = hangul, pattern = fixed(\"자동차\"))## [1] \"자동차\"                 \"한국산 자동차와 손수레\"\n\nstr_count(string = hangul, pattern = fixed(\"자동차\"))## [1] 1 0 1\n\n# 2. 문자열 쪼개기 -------------------------\nhangul <- c(\"한국산 자동차와 손수레 그리고 오토바이\")\n\nstr_split(hangul, pattern = \" 그리고 |와\")## [[1]]\n## [1] \"한국산 자동차\" \" 손수레\"       \"오토바이\"\n\nstr_split(hangul, pattern = \" 그리고 |와\", n=2)## [[1]]\n## [1] \"한국산 자동차\"           \" 손수레 그리고 오토바이\"\n\nhanguls <- c(\"한국산 자동차와 손수레 그리고 오토바이\",\n             \"독일산 자동차 그리고 아우토반\")\n\nstr_split(hanguls, pattern = \" 그리고 |와\", simplify = TRUE)##      [,1]            [,2]       [,3]      \n## [1,] \"한국산 자동차\" \" 손수레\"  \"오토바이\"\n## [2,] \"독일산 자동차\" \"아우토반\" \"\"\n\nhanguls_split <- str_split(hanguls, pattern = \" 그리고 |와\")\n\npurrr::map_int(hanguls_split, length)## [1] 3 2\n\n# 3. 매칭된 문자열 치환 --------------------\n\nstr_replace_all(hanguls,\n                pattern = \"와\",\n                replacement = \" 그리고\")## [1] \"한국산 자동차 그리고 손수레 그리고 오토바이\"\n## [2] \"독일산 자동차 그리고 아우토반\"\n"},{"path":"text-processing.html","id":"tibble-stringr","chapter":"4 .  텍스트 전처리","heading":"4.3 tibble + stringr","text":"짧은 텍스트는 문자열과 stringr 정규표현식을 조합하여 처리해도 되지만,\n데이터프레임 dplyr 기본기를 익힌 경우 텍스트 문자열을 데이터프레임으로\n변환시킨 후에 이를 stringr 함수와 결합하여 텍스트를 처리하게 되면\n좀더 구조화되고 가독성 높은 코드를 작성할 수 있게 된다.이상의 오감도 텍스트를 가져와서 텍스트 분석을 시작할 경우 파일에 읽어와도 되지만,\n문자열로 읽어 ogamdo_txt 변수에 저장해 두고 이를 몇단계를 거쳐 데이터프레임으로\n변환시킨다.이상 오감도는 총 25 줄로 구성되어 있는데 숫자가 들어간 행이 있고 그렇지 않은 행이 있는데\n몇 문장에 숫자가 포함되어 있는지 str_detect() 함수와 숫자패턴을 탐지하는 정규표현식\n\\\\d 를 조합하면 부울 참/거짓으로 변수를 생성시킬 수 있고 이를 count() 개수 함수를\n사용하면 빈도를 쉽게 구할 수 있다.str_squish() 함수를 통해 연속된 공백을 제거하고 하나의 공백으로 처리할 수\n있기 때문에 텍스트가 깔끔하지 못한 경우 큰 도움이 되는 함수로 꼭 기억해두자.\n만약 면자열 시작과 끝에 공백이 있는 경우 str_trim()을 사용하면 큰 힘 들이지 않고\n모든 문자열에 대해 공백을 제거할 수 있다.전체적인 텍스트 정제작업이 마무리된 경우 dplyr::pull() 함수를 사용해서 해당\n변수의 모든 텍스트를 추출하고 str_c() 함수로 각 행별로 구분되었던 문자열을\ncollapse = \" \" 인자를 넣어 하나의 텍스트로 묶어 작업을 마무리 한다.아직 한글화가 되어 있지 않지만, stringr 패키지에 담긴 str_*로 시작하는 다양한 함수에 대한 사항은 String manipulation stringr cheatsheet를 참고한다.","code":"\nlibrary(dplyr)\n\nogamdo_txt <- \"13인의 아해가 도로로 질주하오.\n(길은 막다른 골목이 적당하오.)\n\n제1의 아해가 무섭다고 그리오.\n제2의 아해도 무섭다고 그리오.\n제3의 아해도 무섭다고 그리오.\n제4의 아해도 무섭다고 그리오.\n제5의 아해도 무섭다고 그리오.\n제6의 아해도 무섭다고 그리오.\n제7의 아해도 무섭다고 그리오.\n제8의 아해도 무섭다고 그리오.\n제9의 아해도 무섭다고 그리오.\n제10의 아해도 무섭다고 그리오.\n제11의 아해가 무섭다고 그리오.\n제12의 아해도 무섭다고 그리오.\n제13의 아해도 무섭다고 그리오.\n13인의 아해는 무서운 아해와 무서워하는 아해와 그렇게뿐이 모였소.(다른 사정은 없는 것이 차라리 나았소)\n\n그중에 1인의 아해가 무서운 아해라도 좋소.\n그중에 2인의 아해가 무서운 아해라도 좋소.\n그중에 2인의 아해가 무서워하는 아해라도 좋소.\n그중에 1인의 아해가 무서워하는 아해라도 좋소.\n\n(길은 뚫린 골목이라도 적당하오.)\n13인의 아해가 도로로 질주하지 아니하여도 좋소.\"\n\nogamdo_tbl <- ogamdo_txt %>% \n  str_split(pattern = \"\\n\") %>%       # 문장단위로 행을 구분\n  tibble::enframe(value = \"text\") %>% # 데이터프레임 변환\n  tidyr::unnest(text) %>%             # 행기준으로 펼치기\n  mutate(행 = row_number()) %>%       # 행번호 붙이기\n  select(행, text)\n  \nogamdo_tbl## # A tibble: 25 × 2\n##      행 text                            \n##   <int> <chr>                           \n## 1     1 \"13인의 아해가 도로로 질주하오.\"\n## 2     2 \"(길은 막다른 골목이 적당하오.)\"\n## 3     3 \"\"                              \n## 4     4 \"제1의 아해가 무섭다고 그리오.\" \n## 5     5 \"제2의 아해도 무섭다고 그리오.\" \n## 6     6 \"제3의 아해도 무섭다고 그리오.\" \n## # … with 19 more rows\n\nogamdo_tbl %>% \n  mutate(숫자여부 = str_detect(text, \"\\\\d\")) %>% \n  count(숫자여부)## # A tibble: 2 × 2\n##   숫자여부     n\n##   <lgl>    <int>\n## 1 FALSE        5\n## 2 TRUE        20\n\nogamdo_tbl %>% \n  mutate(text = str_squish(text) %>% str_trim(.)) %>% \n  pull(text) %>% \n  str_c(., collapse = \" \")## [1] \"13인의 아해가 도로로 질주하오. (길은 막다른 골목이 적당하오.)  제1의 아해가 무섭다고 그리오. 제2의 아해도 무섭다고 그리오. 제3의 아해도 무섭다고 그리오. 제4의 아해도 무섭다고 그리오. 제5의 아해도 무섭다고 그리오. 제6의 아해도 무섭다고 그리오. 제7의 아해도 무섭다고 그리오. 제8의 아해도 무섭다고 그리오. 제9의 아해도 무섭다고 그리오. 제10의 아해도 무섭다고 그리오. 제11의 아해가 무섭다고 그리오. 제12의 아해도 무섭다고 그리오. 제13의 아해도 무섭다고 그리오. 13인의 아해는 무서운 아해와 무서워하는 아해와 그렇게뿐이 모였소.(다른 사정은 없는 것이 차라리 나았소)  그중에 1인의 아해가 무서운 아해라도 좋소. 그중에 2인의 아해가 무서운 아해라도 좋소. 그중에 2인의 아해가 무서워하는 아해라도 좋소. 그중에 1인의 아해가 무서워하는 아해라도 좋소.  (길은 뚫린 골목이라도 적당하오.) 13인의 아해가 도로로 질주하지 아니하여도 좋소.\"\n"},{"path":"text-clean.html","id":"text-clean","chapter":"5 .  정제(전처리)","heading":"5 .  정제(전처리)","text":"정제는 자료정보지식지혜(DIKW)위계론의 1차부호화 단계에 해당한다. 정제를 거친 자료를 분석하는 2차부호화 과정을 거쳐 자료가 정보로 가공된다. 광물 정제과정에 비유할 수 있다. 금광석 등 광물을 캐면 먼저 잘게 분쇄한다. 불순물을 제거하고, 규격화한 금괴로 가공한다. 마찬가지로 원자료를 분석할 수 있는 단위로 분쇄(토큰화)하고, 불순물을 제거(불용어 제거)한 다음, 규격화한 양식으로 정규화한다.토큰화불용어제거정규화","code":"\npkg_l <- c(\"tidyverse\", \"tidytext\", \"textdata\")\npurrr::map(pkg_l, require, ch = T)## [[1]]\n## [1] TRUE\n## \n## [[2]]\n## [1] TRUE\n## \n## [[3]]\n## [1] TRUE\n"},{"path":"text-clean.html","id":"토큰화","chapter":"5 .  정제(전처리)","heading":"5.1 토큰화","text":"텍스트 원자료를 분석할 수 있도록 토큰(token)으로 잘게 나누는 단계다. 토큰의 단위는 분석 목적에 따라 글자, 단어, 엔그램(n-gram), 문장, 문단 등 다양하게 지정할 수 있다.","code":""},{"path":"text-clean.html","id":"unnest_tokensdf-output-input-token-words-format-ctext-man-latex-html-xml-to_lower-true-drop-true-collapse-null-...","chapter":"5 .  정제(전처리)","heading":"5.1.1 unnest_tokens(df, output, input, token = \"words\", format = c(\"text\", \"man\", \"latex\", \"html\", \"xml\"), to_lower = TRUE, drop = TRUE, collapse = NULL, ...)","text":"토큰으로 나누는 단위는 분석의 목적에 따라 다양한 단어, 글자, 문장 등 다양한 수준으로 설정할 수 있다.“characters” 글자 단위“character_shingles” 복수의 글자 단위“words” 단어 단위“ngrams” 복수의 단어 단위“regex” 정규표현식으로 지정tidytext패키지에서는 unnest_tokens()함수에서는 token =인자로 토큰 단위를 지정할 수 있다.","code":""},{"path":"text-clean.html","id":"단어","chapter":"5 .  정제(전처리)","heading":"5.1.1.0.1 단어","text":"unnest_tokens()함수에서 토큰의 기본값으로 설정된 단위는 단어(“words”)다.","code":"\ntext_v <- \"You still fascinate and inspire me.\nYou influence me for the better. \nYou’re the object of my desire, the #1 Earthly reason for my existence.\"\n\ntibble(text = text_v) %>% \n  unnest_tokens(output = word, input = text,\n                token = \"words\")## # A tibble: 25 × 1\n##   word     \n##   <chr>    \n## 1 you      \n## 2 still    \n## 3 fascinate\n## 4 and      \n## 5 inspire  \n## 6 me       \n## # … with 19 more rows\n"},{"path":"text-clean.html","id":"글자-토큰","chapter":"5 .  정제(전처리)","heading":"5.1.1.0.2 글자 토큰","text":"token =인자에 “characters”를 투입하면 글자 단위로 토큰화한다.","code":"\ntibble(text = text_v) %>% \n  unnest_tokens(output = word, input = text,\n                token = \"characters\") %>% \n  count(word, sort = T)## # A tibble: 21 × 2\n##   word      n\n##   <chr> <int>\n## 1 e        20\n## 2 t        10\n## 3 o         8\n## 4 r         8\n## 5 i         7\n## 6 n         7\n## # … with 15 more rows\n"},{"path":"text-clean.html","id":"복수의-글자","chapter":"5 .  정제(전처리)","heading":"5.1.1.0.3 복수의 글자","text":"복수의 글자를 토큰의 단위로 할 때는 “character_shingles”을 token =인자에 투입한다. 기본값은 3글자.","code":"\ntibble(text = text_v) %>% \n  unnest_tokens(output = word, input = text,\n                token = \"character_shingles\", n = 4) %>% \n  count(word, sort = T)## # A tibble: 104 × 2\n##   word      n\n##   <chr> <int>\n## 1 ence      2\n## 2 ethe      2\n## 3 reth      2\n## 4 1ear      1\n## 5 andi      1\n## 6 arth      1\n## # … with 98 more rows\n"},{"path":"text-clean.html","id":"복수의-단어n-gram","chapter":"5 .  정제(전처리)","heading":"5.1.1.0.4 복수의 단어(n-gram)","text":"복수의 단어를 토콘 단위로 나눌 때는 token =인자에 “ngrams”인자를 투입한다. 기본값은3개이다.","code":"\ntibble(text = text_v) %>% \n  unnest_tokens(output = word, input = text,\n                token = \"ngrams\", n = 4) %>% \n  count(word, sort = T)## # A tibble: 22 × 2\n##   word                         n\n##   <chr>                    <int>\n## 1 1 earthly reason for         1\n## 2 and inspire me you           1\n## 3 better you’re the object     1\n## 4 desire the 1 earthly         1\n## 5 earthly reason for my        1\n## 6 fascinate and inspire me     1\n## # … with 16 more rows\n"},{"path":"text-clean.html","id":"정규표현식","chapter":"5 .  정제(전처리)","heading":"5.1.1.0.5 정규표현식","text":"정규표현식(regex: regular expressions)을 이용하면, 토콘을 보다 다양한 방식으로 나눌 수 있다.token =인자에 “regex”를 지정한다. pattern =에 정규표현식을 투입한다.정규표현식에서 “new line”을 의미하는 \"\\n\"를 이용해 토큰화할 경우 문장 단위로 토큰화할 경우, 수 있다. 만일 공백 단위로 토큰화한다면, 공백을 의미하는 \"\\\\s\"를 투입한다.","code":"\ntibble(text = text_v) %>% \n  unnest_tokens(output = word, input = text,\n                token = \"regex\", pattern = \"\\n\")## # A tibble: 3 × 1\n##   word                                                     \n##   <chr>                                                    \n## 1 \"you still fascinate and inspire me.\"                    \n## 2 \"you influence me for the better. \"                      \n## 3 \"you’re the object of my desire, the #1 earthly reason f…\n"},{"path":"text-clean.html","id":"format-ctext-man-latex-html-xml","chapter":"5 .  정제(전처리)","heading":"5.1.2 format = c(\"text\", \"man\", \"latex\", \"html\", \"xml\")","text":"format =인자를 이용해 토큰화하는 문서의 형식을 지정할 수 있다.“html”문서를 토큰화해보자.","code":"\npp_html_df <- tibble(text = read_lines(\"https://www.gutenberg.org/files/1342/1342-h/1342-h.htm\"))\npp_html_df[1:5,]\npp_html_df %>% unnest_tokens(word, text, format = \"html\") %>% .[1:5,]"},{"path":"text-clean.html","id":"to_lower-true","chapter":"5 .  정제(전처리)","heading":"5.1.3 to_lower = TRUE","text":"영문은 대문자와 소문자 구분이 있다. to_lower =인자의 기본값은 TRUE다. FALSE를 로 지정하면 대문자를 모두 소문자로 변경하지 않는다. 영문문서에서 사람이름이나 지명을 구분해야 한다면 토큰화 과정에서 모든 단어를 소문자화하지 말아야 한다.","code":"\ntibble(text = text_v) %>% \n  unnest_tokens(output = word, input = text,\n                to_lower = FALSE)## # A tibble: 25 × 1\n##   word     \n##   <chr>    \n## 1 You      \n## 2 still    \n## 3 fascinate\n## 4 and      \n## 5 inspire  \n## 6 me       \n## # … with 19 more rows\n"},{"path":"text-clean.html","id":"strip_punct","chapter":"5 .  정제(전처리)","heading":"5.1.4 strip_punct =","text":"추가인자는 tokenizers함수로 전달해 다양한 설정을 할 수 있다. 예를 들어, strip_punct =인자에 FALSE를 투입하면, 문장부호를 제거하지 않는다.","code":"\ntibble(text = text_v) %>% \n  unnest_tokens(output = word, input = text,\n                token = \"words\",\n                strip_punct = FALSE)## # A tibble: 30 × 1\n##   word     \n##   <chr>    \n## 1 you      \n## 2 still    \n## 3 fascinate\n## 4 and      \n## 5 inspire  \n## 6 me       \n## # … with 24 more rows\n"},{"path":"text-clean.html","id":"불용어stop-words-제거","chapter":"5 .  정제(전처리)","heading":"5.2 불용어(stop words) 제거","text":"불용어는 말 그대로 사용하지 않는 단어다. 불용어를 문자 그대로 해석하면 사용하지 않는 단어에 국한된다. 넓은 의미로 해석하면, 사용빈도가 높아 분석에 의미가 없거나, 내용을 나타내는데 기여하지 않는 단어, 숫자, 특수문자, 구두점, 공백문자, 기호 등이 포함된다.무엇이 불용어가 돼야 하는지는 상황에 따라 다르다. 에를 들어, 대명사는 대부분의 불용어사전에 불용어로 포함돼 있지만, 분석 목적에 따라서는 대명사는 분석의 핵심단위가 되기도 한다. 기호도 마찬가지다. 기호를 이용한 이모티콘은 문서의 의미를 전달하기 때문에 모든 기호를 일괄적으로 제거해서는 안된다.앞서 제시한 연애편지를 문자 단위로 토큰화해 단어의 빈도를 계산해보자.count로 단어빈도를 계산한 결과를 보면 “”가 3회, “”, “”, “”, “”가 각각 2회 사용됐다. 즉, 이 글은 너와 나에 대한 글이런 것을 알수 있다. 사랑고백이란 것이 너와 나의 일이므로 타당하다.분석결과를 보면 단어빈도로 의미를 파악하는데 불필요한 단어도 있다. “”, “”, “”, “” 등과 같은 관사, 전치사, 접속사들처럼 자주 사용하는 단어들이다. 이런 단어는 불용어(stop words)로 처리해 분석대상에 제외하는 것이 보다 정확한 의미를 파악하는데 도움이 되는 경우도 있다.불용어를 제거하는 방법은 크게 두가지가 있는데 혼용한다.anti_join() 불용어목록을 데이터프레임에 저장한 다음, anti_join()함수를 이용해 텍스트데이터프레임과 배제결합하는 방법이다. 두 데이터프레임에서 겹치는 행을 제외하고 결합(join)한다. 이 경우 불용어 목록에 포함된 행이 제외된다.anti_join() 불용어목록을 데이터프레임에 저장한 다음, anti_join()함수를 이용해 텍스트데이터프레임과 배제결합하는 방법이다. 두 데이터프레임에서 겹치는 행을 제외하고 결합(join)한다. 이 경우 불용어 목록에 포함된 행이 제외된다.filter()함수와 str_detect()함수를 함께 이용해 불용어 지정해 걸러내는 방법이다. 불용어사전에 포함돼 있지 않는 단어를 제거할 때 이용한다.filter()함수와 str_detect()함수를 함께 이용해 불용어 지정해 걸러내는 방법이다. 불용어사전에 포함돼 있지 않는 단어를 제거할 때 이용한다.","code":"\ntibble(text = text_v) %>% \n  unnest_tokens(output = word, input = text) %>% \n  count(word, sort = TRUE)## # A tibble: 19 × 2\n##   word      n\n##   <chr> <int>\n## 1 the       3\n## 2 for       2\n## 3 me        2\n## 4 my        2\n## 5 you       2\n## 6 1         1\n## # … with 13 more rows\n"},{"path":"text-clean.html","id":"불용어-사전","chapter":"5 .  정제(전처리)","heading":"5.2.1 불용어 사전","text":"주로 사용되는 불용어목록은 불용어사전으로 제공된다. tidytext패키지는 stop_words에 불용어를 모아 놓았다. stop_words의 구조부터 살며보자.kableExtra패키지를 이용하면 데이터프레임을 깔끔하게 출력할 수 있다.(사용법은 여기)데이터셋을 R세션에 올리는 함수는 data()함수다.stop_words는 행이 1,149개(불용어 1,149개)이고, 열이 2개(word와 lexicon)인 데이터프레임이다. word열에 있는 단어가 불용어고, lexicon열에 있는 값은 불용어 용어집의 이름이다. tidytext패키지의 stop_words에는 세 개의 불용어 용어집(SMART, snowball, onix) 이 포함돼 있다. filter함수로 특정 용어집에 있는 불용어 사전만 골라 이용할 수 있다.불용어사전으로 불용어를 걸러낸 다음 단어빈도를 계산해보자.결과를 보면 “”등 대명사가 포함된 토큰은 모두 제거됐는데, “’re”는 그대로 남아 있다. 불용어 사전에는 “’re”로 홑따옴표(quotation mark)'를 이용했는데, 본문에는 “’re”로 홑낫표(aphostrophe)’를 이용했기 때문이다. 불용어사전으로 본문의 “’re”를 제거하기 위해서는 둘 중 한가지는 해야 한다. 본몬의 홑낫표를 홑따옴표로 변경하거나, 불용어사전을 수정한다.","code":"\ninstall.packages(\"kableExtra\")\nlibrary(kableExtra)\ndata(stop_words)\nstop_words %>% glimpse()## Rows: 1,149\n## Columns: 2\n## $ word    <chr> \"a\", \"a's\", \"able\", \"about\", \"above\", \"acc…\n## $ lexicon <chr> \"SMART\", \"SMART\", \"SMART\", \"SMART\", \"SMART…\n\nstop_words[c(1:3, 701:703, 1001:1003),] %>% \n  kbl() %>% kable_classic(full_width = F)\nstop_words$lexicon %>% unique## [1] \"SMART\"    \"snowball\" \"onix\"\n\ndata(stop_words)\n\ntibble(text = text_v) %>%\n  unnest_tokens(output = word, input = text) %>% \n  anti_join(stop_words) %>% \n  count(word, sort = TRUE)## # A tibble: 10 × 2\n##   word          n\n##   <chr>     <int>\n## 1 1             1\n## 2 desire        1\n## 3 earthly       1\n## 4 existence     1\n## 5 fascinate     1\n## 6 influence     1\n## # … with 4 more rows\n"},{"path":"text-clean.html","id":"본문-수정","chapter":"5 .  정제(전처리)","heading":"5.2.1.1 본문 수정","text":"먼저 본문 수정을 해보자.","code":"\ntapo_v <- text_v %>% str_replace_all(\"’\", \"'\")\n\ntibble(text = tapo_v) %>%\n  unnest_tokens(output = word, input = text) %>% \n  anti_join(stop_words) %>% \n  count(word, sort = TRUE)## # A tibble: 9 × 2\n##   word          n\n##   <chr>     <int>\n## 1 1             1\n## 2 desire        1\n## 3 earthly       1\n## 4 existence     1\n## 5 fascinate     1\n## 6 influence     1\n## # … with 3 more rows\n"},{"path":"text-clean.html","id":"불용어-사전-수정1","chapter":"5 .  정제(전처리)","heading":"5.2.1.2 불용어 사전 수정1","text":"불용어 사전에 “’re”를 추가해보자. 데이터프레임을 만들어 bind_rows()로 데이터프레임을 결합할수도 있고, add_row()로 불용사전에 열을 곧바로 추가할수도 있다.먼저 add_row()로 행에 곧바로 추가하는 방법을 이용해보자. 추가됐는지 확인이 수월하도록 첫째행 전에 추가하자.이번에는 데이터프레임을 결합해보자. 또한 숫자 “1”도 함께 불용어사전에 추가하자.\n먼저 추가할 용어를 불용어사전과 같은 구조의 데이터프레임에 저장한다.bind_rows()함수로 불용어사전과 결합한다.새로 만든 불용어사전으로 정체한 후 단어 빈도를 계산해보자.“’re”와 숫자가 모두 제거됐다.","code":"\nstop_words %>% add_row(word = \"you’re\", lexicon = \"NEW\", .before = 1) %>% head(3)## # A tibble: 3 × 2\n##   word   lexicon\n##   <chr>  <chr>  \n## 1 you’re NEW    \n## 2 a      SMART  \n## 3 a's    SMART\n\nnames(stop_words)## [1] \"word\"    \"lexicon\"\n\nstop_add <- tibble(word = c(\"you’re\", \"1\"),\n                   lexicon = \"added\")\nstop_add## # A tibble: 2 × 2\n##   word   lexicon\n##   <chr>  <chr>  \n## 1 you’re added  \n## 2 1      added\n\nstop_words2 <- bind_rows(stop_words, stop_add)\nstop_words2 %>% tail()## # A tibble: 6 × 2\n##   word     lexicon\n##   <chr>    <chr>  \n## 1 younger  onix   \n## 2 youngest onix   \n## 3 your     onix   \n## 4 yours    onix   \n## 5 you’re   added  \n## 6 1        added\n\ntibble(text = text_v) %>%\n  unnest_tokens(output = word, input = text) %>% \n  anti_join(stop_words2) %>% \n  count(word, sort = TRUE)## # A tibble: 8 × 2\n##   word          n\n##   <chr>     <int>\n## 1 desire        1\n## 2 earthly       1\n## 3 existence     1\n## 4 fascinate     1\n## 5 influence     1\n## 6 inspire       1\n## # … with 2 more rows\n"},{"path":"text-clean.html","id":"불용어-사전-수정2","chapter":"5 .  정제(전처리)","heading":"5.2.1.3 불용어 사전 수정2","text":"통상적으로 쓰이는 불용어 중에는 실은 문서의 의미를 파악하는데 중요한 단서를 제공하는 단어도 있다. “” “” “” 등과 같은 대명사는 흔하게 사용되기 때문에 불용어로 분류되지만, 맥락를 파악하는데 중요한 역할을 하기도 한다. 불용어 사전에서 대명사를 찾아 불용어 사전에서 제거하자.","code":"\nstop_words$word %>% \n  str_subset(\"(^i$|^i[:punct:]+|^mys*|^me$|mine)\")##  [1] \"i\"      \"i'd\"    \"i'll\"   \"i'm\"    \"i've\"   \"me\"    \n##  [7] \"my\"     \"myself\" \"i\"      \"me\"     \"my\"     \"myself\"\n## [13] \"i'm\"    \"i've\"   \"i'd\"    \"i'll\"   \"i\"      \"me\"    \n## [19] \"my\"     \"myself\"\n\nstop_words3 <- stop_words %>% \n  filter(\n    !str_detect(word, \"(^i$|^i[:punct:]+|^mys*|^me$|^mine$)\"),\n    )\nstop_words3$word %>% \n  str_subset(\"^i\")##  [1] \"ie\"          \"if\"          \"ignored\"     \"immediate\"  \n##  [5] \"in\"          \"inasmuch\"    \"inc\"         \"indeed\"     \n##  [9] \"indicate\"    \"indicated\"   \"indicates\"   \"inner\"      \n## [13] \"insofar\"     \"instead\"     \"into\"        \"inward\"     \n## [17] \"is\"          \"isn't\"       \"it\"          \"it'd\"       \n## [21] \"it'll\"       \"it's\"        \"its\"         \"itself\"     \n## [25] \"it\"          \"its\"         \"itself\"      \"is\"         \n## [29] \"it's\"        \"isn't\"       \"if\"          \"into\"       \n## [33] \"in\"          \"if\"          \"important\"   \"in\"         \n## [37] \"interest\"    \"interested\"  \"interesting\" \"interests\"  \n## [41] \"into\"        \"is\"          \"it\"          \"its\"        \n## [45] \"itself\"\n"},{"path":"text-clean.html","id":"불용어-목록-만들기","chapter":"5 .  정제(전처리)","heading":"5.2.1.4 불용어 목록 만들기","text":"제거하고 싶은 불용어를 최소화하고 싶을 때는 불용어 목록을 직접 만들수도 있다. “, , ”등이 포함된 불용어 목록을 만들어 정제해 보자. “, , ”등 불용어목록을 데이터프레임에 저장한 다음, anti_join()함수를 이용해 토큰데이터프레임과 배제결합한다.","code":"\nstop_df <- tibble(word = c(\"the\",\"for\", \"and\"))\n\ntibble(text = text_v) %>% \n  unnest_tokens(output = word, input = text) %>% \n  anti_join(stop_df) %>% \n  filter(!str_detect(word, \"\\\\d+\")) %>% \n  count(word, sort = TRUE)## # A tibble: 15 × 2\n##   word        n\n##   <chr>   <int>\n## 1 me          2\n## 2 my          2\n## 3 you         2\n## 4 better      1\n## 5 desire      1\n## 6 earthly     1\n## # … with 9 more rows\n"},{"path":"text-clean.html","id":"filter","chapter":"5 .  정제(전처리)","heading":"5.2.2 filter()","text":"filter()함수를 이용해 불용어사전을 수정하지 않고 불용어를 추가로 제거할 수 있다.예를 들어, 숫자를 불용어로 취급해 제거하는 상황에서 숫자를 불용어 사전에 넣지 말고 filter()로 걸러보자. 정규표현식(regex: regular expression)에서 숫자를 의미하는 [:digit:] 또는 \\\\d를 이용해 filter()함수와 str_detect()함수 및 부정연산자 !를 이용해 걸러낸다.filter()함수를 str_detect()함수와 함께 이용하는 이유는 다음과 같다.str_subset()함수는 패턴이 일치하는 문자를 출력하는 반면, str_detect()함수는 패턴이 일치하는 문자에 대한 논리값(TRUE FALSE)을 출력한다.불용어를 제거한 다음 추가로 본문에서 숫자와 홑낫표”’“가 포함된 문제를 제거하자.","code":"\ndf <- tibble(text = text_v) %>%\n  unnest_tokens(output = word, input = text) %>% \n  anti_join(stop_words)\ndf$word %>% str_subset(pattern = \"\\\\d+\")## [1] \"1\"\n\ndf$word %>% str_detect(pattern = \"\\\\d+\")##  [1] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n## [10] FALSE\n\ntibble(text = text_v) %>%\n  unnest_tokens(output = word, input = text) %>% \n  anti_join(stop_words) %>% \n  filter(\n    !str_detect(word, pattern = \"\\\\d+\"),\n    !str_detect(word, pattern = \"you’re\")\n    ) %>% \n  count(word, sort = TRUE)## # A tibble: 8 × 2\n##   word          n\n##   <chr>     <int>\n## 1 desire        1\n## 2 earthly       1\n## 3 existence     1\n## 4 fascinate     1\n## 5 influence     1\n## 6 inspire       1\n## # … with 2 more rows\n"},{"path":"text-clean.html","id":"정규화normalization","chapter":"5 .  정제(전처리)","heading":"5.3 정규화(Normalization)","text":"","code":""},{"path":"text-clean.html","id":"개괄","chapter":"5 .  정제(전처리)","heading":"5.3.1 개괄","text":"정규화는 추출한 단어를 일정한 틀로 규격화하는 작업이다. 한 단어는 문법적인 기능에 따라 다양한 표현이 있다. ‘먹다’에는 ’먹었니’ ‘먹었다’ ‘먹고’ 등의 표현이 있다. ‘’는 격에 따라 ’’ ‘’ ‘mine’ 등의 변형이 있다. 다양한 표현이 같은 의미를 나타낸다면 정규화를 통해 일정한 틀로 규격화해야 한다.정규화는 형태소(morpheme) 추출, 어간(stem) 추출, 표제어(lemme) 추출 등을 이용해 달성할 수 있다.단어(word): 형태소의 집합. 자립이 가능한 최소 형태(예: 사과나무)형태소(morpheme): 뜻을 지닌 가장 작은 말의 단위. 예를 들어, ’사과나무’는 ’사과’와 ’나무’로 나눠도 뜻을 지니지만, ’사과’를 ’사’와 ’과’로 나누면 의미가 사라진다.어기(base): 어근과 어간 등 단어에서 실질적인 의미를 나타내는 형태소\n어근(root) 어미와 직접결합이 안되는 어기. 예: ‘급하다’의 ’급’ ‘시원하다’의 ’시원’.\n어간(stem) 어미와 직접결합이 되는 어기. 예: ‘뛰어라’의 ’뛰-’. ‘먹다’의 ’먹-’.\n어근(root) 어미와 직접결합이 안되는 어기. 예: ‘급하다’의 ’급’ ‘시원하다’의 ’시원’.어간(stem) 어미와 직접결합이 되는 어기. 예: ‘뛰어라’의 ’뛰-’. ‘먹다’의 ’먹-’.표제어(lemme) 사전에 등재된 대표단어. 원형 혹은 기본형(canonical form)이라고도 한다.","code":""},{"path":"text-clean.html","id":"형태소morpheme","chapter":"5 .  정제(전처리)","heading":"5.3.2 형태소(morpheme)","text":"형태소는 뜻을 지난 가장 작은 말의 단위다.\n‘바지가 크다’는 문장에서 단어는 ’바지’ ‘가’ ‘크다’가 있다. 형태소는 ’바지’ ‘가’ ‘크’ ‘다’로 구분할 수 있다. 명사인 ’바지’를 ’바’와 ’지’로 나누면 ’아랫도리에 입는 옷’이란 의미가 사라진다. 반면 형용사인 ’크다’에는 어간인 ’크-’에 ’크다’는 의미가 담겨 있고,’-다’에는 문장을 마무리하는 의미가 담겨 있다.","code":""},{"path":"text-clean.html","id":"품사태깅parts-of-speech-tagging-pos-tagging","chapter":"5 .  정제(전처리)","heading":"5.3.2.0.1 품사태깅(Parts of Speech Tagging: PoS Tagging)","text":"형태소를 추출하기 위해서 문장의 단어에 품사를 붙인다(tag).형태소 분석기마다 품사태깅의 방법이 조금씩 다르다. 한나눔(Hannaum)은 크게 9개 품사로 분류한뒤 22개로 세부 분류했다. Mecab-ko는 43개로 분류했다.한국어 품사 태그 비교표한나눔과 MeCab-ko의 품사태그 (Table 5.1).\nTable 5.1: 한나눔과 MeCab-ko의 한국어 품사 태그 비교\n","code":""},{"path":"text-clean.html","id":"형태소-추출","chapter":"5 .  정제(전처리)","heading":"5.3.3 형태소 추출","text":"정규화는 형태소를 추출해 달성할 수 있다. R의 대표적인 형태소 분석기로는 RcppMeCab와 KoNLP가 있다.","code":""},{"path":"text-clean.html","id":"rcppmecab","chapter":"5 .  정제(전처리)","heading":"5.3.3.1 RcppMeCab","text":"한국어뿐 아니라 일본어와 중국어 형태소 분석도 가능하고 실행속도가 빠르다.\n일본교토대학정보학연구대학원과 일본전신전화의 커뮤니케이션기본과학연구소가 공동으로 개발한 오픈소스 형태소 분석기 MeCab 기반이다. 은전 프로젝트로 한국어 형태소를 분석할수 있게 개발했다. RcppMeCab패키지는 MeCab을 R에서 사용할 수 있도록 한 패키지다. 일본어 기반이라 띄어쓰기에 덜 민감하다.","code":""},{"path":"text-clean.html","id":"konlp","chapter":"5 .  정제(전처리)","heading":"5.3.3.2 KoNLP","text":"Java 기반의 한나눔(Hannanum) 분석기 기반이다. 널리 사용되는 형태소분석기다. NIADic, Sejong등 사전을 선택할 수 있다. 사용설명서가 있고, 사용자 사전을 수정하기 용이하다.","code":""},{"path":"text-clean.html","id":"rcppmecab-1","chapter":"5 .  정제(전처리)","heading":"5.3.4 RcppMeCab","text":"RcppMeCab패키지를 설치하고 실행하자. 설치되는 기본폴더는 C:\\mecab다. 설치폴더를 변경하지 않는다.주의C드라이브에 C:\\mecab폴더가 생성됐는지 확인한다. 사전파일이 이곳에 있다. 만일 C:\\mecab폴더가 생성되지 않았다면 설치가 안된것이다. RStudio를 관리자권한으로 실행해 설치한다.만일, RStudio를 관리자권한으로 실행해 설치해도 C:\\mecab가 생성되지 않는경우, 이 링크에서 사전파일을 다운로드 받아 파일을 압축해제해 복사한다. C:\\mecab가 생성되고, 이 폴더 바로 아래에 libmecab.dll파일과 mecab-ko-dic폴더가 생성돼 있어야 한다.기본함수는 pos()다. 문자벡터를 받아 리스트로 산출한다.한글이 깨지는 경우가 있는데, 이는 한글인코딩 방식이 맞지 않기 때문이다. 윈도는 EUC-KR방식을 확장한 CP949방식을 사용하기 때문에 UTF-8방식과 호환이 안된다. 이 경우 enc2utf8함수를 이용해 한글인코딩 방식을 UTF-8으로 변경한다.참고: UTF-8을 CP949로 인코딩을 바꾸고 싶으면 iconv함수를 이용한다.x는 문자벡터. 자세한 사용법은 ?iconv 참조.","code":"\ninstall.packages('RcppMeCab')\nlibrary(RcppMeCab)\ntest <- \"한글 테스트 입니다.\"\npos(test)## $`한글 테스트 입니다.`\n## [1] \"한글/NNG\"      \"테스트/NNG\"    \"입니다/VCP+EF\"\n## [4] \"./SF\"\n\nlibrary(tidyverse)\ntest_v <- enc2utf8(test)\ntest_v %>% pos## $`한글 테스트 입니다.`\n## [1] \"한글/NNG\"      \"테스트/NNG\"    \"입니다/VCP+EF\"\n## [4] \"./SF\"\niconv(x, from = \"UTF-8\", to = \"CP949\")`"},{"path":"text-clean.html","id":"join-false","chapter":"5 .  정제(전처리)","heading":"5.3.4.1 join = FALSE","text":"join = FALSE인자를 이용하면 품사태그를 제외하고 형태소만 산출한다.","code":"\ntest_v %>% pos(join = FALSE)## $`한글 테스트 입니다.`\n##      NNG      NNG   VCP+EF       SF \n##   \"한글\" \"테스트\" \"입니다\"      \".\"\n"},{"path":"text-clean.html","id":"format-data.frame","chapter":"5 .  정제(전처리)","heading":"5.3.4.2 format = \"data.frame\"","text":"format = \"data.frame\"을 지정하면 데이터프레임으로 산출한다.","code":"\ntest_v %>% pos(format = \"data.frame\")##   doc_id sentence_id token_id  token    pos subtype\n## 1      1           1        1   한글    NNG        \n## 2      1           1        2 테스트    NNG    행위\n## 3      1           1        3 입니다 VCP+EF        \n## 4      1           1        4      .     SF\n"},{"path":"text-clean.html","id":"posparallelx","chapter":"5 .  정제(전처리)","heading":"5.3.4.3 posParallel(x)","text":"posParallel()함수는 메모리를 많이 사용하지만 처리속도가 빠르다.","code":"\ntest_v %>% posParallel(format = \"data.frame\")##   doc_id sentence_id token_id  token    pos subtype\n## 1      1           1        1   한글    NNG        \n## 2      1           1        2 테스트    NNG    행위\n## 3      1           1        3 입니다 VCP+EF        \n## 4      1           1        4      .     SF\n"},{"path":"text-clean.html","id":"konlp-1","chapter":"5 .  정제(전처리)","heading":"5.3.5 KoNLP","text":"형태소 분석기 한나눔(Hannanum)을 R에서 사용할 수 있도록한 패키지다. CRAN에서 내려져 있기 때문에 개발자 깃헙에서 remote패키지의 install_github()함수를 이용해 설치한다.KoNLP를 설치하기 위해서는 자바와 rtools가 필요하다.","code":""},{"path":"text-clean.html","id":"자바jdk","chapter":"5 .  정제(전처리)","heading":"5.3.5.1 자바JDK","text":"자바JDK를 다운로드받아 설치한다.jdk설치여부는 C:\\Program Files\\Java\\jdk-16폴더에서 확인할 수 있다. Java SE 16보다 최신 버전이 있으면 최신버전으로 새로 설치한다.윈도 시스템 속성창의 환경변수에서 JAVA_HOME을 설정한다.시스템 속성창 여는 방법간단한 방법:윈도 실행창 단축키인 윈도키와 키보드의 R을 함께 누르면 윈도 실행창이 열린다. 실행창이 뜨면 sysdm.cpl ,3을 입력하고 엔터키를 누르면 시스템 속성 창이 열린다.복잡한 방법 (윈도10 기준):윈도탐색기에서 내 PC에 마우스 커서를 놓고 마우스 오른쪽 버튼 클릭 -> 속성(R) 클릭해 윈도설정에서 정보창이 열린다.윈도탐색기에서 내 PC에 마우스 커서를 놓고 마우스 오른쪽 버튼 클릭 -> 속성(R) 클릭해 윈도설정에서 정보창이 열린다.윈도설정에서 정보창이 열리면 관련설정 항목에서 고급 시스템 설정 클릭하면 시스템 속성 창의 고급 탭이 열린다.윈도설정에서 정보창이 열리면 관련설정 항목에서 고급 시스템 설정 클릭하면 시스템 속성 창의 고급 탭이 열린다.환경변수 설정하기시스템 속성창 하단의 환경설정버튼을 클릭한다. 환경변수창이 열리면 시스템변수(S)아래의 새로만들기(W)버튼을 클릭한다.시스템 속성창 하단의 환경설정버튼을 클릭한다. 환경변수창이 열리면 시스템변수(S)아래의 새로만들기(W)버튼을 클릭한다.새 시스템 변수 창이 열리면 변수 이름(N):에 JAVA_HOME을 기입하고, 변수 값(V)에서 JDK설치 경로 C:\\Program Files\\Java\\jdk-16\\를 기입하고 확인을 클릭한다.새 시스템 변수 창이 열리면 변수 이름(N):에 JAVA_HOME을 기입하고, 변수 값(V)에서 JDK설치 경로 C:\\Program Files\\Java\\jdk-16\\를 기입하고 확인을 클릭한다.","code":""},{"path":"text-clean.html","id":"rtools","chapter":"5 .  정제(전처리)","heading":"5.3.5.2 rtools","text":"CRAN의 rtools페이지에 접속한다.CRAN의 rtools페이지에 접속한다.설치된 R버전과 일치하는 버전의 rtools를 다운로드받는다. R이 4.0대 버전이면 rtools40을 설치한다.설치된 R버전과 일치하는 버전의 rtools를 다운로드받는다. R이 4.0대 버전이면 rtools40을 설치한다.rtools는 다른 패키지와 달리 install.packages()함수로 설치하지 않고, 설치파일을 컴퓨터에서 직접 실행해 설치한다.설치할때 경로변경하지 말고 C:/Rtools에 설치한다. 실치과정에서 Add rtools system PATH가 체크돼 있는지 확인한다.기타 KoNLP 설치이슈에 대해서는 이 문서 참조","code":""},{"path":"text-clean.html","id":"의존패키지","chapter":"5 .  정제(전처리)","heading":"5.3.5.3 의존패키지","text":"설치할 때 오류가 나는 경우가 있다. 의존패키지(패키지작동에 필요한 다른 패키지)가 설치돼 있지 않기 때문이다. 다음은 KoNLP의존 패키지다.rJava (>= 0.9-8),utils (>= 3.3.1),stringr (>= 1.1.0),hash (>= 2.2.6),tau (>= 0.0-18),Sejong (>= 0.01),RSQLite (>= 1.0.0),devtools (>= 1.12.0)아래 코드로 필요한 패키지가 이미 설치돼 있는지 확인할 수 있다.아래 코드로 미설치된 패키지를 한번에 설치할 수 있다. 만일 설치가 안되면 이 교재 2.4 오류를 참고해 개별적으로 설치한다.의존성 패키지가 의존하는 패키지가 있다. 만일 오류메시지가 나오면 메시지를 잘 읽어보고, 필요한 패키지를 추가로 설치한다.KoNLP설치 준비가 됐으면 아래 코드로 설치한다.제대로 설치됐는지 확인해보자.KoNLP에서 사용할 사전을 설치하자. NIADic이 SejongDic보다 더 많은 형태소를 포함하고 있다. 설치과정에서 기 설치된 패키지 업데이트 여부를 묻는다. 모두 최신버전으로 업데이트한다.한글띄어쓰기가 안돼 있는 문서는 autoSpacing = T인자를 투입한다.띄어쓰기 안된 문서의 행태소 분석에는 MeCab이 유리하다.KoNLP를 이용해 띄어쓰기가 잘 안된 문서를 분석할 때는 KoSpacing패키지로 띄어쓰기를 조절할 수 있다(설치과정이 복잡하므로 선택 사항.)KoNLP의 장점은 기존 사전에 사용자사전을 추가할 수 있다는데 있다.","code":"\npackage_list <- c(\"rJava\", \"utils\", \"stringr\", \"hash\", \"remote\",\n                  \"tau\", \"Sejong\", \"RSQLite\", \"devtools\")\n( package_list_installed <- package_list %in% installed.packages()[,\"Package\"] )## [1] FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE\n\n( new_pkg <- package_list[!package_list_installed] )## [1] \"rJava\"  \"hash\"   \"remote\" \"tau\"    \"Sejong\"\n\nif(length(new_pkg)) install.packages(new_pkg)\nremotes::install_github('haven-jeon/KoNLP', \n                        upgrade = \"never\", \n                        INSTALL_opts=c(\"--no-multiarch\"))\n# library(KoNLP)\n# extractNoun(\"한글테스트입니다.\")\n# SimplePos09(\"한글테스트입니다.\")\nuseNIADic()\n\"아버지가가방에들어가신다\" %>% SimplePos09()\n\"아버지가가방에들어가신다\" %>% SimplePos09(autoSpacing = T)\n\"한글테스트입니다\" %>% SimplePos09(autoSpacing = T)\n\"한글테스트입니다\" %>% enc2utf8() %>% RcppMeCab::pos() \n\"아버지가가방에들어가신다\" %>% enc2utf8() %>% RcppMeCab::pos() \n# remotes::install_github(\"haven-jeon/KoSpacing\")\n# library(KoSpacing)\n# set_env()\n# \n# spacing(\"한글테스트입니다.\")\n\"힣탈로미를 어떻게 할까요\" %>% SimplePos09\nbuildDictionary(ext_dic = c('sejong', 'woorimalsam'),\n                user_dic = data.frame(term=\"전작권\", tag='ncn'),\n                category_dic_nms=c('political'))\n\nbuildDictionary(ext_dic = \"woorimalsam\", \n                user_dic=data.frame(\"힣탈로미\", \"ncn\"),\n                replace_usr_dic = T)"},{"path":"text-clean.html","id":"konlpy","chapter":"5 .  정제(전처리)","heading":"5.3.6 KoNLPy","text":"형태소 분석기로는 한나눔(Hannanum)과 MeCab외 꼬꼬마(Kkma), 코모란(Komoran), Okt 등의 형태소는 파이썬 패키지인 KoNLPy로 추출가능하다.구글 colab에서 파이썬을 구동하면 KoNLPy패키지를 설치해 다양한 패키지로 형태소를 분석할 수 있다.","code":""},{"path":"text-clean.html","id":"연습-1","chapter":"5 .  정제(전처리)","heading":"5.3.7 연습","text":"이상의 오감도를 KoNLP와 RcppMeCab을 이용해 각각 형태소분석해 자주 사용된 단어의 빈도를 비교하자. 이 결과를 형태소를 추출하지 않은 결과와도 비교하자.","code":"\nogamdo_txt <- \"13인의 아해가 도로로 질주하오.\n(길은 막다른 골목이 적당하오.)\n\n제1의 아해가 무섭다고 그리오.\n제2의 아해도 무섭다고 그리오.\n제3의 아해도 무섭다고 그리오.\n제4의 아해도 무섭다고 그리오.\n제5의 아해도 무섭다고 그리오.\n제6의 아해도 무섭다고 그리오.\n제7의 아해도 무섭다고 그리오.\n제8의 아해도 무섭다고 그리오.\n제9의 아해도 무섭다고 그리오.\n제10의 아해도 무섭다고 그리오.\n제11의 아해가 무섭다고 그리오.\n제12의 아해도 무섭다고 그리오.\n제13의 아해도 무섭다고 그리오.\n13인의 아해는 무서운 아해와 무서워하는 아해와 그렇게뿐이 모였소.(다른 사정은 없는 것이 차라리 나았소)\n\n그중에 1인의 아해가 무서운 아해라도 좋소.\n그중에 2인의 아해가 무서운 아해라도 좋소.\n그중에 2인의 아해가 무서워하는 아해라도 좋소.\n그중에 1인의 아해가 무서워하는 아해라도 좋소.\n\n(길은 뚫린 골목이라도 적당하오.)\n13인의 아해가 도로로 질주하지 아니하여도 좋소.\""},{"path":"text-clean.html","id":"konlp-2","chapter":"5 .  정제(전처리)","heading":"5.3.7.1 KoNLP","text":"KoNLP의 SimplePos09()함수를 unnset_tokens()의 token =인자로 투입하면 오류가 발생한다.문자벡터에서 형태소 추출해 데이터프레임으로 저장한다.KoNLP의 extractNoun()함수 이용","code":"\nogamdo_txt %>% tibble(text = .) %>% \n  unnest_tokens(word, text, token = SimplePos09)\nogamdo_txt %>% SimplePos09() %>% flatten_dfc() %>% \n  pivot_longer(everything(), names_to = \"header\", values_to = \"value\") %>% \n  separate_rows(value, sep = \"\\\\+\") %>% \n  separate(value, into = c(\"word\", \"pos\"), sep = \"/\") %>% \n  count(word, sort = T) %>% \n  filter(str_length(word) > 1) %>% \n  slice_max(n, n = 10) %>% \n  mutate(word = reorder(word, n)) %>% \n  ggplot() + geom_col(aes(word, n)) +\n  coord_flip()\nogamdo_txt %>% extractNoun() %>% tibble(text = .)"},{"path":"text-clean.html","id":"rcppmecab-2","chapter":"5 .  정제(전처리)","heading":"5.3.7.2 RcppMeCab","text":"RcppMeCab의 pos()함수 이용RcppMeCab의 pos()함수는 unnest_tokens()의 token =인자에 투입해도 된다.","code":"\nenc2utf8(ogamdo_txt) %>% pos(format = \"data.frame\") %>% \n  select(token:pos) %>% \n  count(token, sort = T) %>% \n  filter(str_length(token) >1) %>% \n  slice_max(n, n = 10) %>% \n  mutate(token = reorder(token, n)) %>% \n  ggplot + geom_col(aes(token, n)) +\n  coord_flip()\nogamdo_txt %>% enc2utf8 %>% tibble(text = .) %>% \n  unnest_tokens(word, text, token = pos) %>% \n  separate(col = word, \n           into = c(\"word\", \"morph\"), \n           sep = \"/\" ) %>% \n  count(word, sort = T) %>% \n  filter(str_length(word) > 1) %>% \n  slice_max(n, n = 10) %>% \n  mutate(word = reorder(word, n)) %>% \n  ggplot() + geom_col(aes(word, n)) +\n  coord_flip()"},{"path":"text-clean.html","id":"형태소-미추출","chapter":"5 .  정제(전처리)","heading":"5.3.7.3 형태소 미추출","text":"","code":"\nogamdo_txt %>% tibble(text = .) %>% \n  unnest_tokens(word, text) %>% \n  count(word, sort = T) %>% \n  filter(str_length(word) > 1) %>% \n  slice_max(n, n = 10) %>% \n  mutate(word = reorder(word, n)) %>% \n  ggplot() + geom_col(aes(word, n)) +\n  coord_flip()"},{"path":"text-clean.html","id":"비교","chapter":"5 .  정제(전처리)","heading":"5.3.7.4 비교","text":"먼저 데이터프레임으로 결합한 후 행과 열 확인.행의 수와 열의 이름이 같으므로 세 데이터프레임을 행방향 결합할 수 있다. 결합한 데이터프레임으로 막대도표로 시각화한다.세 경우에 대해 ID열로 값이 부여돼 있으므로 facet_wrap()함수로 구분할 수 있다.라벨을 세개의 도표에 분리해 표시하자. scales =인자를 “free”로 지정한다. 기본값은 “fixed”다.","code":"\nKoNLP_df <- ogamdo_txt %>% SimplePos09() %>% flatten_dfc() %>% \n  pivot_longer(everything(), names_to = \"header\", values_to = \"value\") %>% \n  separate_rows(value, sep = \"\\\\+\") %>% \n  separate(value, into = c(\"word\", \"pos\"), sep = \"/\") %>% \n  count(word, sort = T) %>% \n  filter(str_length(word) > 1) %>% \n  slice_max(n, n = 10)\n\nMeCab_df <- ogamdo_txt %>% enc2utf8 %>% tibble(text = .) %>% \n  unnest_tokens(word, text, token = pos) %>% \n  separate(col = word, \n           into = c(\"word\", \"morph\"), \n           sep = \"/\" ) %>% \n  count(word, sort = T) %>% \n  filter(str_length(word) > 1) %>% \n  slice_max(n, n = 10)\n\nword_df <- ogamdo_txt %>% tibble(text = .) %>% \n  unnest_tokens(word, text) %>% \n  count(word, sort = T) %>% \n  filter(str_length(word) > 1) %>% \n  slice_max(n, n = 10)\n\nKoNLP_df %>% glimpse()\nMeCab_df %>% glimpse()\nword_df %>% glimpse()\ndf <- bind_rows(KoNLP = KoNLP_df, MeCab = MeCab_df, word = word_df, .id = \"ID\")\n\ndf %>% mutate(word = reorder(word, n)) %>% \n  ggplot() + geom_col(aes(word, n, fill = ID)) + \n  coord_flip()\ndf %>% mutate(word = reorder(word, n)) %>% \n  ggplot() + geom_col(aes(word, n, fill = ID), show.legend = F) + \n  coord_flip() +\n  facet_wrap(~ID)\ndf %>% mutate(word = reorder(word, n)) %>% \n  ggplot() + geom_col(aes(word, n, fill = ID), show.legend = F) + \n  coord_flip() +\n  facet_wrap(~ID, scales = \"free\") +\n  ggtitle(\"형태소분석기 비교\") +\n  xlab(\"단어\") + ylab(\"빈도\") +\n  theme(plot.title = element_text(size = 24, hjust = 0.5),\n        axis.title.x = element_text(size = 18),\n        axis.title.y = element_text(size = 18))"},{"path":"text-clean.html","id":"어간stem-추출","chapter":"5 .  정제(전처리)","heading":"5.3.8 어간(stem) 추출","text":"단어(word): 형태소의 집합. 자립이 가능한 최소 형태(예: 사과나무)단어(word): 형태소의 집합. 자립이 가능한 최소 형태(예: 사과나무)형태소(morpheme): 뜻을 지닌 가장 작은 말의 단위. 예를 들어, ’사과나무’는 ’사과’와 ’나무’로 나눠도 뜻을 지니지만, ’사과’를 ’사’와 ’과’로 나누면 의미가 사라진다.형태소(morpheme): 뜻을 지닌 가장 작은 말의 단위. 예를 들어, ’사과나무’는 ’사과’와 ’나무’로 나눠도 뜻을 지니지만, ’사과’를 ’사’와 ’과’로 나누면 의미가 사라진다.어기(base): 어근과 어간 등 단어에서 실질적인 의미를 나타내는 형태소\n어근(root) 어미와 직접결합이 안되는 어기. 예: ‘급하다’의 ’급’ ‘시원하다’의 ’시원.’\n어간(stem) 어미와 직접결합이 되는 어기. 예: ‘뛰어라’의 ’뛰-.’ ‘먹다’의 ’먹-.’\n어기(base): 어근과 어간 등 단어에서 실질적인 의미를 나타내는 형태소어근(root) 어미와 직접결합이 안되는 어기. 예: ‘급하다’의 ’급’ ‘시원하다’의 ’시원.’어근(root) 어미와 직접결합이 안되는 어기. 예: ‘급하다’의 ’급’ ‘시원하다’의 ’시원.’어간(stem) 어미와 직접결합이 되는 어기. 예: ‘뛰어라’의 ’뛰-.’ ‘먹다’의 ’먹-.’어간(stem) 어미와 직접결합이 되는 어기. 예: ‘뛰어라’의 ’뛰-.’ ‘먹다’의 ’먹-.’표제어(lemme) 사전에 등재된 대표단어. 원형 혹은 기본형(canonical form)이라고도 한다.표제어(lemme) 사전에 등재된 대표단어. 원형 혹은 기본형(canonical form)이라고도 한다.어간추출과 표제어 추출에 대한 설명 참고 문헌어간추출 패키지SnowballC어간추출로 널리 사용되는 알고리듬인 포터 알고리듬 스테밍을 적용.hunspell포터알고리즘에 사전 방식 결합SnowballC와 hunspell은 tidytext 등 텍스트분석 패키지와 패키지로 함께 설치되나 함께 부착되지는 않는다.hunspell은 리스트로 산출하므로 unnest()함수로 리스트구조를 풀어준다. unnest()는 flatten_()계열과 달리 데이터프레임을 입력값으로 받는다.unnest()로 리스트를 풀면 토큰의 수가 늘어난다. hunspell_stem()함수가 스테밍 전후의 단어를 모두 산출하기 때문이다. hunspell로 어간추출할때는 주의해야 한다. hunspell패키지의 목적이 텍스트분석이 아니라 철자확인이다.","code":"\nlove_v <- c(\"love\", \"loves\", \"loved\",\"love's\" ,\"lovely\", \n            \"loving\", \"lovingly\", \"lover\", \"lovers\", \"lovers'\", \"go\", \"went\") \n\nSnowballC::wordStem(love_v)##  [1] \"love\"     \"love\"     \"love\"     \"love'\"    \"love\"    \n##  [6] \"love\"     \"lovingli\" \"lover\"    \"lover\"    \"lovers'\" \n## [11] \"go\"       \"went\"\n\nhunspell::hunspell_stem(love_v)## [[1]]\n## [1] \"love\"\n## \n## [[2]]\n## [1] \"love\"\n## \n## [[3]]\n## [1] \"loved\" \"love\" \n## \n## [[4]]\n## [1] \"love\"\n## \n## [[5]]\n## [1] \"lovely\" \"love\"  \n## \n## [[6]]\n## [1] \"loving\" \"love\"  \n## \n## [[7]]\n## [1] \"loving\"\n## \n## [[8]]\n## [1] \"lover\" \"love\" \n## \n## [[9]]\n## [1] \"love\"\n## \n## [[10]]\n## character(0)\n## \n## [[11]]\n## [1] \"go\"\n## \n## [[12]]\n## [1] \"went\"\n\nlibrary(hunspell)\nlove_v %>% tibble(text = .) %>% unnest_tokens(word, text) %>% \n  mutate(hunspell = hunspell_stem(word)) ## # A tibble: 12 × 2\n##   word   hunspell \n##   <chr>  <list>   \n## 1 love   <chr [1]>\n## 2 loves  <chr [1]>\n## 3 loved  <chr [2]>\n## 4 love's <chr [1]>\n## 5 lovely <chr [2]>\n## 6 loving <chr [2]>\n## # … with 6 more rows\n\nlibrary(SnowballC)\nlove_v %>% tibble(text = .) %>% unnest_tokens(word, text) %>% \n  mutate(SnowballC = wordStem(word)) %>% \n  mutate(hunspell = hunspell_stem(word)) %>% \n  unnest(hunspell)## # A tibble: 16 × 3\n##   word   SnowballC hunspell\n##   <chr>  <chr>     <chr>   \n## 1 love   love      love    \n## 2 loves  love      love    \n## 3 loved  love      loved   \n## 4 loved  love      love    \n## 5 love's love'     love    \n## 6 lovely love      lovely  \n## # … with 10 more rows\n"},{"path":"text-clean.html","id":"표제어lemme-추출","chapter":"5 .  정제(전처리)","heading":"5.3.9 표제어(lemme) 추출","text":"","code":""},{"path":"text-clean.html","id":"어간stem과-표제어lemme의-차이","chapter":"5 .  정제(전처리)","heading":"5.3.9.1 어간(stem)과 표제어(lemme)의 차이","text":"어근은 단어의 일부로서 변하지 않는다. 예를 들어, “produced” “producing” “production”의 표제어는 “produce”이고 어근은 “produc-”다.“”와 “” 그리고, “”와 “’re”는 형태는 다르지만, 같은 의미를 공유하하고 있다. 각각 같은 의미이므로 하나로 묶어 줄 필요가 있지만, 어근추출로는 그 목적을 달성할 수 없다. 형태가 달라 어근추출처럼 규칙성을 찾을 수 없기 때문이다.","code":"\nword_v <- c(\"love\", \"loves\", \"loved\", \"You\", \"You're\", \"You'll\", \"me\", \"my\", \"myself\", \"go\", \"went\") \n\nSnowballC::wordStem(word_v)##  [1] \"love\"   \"love\"   \"love\"   \"You\"    \"You'r\"  \"You'll\"\n##  [7] \"me\"     \"my\"     \"myself\" \"go\"     \"went\"\n"},{"path":"text-clean.html","id":"spacyr","chapter":"5 .  정제(전처리)","heading":"5.3.9.2 spacyr","text":"표제어 추출에 사용하는 패키지는 spacyr이다. 사용자설명서","code":""},{"path":"text-clean.html","id":"미니콘다-설치","chapter":"5 .  정제(전처리)","heading":"5.3.9.2.1 미니콘다 설치","text":"(아나콘다 혹은 미니콘다가 이미 컴퓨터에 설치돼 있으면 곧바로 spacyr 패키지 설치로 이동)spacyr은 파이썬 spaCy패키지를 R에서 사용할 수 있도록한 패키지이므로, spacyr을 이용하기 위해서는 컴퓨터에 파이썬과 필요한 패키지가 설치돼 있어야 한다. 파이썬과 자주사용하는 패키지를 한번에 설치할 수 있는 것이 아나콘다와 미니콘다이다.아나콘다: 파이썬 + 자주 사용하는 패키지 1500여개 설치(3GB 이상 설치공간 필요)미니콘다: 파이썬 + 필수 패키지 700여개 설치(260MB 설치공간 필요)\n(미니콘다 설치 안내)미니콘다 설치 안내 페이지에서 본인의 운영체제에 맞는 파일을 선택해 컴퓨터에 설치한다. 관리자권한으로 설치한다 (다운로드받은 파일에 마우스커서 올려 놓고 오른쪽 버튼 클릭해 ‘관리자권한으로 실행’ 선택)설치후 Anaconda Prompt를 연다.윈도화면 왼쪽 아래의 시작버튼을 클릭하면 윈도 시작 메뉴가 열린다. 상단 ’최근에 추가한 앱’에서 Anaconda Prompt(Miniconda 3)을 클릭하면 Anaconda Prompt가 열린다. (Anaconda Powershell Prompt를 이용해도 된다)프롬프트가 열리면 conda --version을 입력한다. conda 4.9.2처럼 콘다의 버전 정보가 뜨면 설치에 성공.","code":""},{"path":"text-clean.html","id":"spacyr-패키지-설치","chapter":"5 .  정제(전처리)","heading":"5.3.9.2.2 spacyr 패키지 설치","text":"spacyr 패키지를 설치하고 구동한다. (패키지 설치할 때는 R이나 RStudio를 관리자 권한으로 실행해 설치한다.)패키지를 설치하고 구동했으면 spacy_install()을 실행한다. 콘솔에 Proceed여부를 묻는 화면이 나면 2: Yes를 선택해 진행한다.spacy_install()은 시스템 파이썬(또는 아나콘다 파이썬)과는 별개로 R환경에서 파이썬을 실행할 수 있는 콘다환경이 생성된다.","code":"\n# install.packages(\"spacyr\")\n# library(spacyr)\nspacy_install()"},{"path":"text-clean.html","id":"reticulate-패키지-설치","chapter":"5 .  정제(전처리)","heading":"5.3.9.2.3 reticulate 패키지 설치","text":"파이썬 모듈을 R환경에서 실행할 수 있도록 하는 파이썬-R 인터페이스 패키지다.","code":"\ninstall.packages(\"reticulate\")"},{"path":"text-clean.html","id":"spacy_initialize","chapter":"5 .  정제(전처리)","heading":"5.3.9.2.4 spacy_initialize()","text":"spacy_initialize()로 R에서 spaCy를 초기화한다.","code":"\n# library(spacyr)\n# spacy_initialize() "},{"path":"text-clean.html","id":"파이썬-설정-오류","chapter":"5 .  정제(전처리)","heading":"5.3.9.2.5 파이썬 설정 오류","text":"과거에 파이썬을 설치했었거나 혹은 파이썬에 의존하는 R패키지를 설치했었던 경우 오류가 발생할 수있다.spacy_initialize()를 실행하면 아래와 같은 파이썬 설정 오류가 발생할 수 있다.spacy_initialize()함수가 파이썬을 C:\\Users\\[사용자ID]\\AppData\\Local\\r-miniconda\\envs\\spacy_condaenv\\python.exe에서 찾는다는 의미다.이 곳으로 파이썬 환경을 설정한다. 세 가지 방법이 있다. (이 위치는 사용자별로 파이썬이 설치된 환경에 따라 다르다.)","code":"spacy python option is already set, spacyr will use:\n    condaenv = \"spacy_condaenv\"\nERROR: The requested version of Python\n('C:\\Users\\[사용자ID]\\AppData\\Local\\r-miniconda\\envs\\spacy_condaenv\\python.exe')\ncannot be used, as another version of Python\n('새로 설치한 미니콘다 경로') has\nalready been initialized . Please restart the R\nsession if you need to attach reticulate to a\ndifferent version of Python.\nError in use_python(python, required = required) : \n  failed to initialize requested version of Python"},{},{},{},{},{"path":"text-clean.html","id":"spacyr-설치-확인","chapter":"5 .  정제(전처리)","heading":"5.3.9.2.6 spacyr 설치 확인","text":"어간추출과 달리, went의 표제어인 go로 산출한다. me에 대해서는 I를 표제어로 산출하나, my에 대해서는 my를 표제어로 제시한다.","code":"\n# word_v <- c(\"love\", \"loves\", \"loved\", \"You\", \"You're\", \"You'll\", \"me\", \"my\", \"myself\", \"go\", \"went\")\n# \n# library(spacyr)\n# spacy_initialize()\n# \n\n# word_v %>% spacy_parse()"},{"path":"text-clean.html","id":"연습-2","chapter":"5 .  정제(전처리)","heading":"5.3.10 연습","text":"셰익스피어의 소네트27을 SnowbalC와 spacyr을 이용해 분석해 보자","code":"\ns27_v <- \"Weary with toil I haste me to my bed,\nThe dear repose for limbs with travel tired;\nBut then begins a journey in my head\nTo work my mind when body's work's expired;\nFor then my thoughts, from far where I abide,\nIntend a zealous pilgrimage to thee,\nAnd keep my drooping eyelids open wide\nLooking on darkness which the blind do see:\nSave that my soul's imaginary sight\nPresents thy shadow to my sightless view,\nWhich like a jewel hung in ghastly night\nMakes black night beauteous and her old face new.\nLo! thus by day my limbs, by night my mind,\nFor thee, and for myself, no quietness find.\""},{"path":"text-clean.html","id":"snowballc","chapter":"5 .  정제(전처리)","heading":"5.3.10.1 SnowballC","text":"unnest_tokens()의 token =인자에 wordStem()함수를 투입하면 오류 발생.어근추츨(stemming)을 먼저 한 다음 정돈텍스트(tidy text)로 전환한다.행(row) 하나에 토큰(token)이 하나만 할당 (one-token-per-row).토큰화를 먼저 한 다음에 어간을 추출한다.","code":"\nlibrary(SnowballC)\ns27_v %>% tibble(text = .) %>% \n  unnest_tokens(word, text, token = wordStem)\ns27_v %>% SnowballC::wordStem()## [1] \"Weary with toil I haste me to my bed,\\nThe dear repose for limbs with travel tired;\\nBut then begins a journey in my head\\nTo work my mind when body's work's expired;\\nFor then my thoughts, from far where I abide,\\nIntend a zealous pilgrimage to thee,\\nAnd keep my drooping eyelids open wide\\nLooking on darkness which the blind do see:\\nSave that my soul's imaginary sight\\nPresents thy shadow to my sightless view,\\nWhich like a jewel hung in ghastly night\\nMakes black night beauteous and her old face new.\\nLo! thus by day my limbs, by night my mind,\\nFor thee, and for myself, no quietness find.\"\n\ns27_v %>% tibble(text = . ) %>% \n  unnest_tokens(word, text) %>% \n  mutate(stemmed = wordStem(word)) %>% \n  count(stemmed, sort = T)## # A tibble: 81 × 2\n##   stemmed     n\n##   <chr>   <int>\n## 1 my          9\n## 2 for         4\n## 3 to          4\n## 4 a           3\n## 5 and         3\n## 6 night       3\n## # … with 75 more rows\n"},{"path":"text-clean.html","id":"spacyr-1","chapter":"5 .  정제(전처리)","heading":"5.3.10.2 spacyr","text":"spacy_parse()는 표제어(lemme)와 품사 태그(pos) 등의 정보가 포함된 데이터프레임으로 산출한다.분석에 필요한 열만 선택한다.unnest_tokens()로 출력 형식 통일","code":"\ns27_v %>% spacy_parse()\ns27_v %>% spacy_parse() %>% \n  select(token:pos) \ns27_v %>% spacy_parse() %>% \n  select(token:pos) %>% \n  unnest_tokens(word, lemma) %>% \n  count(word, sort = T)"},{"path":"text-clean.html","id":"비교-1","chapter":"5 .  정제(전처리)","heading":"5.3.10.3 비교","text":"불용어를 제거하지 않고 SnowballC 및 spacyr를 이용한 정규화 결과와 정규화하지 않은 결과를 비교해보자.불용어를 제거하고 SnowballC 및 spacyr를 이용한 정규화 결과와 정규화하지 않은 결과를 비교해보자.막대도표 대신 표를 만들어 비교해 보자. 이를 위해서는 데이터프레임을 행방향으로 결합해야 한다.행결합하면 행의 이름이 구분할 필요가 있으므로, unnest_tokens()함수에서 output =인자를 설정할 때 해당 패키지 이름으로 설정한다.불용어 처리 전과 후를 구분해서 비교해보자.불용어 처리한 다음에도 결과를 비교해보자.","code":"\nSnowballC_df <- s27_v %>% tibble(text = . ) %>% \n  unnest_tokens(word, text) %>% \n  mutate(stemmed = wordStem(word)) %>% \n  count(stemmed, sort = T) %>% \n  slice_max(n, n = 15)\n\nspacyr_df <- s27_v %>% spacy_parse() %>% \n  select(token:pos) %>% \n  unnest_tokens(word, lemma) %>% \n  count(word, sort = T) %>% \n  slice_max(n, n = 15)\n\nnoNor_df <- SnowballC_df <- s27_v %>% tibble(text = . ) %>% \n  unnest_tokens(word, text) %>% \n  count(word, sort = T) %>% \n  slice_max(n, n = 15)\n\ndf <- bind_rows(SnowballC = SnowballC_df, spacyr = spacyr_df, noNor = noNor_df,\n                .id = \"ID\")\ndf %>% mutate(word = reorder(word, n)) %>% \n  ggplot() + geom_col(aes(word, n, fill = ID), show.legend = F) + \n  coord_flip() +\n  facet_wrap(~ID, scales = \"free\") +\n  ggtitle(\"정규화 결과 비교\") +\n  xlab(\"단어\") + ylab(\"빈도\") +\n  theme(plot.title = element_text(size = 24, hjust = 0.5),\n        axis.title.x = element_text(size = 18),\n        axis.title.y = element_text(size = 18))\nSnowballC_df <- s27_v %>% tibble(text = . ) %>% \n  unnest_tokens(word, text) %>% \n  mutate(stemmed = wordStem(word)) %>% \n  anti_join(stop_words) %>% \n  count(stemmed, sort = T) %>% \n  head(20)\n\nspacyr_df <- s27_v %>% spacy_parse() %>% \n  select(token:pos) %>% \n  unnest_tokens(word, lemma) %>% \n  anti_join(stop_words) %>% \n  count(word, sort = T) %>% \n  head(20)\n\nnoNor_df <- SnowballC_df <- s27_v %>% tibble(text = . ) %>% \n  unnest_tokens(word, text) %>% \n  anti_join(stop_words) %>% \n  count(word, sort = T) %>% \n  head(20)\n\ndf <- bind_rows(SnowballC = SnowballC_df, spacyr = spacyr_df, noNor = noNor_df,\n                .id = \"ID\")\ndf %>% mutate(word = reorder(word, n)) %>% \n  ggplot() + geom_col(aes(word, n, fill = ID), show.legend = F) + \n  coord_flip() +\n  facet_wrap(~ID, scales = \"free\") +\n  ggtitle(\"정규화 결과 비교\") +\n  xlab(\"단어\") + ylab(\"빈도\") +\n  theme(plot.title = element_text(size = 24, hjust = 0.5),\n        axis.title.x = element_text(size = 18),\n        axis.title.y = element_text(size = 18))\nSnowballC_df <- s27_v %>% tibble(text = . ) %>% \n  unnest_tokens(SnowballC, text) %>% \n  mutate(SnowballC = wordStem(SnowballC)) %>% \n  count(SnowballC) %>% \n  arrange(SnowballC) %>% \n  head(40)\n\nspacyr_df <- s27_v %>% spacy_parse() %>% \n  select(lemma) %>% \n  unnest_tokens(spacyr, lemma) %>% \n  count(spacyr) %>% \n  arrange(spacyr) %>% \n  head(40)\n\nnoNor_df <- s27_v %>% tibble(text = . ) %>% \n  unnest_tokens(noNor, text) %>% \n  count(noNor) %>% \n  arrange(noNor) %>% \n  head(40)\n\n\nbind_cols(noNor_df, SnowballC_df, spacyr_df)\nSnowballC2_df <- s27_v %>% tibble(text = . ) %>% \n  unnest_tokens(SnowballC, text) %>% \n  mutate(word = wordStem(SnowballC)) %>% \n  anti_join(stop_words) %>% \n  mutate(SnowballC = word) %>% \n  count(SnowballC) %>% \n  arrange(SnowballC) %>% \n  head(40)\n\nspacyr2_df <- s27_v %>% spacy_parse() %>% \n  select(lemma) %>% \n  unnest_tokens(word, lemma) %>% \n  anti_join(stop_words) %>% \n  rename(spacyr = word) %>% \n  count(spacyr) %>% \n  arrange(spacyr) %>% \n  head(40)\n\nnoNor2_df <- s27_v %>% tibble(text = . ) %>% \n  unnest_tokens(word, text) %>% \n  anti_join(stop_words) %>% \n  count(word) %>% \n  arrange(word) %>% \n  head(40)\n\n\nbind_cols(noNor2_df, SnowballC2_df, spacyr2_df)\ns27_v <- \"Weary with toil I haste me to my bed,\nThe dear repose for limbs with travel tired;\nBut then begins a journey in my head\nTo work my mind when body's work's expired;\nFor then my thoughts, from far where I abide,\nIntend a zealous pilgrimage to thee,\nAnd keep my drooping eyelids open wide\nLooking on darkness which the blind do see:\nSave that my soul's imaginary sight\nPresents thy shadow to my sightless view,\nWhich like a jewel hung in ghastly night\nMakes black night beauteous and her old face new.\nLo! thus by day my limbs, by night my mind,\nFor thee, and for myself, no quietness find.\""},{"path":"text-clean.html","id":"과제-1","chapter":"5 .  정제(전처리)","heading":"5.3.11 과제","text":"구텐베르크 프로젝트에서 영문 문서 한편을 선택해 두 가지 방식의 정규화(SnowballC를 이용한 어근 추출과 spacyr을 이용한 표제어 추출)한 결과와 정규화하지 않은 결과를 비교한다.막대도표로 시각화해 비교표로 만들어 비교3건의 결과에 대해 간략하게 설명","code":""},{"path":"tf-idf.html","id":"tf-idf","chapter":"6 .  단어 빈도수","heading":"6 .  단어 빈도수","text":"","code":""},{"path":"tf-idf.html","id":"tf-idf-overview","chapter":"6 .  단어 빈도수","heading":"6.1 개관","text":"단어(term) - 문서(document) - 말뭉치(corpus)문서에 사용된 단어(term)를 통해 그 문서의 주제를 추론할 수 있다. 말뭉치는 문서들의 ’뭉치’다. 신문 말뭉치는 여러 신문들의 기사를 모아놓은 것이고, 소설말뭉치는 여러 소설을 모아놓은 문서들의 집합이다. 말뭉치는 상대적인 개념이다. 예를 들어, 소설집을 말뭉치라고 하면 개별 소설이 문서가 된다. 반면, 소설 한편을 말뭉치라고 하면, 소설의 각 장이 문서가 된다.개별 문서의 주제를 추론하려면 모든 문서에 걸쳐 사용빈도가 높은 단어보다는 개별 문서에서만 사용빈도가 높은 단어를 찾아야 한다. 즉, 개별 문서에 등장하는 단어의 상대빈도가 높은 단어가 개별 문서의 의미를 잘 나타낸다.상대빈도를 계산하는 방법으로 널리 사용되는 지표가 tf_idf, 승산비, 가중로그승산비 등이다.tf_idf널리 사용되는 상대빈도 지표. 문서 전반에 걸쳐 등장하는 단어의 점수를 낮게 계산하고, 특정 문서에서 등장하는 빈도에 점수를 높게 부여해 상대 빈도를 계산한다. 예를 들어, ‘운수좋은 날’과 ’사랑손님과 어머니’ 등 2편으로 이뤄진 소설집 말뭉치가 있다고 하자. “은/는/이/가”와 같은 단어는 이 말뭉치에가 사용빈도가 꽤 높지만, 각 소설의 주제를 파악하는데 크게 기여하지 못한다. 반면 ’어머니’와 같은 단어는 ’운수좋은 날’에는 등장하지 않고, ’사랑손님과 어머니’에는 많이 등장해, 해당 문서의 주제를 파악하는데 크게 기여한다.승산비(odds ratio)승산(odds)의 비를 이용해 복수의 문서에 등장한 단어의 상대적인 빈도를 계산한다. 2종의 문서에 대해서만 사용할 수 있다.가중 로그 승산비(weighted log odds)베이지언 확률모형으로 가중치를 계산해 단어의 상대적인 빈도를 계산한다. tf_idf와 승산비의 단점을 보완한 방법이다.","code":"\npkg_v <- c(\"tidyverse\", \"tidytext\",\"RcppMeCab\")\nlapply(pkg_v, require, ch = T)## [[1]]\n## [1] TRUE\n## \n## [[2]]\n## [1] TRUE\n## \n## [[3]]\n## [1] TRUE\n"},{"path":"tf-idf.html","id":"tf_idf","chapter":"6 .  단어 빈도수","heading":"6.2 tf_idf","text":"단어빈도-역문서빈도(term frequency-inverse document frequency: tf_idf).말 그대로 개별 문서의 단어빈도(tf)와 문서전반에 걸쳐 사용된 정도의 역수(idf)를 곱해 구한다.","code":""},{"path":"tf-idf.html","id":"공식","chapter":"6 .  단어 빈도수","heading":"6.2.1 공식","text":"tf_idf의 요소인 tf와 idf에 대해 각각 알아보자.","code":""},{"path":"tf-idf.html","id":"tfterm-frequency","chapter":"6 .  단어 빈도수","heading":"6.2.1.0.1 tf(term frequency)","text":"개별 문서(d)에 등장하는 단어(t)의 수를 각 문서에 등장한 모든 단어의 수로 나눈 값.\\[\ntf(t, d) = \\frac{tf_{document}}{tf_{total}}\n\\]\\(tf_{document}\\): 각 문서에 등장한 해당 단어의 빈도\\(tf_{total}\\): 각 문서에 등장한 모든 단어의 수","code":""},{"path":"tf-idf.html","id":"dfdocument-frequency","chapter":"6 .  단어 빈도수","heading":"6.2.1.0.2 df(document frequency)","text":"특정 단어(t)가 나타난 문서(D)의 수. df가 높다면 ’은/는/이/가’처럼 대부분의 문서에서 사용되는 단어임을 나타낸다.\\[\ndf(t, D)\n\\]","code":""},{"path":"tf-idf.html","id":"idfinverse-document-frequency","chapter":"6 .  단어 빈도수","heading":"6.2.1.0.3 idf(inverse document frequency)","text":"전체 문서의 수(N)를 해당 단어의 df로 나눈 뒤 로그를 취한 값. 값이 클수록 특정 문서에서만 등장하는 특이한 단어라는 의미가 된다.\\[\nidf(t, D) = log(\\frac{N}{df})\n\\]","code":""},{"path":"tf-idf.html","id":"tf_idf-1","chapter":"6 .  단어 빈도수","heading":"6.2.1.0.4 tf_idf","text":"tf와 idf를 곱하면 tf_idf를 구할 수 있다.\\[\n  tfidf(t, d, D) = tf(t, d) \\times idf(t, D)\n\\]t: 단어d: 개별 문서D: 개별 문서의 집합(말뭉치)","code":""},{"path":"tf-idf.html","id":"적용","chapter":"6 .  단어 빈도수","heading":"6.2.2 적용","text":"짧은 문장을 이용해 tf_idf의 원리를 파악해 보자. 먼저 토큰화한다. 여기서 각 행은 개별 문서에 해당하고, 4개 행의 합의 말뭉치에 해당한다. 각 행을 식별하기 위해 문장별로 토큰화해 ID를 부여하자.","code":"\nsky_v <-  c( \"The sky is blue.\", \n           \"The sun is bright today.\",\n           \"The sun in the sky is bright.\", \n           \"We can see the shining sun, the bright sun.\")\n\nsky_doc <- tibble(text = sky_v) %>% \n  unnest_tokens(sentence, text, token = \"sentences\") %>% \n  mutate(lineID = row_number()) %>% \n  mutate(lineID = as.factor(lineID))\n\nsky_doc## # A tibble: 4 × 2\n##   sentence                                    lineID\n##   <chr>                                       <fct> \n## 1 the sky is blue.                            1     \n## 2 the sun is bright today.                    2     \n## 3 the sun in the sky is bright.               3     \n## 4 we can see the shining sun, the bright sun. 4\n"},{"path":"tf-idf.html","id":"tf","chapter":"6 .  단어 빈도수","heading":"6.2.2.1 tf","text":"개별 문서(d)에 등장하는 단어(t)의 수를 각 문서에 등장한 모든 단어의 수로 나눈 값.\\[\ntf(t, d) = \\frac{tf_{document}}{tf_{total}}\n\\]\\(tf_{document}\\): 각 문서에 등장한 해당 단어의 빈도\\(tf_{total}\\): 각 문서에 등장한 모든 단어의 수tf의 분자인 \\(tf_{document}\\)(각 문서에 등장한 해당 단어의 빈도)를 구한다. 여기서 각 문서는 각 행이다. 따라서 각 행에서 등장한 단어의 빈도를 계산하면 된다.tf의 분모 \\(tf_{total}\\)(각 문서에 등장한 모든 단어의 수)를 구한다. 각 행을 이루는 모든 단어의 수를 계산하면 된다.\\(tf_{document}\\)(각 문서에 등장한 해당 단어의 빈도)를 \\(tf_{total}\\)(각 문서에 등장한 모든 단어의 수)로 나눈다.","code":"\nsky_tfd <- \nsky_doc %>% \n  unnest_tokens(word, sentence) %>% \n  anti_join(stop_words) %>% \n  count(lineID, word) %>% \n  rename(tfdoc = n)\n  \nsky_tfd## # A tibble: 10 × 3\n##   lineID word   tfdoc\n##   <fct>  <chr>  <int>\n## 1 1      blue       1\n## 2 1      sky        1\n## 3 2      bright     1\n## 4 2      sun        1\n## 5 3      bright     1\n## 6 3      sky        1\n## # … with 4 more rows\n\nsky_tft <- sky_tfd %>% \n  mutate(N = length(unique(lineID))) %>%  #total number of documnets\n  count(lineID) %>% \n  rename(tftotal = n)\n\nsky_tft## # A tibble: 4 × 2\n##   lineID tftotal\n##   <fct>    <int>\n## 1 1            2\n## 2 2            2\n## 3 3            3\n## 4 4            3\n\nsky_tf <- left_join(sky_tfd, sky_tft) %>% \n  mutate(tf = tfdoc/tftotal)\n\nsky_tf## # A tibble: 10 × 5\n##   lineID word   tfdoc tftotal    tf\n##   <fct>  <chr>  <int>   <int> <dbl>\n## 1 1      blue       1       2 0.5  \n## 2 1      sky        1       2 0.5  \n## 3 2      bright     1       2 0.5  \n## 4 2      sun        1       2 0.5  \n## 5 3      bright     1       3 0.333\n## 6 3      sky        1       3 0.333\n## # … with 4 more rows\n"},{"path":"tf-idf.html","id":"dfdocument-frequency-1","chapter":"6 .  단어 빈도수","heading":"6.2.2.2 df(document frequency)","text":"특정 단어(t)가 나타난 문서(D)의 수. df가 높다면 ’은/는/이/가’처럼 대부분의 문서에서 사용되는 단어임을 나타낸다.\\[\ndf(t, D)\n\\]\n특정 단어가 등장한 문서의 수를 계산해 구할 수 있다.‘blue’: 1번행에만 등장했으므로 1‘sun’: 2,3, 4번 행에 등장했으므로 3","code":"\nsky_df <- table(sky_tf$word) %>% \n  as.data.frame() %>% \n  rename(word = Var1, df = Freq)\nsky_df##      word df\n## 1    blue  1\n## 2  bright  3\n## 3 shining  1\n## 4     sky  2\n## 5     sun  3\n"},{"path":"tf-idf.html","id":"idfinverse-document-frequency-1","chapter":"6 .  단어 빈도수","heading":"6.2.2.3 idf(inverse document frequency)","text":"전체 문서의 수(N)를 해당 단어의 df로 나눈 뒤 로그를 취한 값. 값이 클수록 특정 문서에서만 등장하는 특이한 단어라는 의미가 된다.\\[\nidf(t, D) = log(\\frac{N}{df})\n\\]공식을 그대로 적용해 계산한다. sky_tf에 N의 값을 계산해 두 데이터프레임을 열방향 결합한다.3개 행에 걸쳐 등장하는 ’bright’의 idf가 1개 행에만 등장한 ’blue’보다 idf가 작다.","code":"\nsky_tf <- \n  sky_tf %>% \n  mutate(N = length(unique(lineID))) #total number of documnets\n\nsky_idf <- \n  left_join(sky_tf, sky_df) %>% \n  mutate(idf = log(N / df))\n\nsky_idf## # A tibble: 10 × 8\n##   lineID word   tfdoc tftotal    tf     N    df   idf\n##   <fct>  <chr>  <int>   <int> <dbl> <int> <int> <dbl>\n## 1 1      blue       1       2 0.5       4     1 1.39 \n## 2 1      sky        1       2 0.5       4     2 0.693\n## 3 2      bright     1       2 0.5       4     3 0.288\n## 4 2      sun        1       2 0.5       4     3 0.288\n## 5 3      bright     1       3 0.333     4     3 0.288\n## 6 3      sky        1       3 0.333     4     2 0.693\n## # … with 4 more rows\n"},{"path":"tf-idf.html","id":"tf_idf-2","chapter":"6 .  단어 빈도수","heading":"6.2.2.4 tf_idf","text":"tf와 idf를 곱하면 tf_idf를 구할 수 있다.\\[\n  tfidf(t, d, D) = tf(t, d) \\times idf(t, D)\n\\]t: 단어d: 개별 문서D: 개별 문서의 집합(말뭉치)","code":"\nsky_idf %>% \n  mutate(tf_idf = tf * idf) %>% \n  arrange(-tf_idf)## # A tibble: 10 × 9\n##   lineID word   tfdoc tftotal    tf     N    df   idf tf_idf\n##   <fct>  <chr>  <int>   <int> <dbl> <int> <int> <dbl>  <dbl>\n## 1 1      blue       1       2 0.5       4     1 1.39   0.693\n## 2 4      shini…     1       3 0.333     4     1 1.39   0.462\n## 3 1      sky        1       2 0.5       4     2 0.693  0.347\n## 4 3      sky        1       3 0.333     4     2 0.693  0.231\n## 5 4      sun        2       3 0.667     4     3 0.288  0.192\n## 6 2      bright     1       2 0.5       4     3 0.288  0.144\n## # … with 4 more rows\n"},{"path":"tf-idf.html","id":"bind_tf_idftbl-term-document-n","chapter":"6 .  단어 빈도수","heading":"6.2.3 bind_tf_idf(tbl, term, document, n)","text":"tidytext패키지에서 제공하는 bind_tf_idf()함수를 이용하면 tf_idf를 계산할 수 있다.tbl: 정돈데이터(한행에 값 하나).tbl: 정돈데이터(한행에 값 하나).term: 문자열이나 기호 등의 단어가 저장된 열.term: 문자열이나 기호 등의 단어가 저장된 열.document: 문자열이나 기호 등의 문서식별부호가 저장된 열.document: 문자열이나 기호 등의 문서식별부호가 저장된 열.n: 문자열이나 기호 등의 문서-용어의 빈도가 저장된 열.n: 문자열이나 기호 등의 문서-용어의 빈도가 저장된 열.","code":"\nsky_tfd %>% \n  bind_tf_idf(tbl = ., term = word, document = lineID, n = tfdoc)## # A tibble: 10 × 6\n##   lineID word   tfdoc    tf   idf tf_idf\n##   <fct>  <chr>  <int> <dbl> <dbl>  <dbl>\n## 1 1      blue       1 0.5   1.39  0.693 \n## 2 1      sky        1 0.5   0.693 0.347 \n## 3 2      bright     1 0.5   0.288 0.144 \n## 4 2      sun        1 0.5   0.288 0.144 \n## 5 3      bright     1 0.333 0.288 0.0959\n## 6 3      sky        1 0.333 0.693 0.231 \n## # … with 4 more rows\n"},{"path":"tf-idf.html","id":"승산비odds-ratio","chapter":"6 .  단어 빈도수","heading":"6.3 승산비(odds ratio)","text":"문서의 상대빈도를 구하는 또 다른 방법이 승산비다. 승산(odds)의 비를 이용해 복수의 문서에 등장한 단어의 상대적인 빈도 계산할 수 있다. 따라서 승산비는 2종의 문서에 대해서만 구할 수 있다.승산비(odds ratio)한 사건의 승산(odds)에 대한 다른 한 사건 승산의 비(ratio)다. 다음은 승산B에 대한 승산A의 비.\\[\n승산비 = \\frac{승산A}{승산B}\n\\]비(ratio)두 부분 중 한 부분에 대한 다른 한 부분의 비. 예를 들어, 축구경기를 할때 한국이 3골을 넣고, 일본이 1골을 넣었다면, 비는 3대 1로서, 일본팀 1점에 대한 한국팀 3점의 비(ratio)다.\\[\n:B = \\frac{}{B}\n\\]승산(odds)어느 한 사건이 일어날 가능성이 승산(odds)이다. 영어를 그대로 읽어 ’오즈’라고도 한다. 한 사건의 발생빈도(n)를 전체의 값(total)으로 나눈다.\\[\n승산(odds) = \\frac{발생빈도(n) + 1}{총빈도(total) + 1}\n\\]\n분자와 분모에 각각 1을 더하는 이유는 발생빈도가 0인 경우도 있기 때문이다. 분모가 0이면 무한대가 된다.","code":""},{"path":"tf-idf.html","id":"자료준비","chapter":"6 .  단어 빈도수","heading":"6.3.1 자료준비","text":"네이버영화 댓글의 승산비를 구해보자. 댓글을 긍정과 부정으로 구분해 라벨링한 자료를 이용한다.Naver sentiment movie corpus다운로드한 자료 이입 및 검토공백문자 \\t으로 구분된 문자데이터이므로, read_tsv()로 이입한다.데이터셋이 커 처리에 시간이 걸리므로 각 라벨별로 1000개 행씩 추출한다.RcppMeCab패키지에서 pos()함수로 형태소를 추출한 뒤, 일반명사(‘nng’)만 추출해 빈도를 계산한다.","code":"\nurl_v <- 'https://github.com/e9t/nsmc/raw/master/ratings.txt'\ndest_v <- 'data/ratings.txt'\n\ndownload.file(url_v, dest_v, mode = \"wb\")\nlist.files('data/.')\nread_lines(\"data/ratings.txt\") %>% glimpse()##  chr [1:200001] \"id\\tdocument\\tlabel\" ...\n\nread_tsv(\"data/ratings.txt\") %>% glimpse()## Rows: 200,000\n## Columns: 3\n## $ id       <dbl> 8112052, 8132799, 4655635, 9251303, 10067…\n## $ document <chr> \"어릴때보고 지금다시봐도 재밌어요ㅋㅋ\", \"…\n## $ label    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n\nratings_df <- read_tsv(\"data/ratings.txt\") \nhead(ratings_df)## # A tibble: 6 × 3\n##         id document                                    label\n##      <dbl> <chr>                                       <dbl>\n## 1  8112052 어릴때보고 지금다시봐도 재밌어요ㅋㅋ            1\n## 2  8132799 디자인을 배우는 학생으로, 외국디자이너와 …      1\n## 3  4655635 폴리스스토리 시리즈는 1부터 뉴까지 버릴께 …     1\n## 4  9251303 와.. 연기가 진짜 개쩔구나.. 지루할거라고 …      1\n## 5 10067386 안개 자욱한 밤하늘에 떠 있는 초승달 같은 …      1\n## 6  2190435 사랑을 해본사람이라면 처음부터 끝까지 웃을…     1\n\nset.seed(37)\nby1000_df <- \n  ratings_df %>% \n  group_by(label) %>% \n  sample_n(size = 1000)\n\nby1000_df %>% glimpse()## Rows: 2,000\n## Columns: 3\n## Groups: label [2]\n## $ id       <dbl> 10101634, 5925555, 9809791, 4541051, 8063…\n## $ document <chr> \"액션이 부족한 가운데 이야기 자체가 허접…\n## $ label    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\nby1000_df %>% \n  unnest_tokens(word, document, token = pos) %>% \n  separate(word, c(\"word\", \"pos\"), sep = \"/\") %>% \n  filter(pos == \"nng\") %>% \n  filter(word != \"영화\") %>% \n  count(word, sort = T)## # A tibble: 2,831 × 3\n## # Groups:   label [2]\n##   label word       n\n##   <dbl> <chr>  <int>\n## 1     1 최고      83\n## 2     1 감동      58\n## 3     1 연기      58\n## 4     0 감독      46\n## 5     0 스토리    46\n## 6     0 시간      46\n## # … with 2,825 more rows\n"},{"path":"tf-idf.html","id":"자료-준비","chapter":"6 .  단어 빈도수","heading":"6.3.1.1 자료 준비","text":"영화평은 긍정적인 평에는 ‘1’, 부정적인 평에는 ’0’으로 분류돼 있다. 긍정적인 평과 부정적인 평에 사용된 명사의 승산비를 구해보자. 이를 위해 long form을 wide form으로 변형해 label열의 값을 열의 헤더로 변환하고, 각 열의 값으로는 토큰을 투입한다. 결측값이 있으면 연산이 안되므로 NA값은 ’0’으로 채운다.","code":"\nby1000_df %>% \n  unnest_tokens(word, document, token = pos) %>% \n  separate(word, c(\"word\", \"pos\"), sep = \"/\") %>% \n  filter(pos == \"nng\") %>% \n  filter(word != \"영화\") %>% \n  count(word) %>% \n  pivot_wider(names_from = label,\n              values_from = n, \n              values_fill = list(n = 0))## # A tibble: 2,303 × 3\n##   word    `0`   `1`\n##   <chr> <int> <int>\n## 1 가가      1     0\n## 2 가관      1     0\n## 3 가능      1     4\n## 4 가든      1     0\n## 5 가리      1     0\n## 6 가문      1     0\n## # … with 2,297 more rows\n"},{"path":"tf-idf.html","id":"승산비-계산","chapter":"6 .  단어 빈도수","heading":"6.3.1.2 승산비 계산","text":"긍정평과 부정평의 승산을 구한다음, 승산비를 계산한다.승산비를 이용해 영화의 긍정평과 부정평에서 상대적으로 많이 사용된 명사를 추출했다. 순서대로 정렬하자.’밋’과 ’ㄷ’이 사용된 문장을 살펴보자. drop =인자에 FALSE를 투입하면 토큰화한 문장을 제거하지 않고 남겨둔다.’재밋었음’에서 MeCab형태소 분석기가 ’밋’을 명사로 추출한 결과이다.막대도표로 시각화하자. 긍정평과 부정평 도표를 분리해 표시하기 위해 승산비 1을 기준으로 크면 ‘긍정평’ 작으면 ’부정평’을 부여한다.중요도가 비슷한 단어를 추출해보자. 승산비가 1이면 분자와 분모가 같으므로 긍정평과 부정평의 승산이 같다는 의미다.중요도가 비슷하면서 빈도가 높은 단어를 찾아 보자.","code":"\nby1000_df %>% \n  unnest_tokens(word, document, token = pos) %>% \n  separate(word, c(\"word\", \"pos\"), sep = \"/\") %>% \n  filter(pos == \"nng\") %>% \n  filter(word != \"영화\") %>% \n  count(word) %>% \n  pivot_wider(names_from = label,\n              values_from = n, \n              values_fill = list(n = 0)) %>% \n  rename(posi = `1`, nega = `0`) %>% \n  mutate(odds_posi = ((posi+1)/sum(posi+1)),\n         odds_nega = ((nega+1)/sum(nega+1))) %>% \n  mutate(posi_odds_ratio = (odds_posi / odds_nega)) ## # A tibble: 2,303 × 6\n##   word   nega  posi odds_posi odds_nega posi_odds_ratio\n##   <chr> <int> <int>     <dbl>     <dbl>           <dbl>\n## 1 가가      1     0  0.000181  0.000334           0.542\n## 2 가관      1     0  0.000181  0.000334           0.542\n## 3 가능      1     4  0.000904  0.000334           2.71 \n## 4 가든      1     0  0.000181  0.000334           0.542\n## 5 가리      1     0  0.000181  0.000334           0.542\n## 6 가문      1     0  0.000181  0.000334           0.542\n## # … with 2,297 more rows\n\nrate_odds_df <- \nby1000_df %>% \n  unnest_tokens(word, document, token = pos) %>% \n  separate(word, c(\"word\", \"pos\"), sep = \"/\") %>% \n  filter(pos == \"nng\") %>% \n  filter(word != \"영화\") %>% \n  count(word) %>% \n  pivot_wider(names_from = label,\n              values_from = n, \n              values_fill = list(n = 0)) %>% \n  rename(posi = `1`, nega = `0`) %>% \n  mutate(odds_posi = ((posi+1)/sum(posi+1)),\n         odds_nega = ((nega+1)/sum(nega+1))) %>% \n  mutate(posi_odds_ratio = (odds_posi / odds_nega)) %>% \n  # 긍정평과 부정평 각각 상위 20개씩 필터 \n  filter(rank(posi_odds_ratio) <= 20 | rank(-posi_odds_ratio) <= 20) %>%   arrange(-posi_odds_ratio)\n\nrate_odds_df %>% head()## # A tibble: 6 × 6\n##   word   nega  posi odds_posi odds_nega posi_odds_ratio\n##   <chr> <int> <int>     <dbl>     <dbl>           <dbl>\n## 1 굿        0    18   0.00344  0.000167            20.6\n## 2 밋        0    16   0.00307  0.000167            18.4\n## 3 ㄷ        0    15   0.00289  0.000167            17.3\n## 4 삶        0    10   0.00199  0.000167            11.9\n## 5 마음      0     9   0.00181  0.000167            10.8\n## 6 최고      8    83   0.0152   0.00150             10.1\n\nrate_odds_df %>% tail()## # A tibble: 6 × 6\n##   word    nega  posi odds_posi odds_nega posi_odds_ratio\n##   <chr>  <int> <int>     <dbl>     <dbl>           <dbl>\n## 1 낭비       9     0  0.000181   0.00167          0.108 \n## 2 얘기      10     0  0.000181   0.00184          0.0985\n## 3 쓰레기    36     2  0.000543   0.00617          0.0879\n## 4 돈        19     0  0.000181   0.00334          0.0542\n## 5 짜증      24     0  0.000181   0.00417          0.0433\n## 6 최악      24     0  0.000181   0.00417          0.0433\n\nby1000_df %>% \n  unnest_tokens(word, document, token = pos, drop = F) %>% \n  separate(word, c(\"word\", \"pos\"), sep = \"/\") %>% \n  filter(pos == \"nng\") %>% \n  filter(word == \"밋\" | word == \"ㄷ\") ## # A tibble: 31 × 4\n## # Groups:   label [1]\n##   label document                                word  pos  \n##   <dbl> <chr>                                   <chr> <chr>\n## 1     1 \"이보다 더 유쾌할 수 없고 이보다 더 무… 밋    nng  \n## 2     1 \"이보다 더 유쾌할 수 없고 이보다 더 무… ㄷ    nng  \n## 3     1 \"이보다 더 유쾌할 수 없고 이보다 더 무… ㄷ    nng  \n## 4     1 \"이보다 더 유쾌할 수 없고 이보다 더 무… ㄷ    nng  \n## 5     1 \"이보다 더 유쾌할 수 없고 이보다 더 무… ㄷ    nng  \n## 6     1 \"이보다 더 유쾌할 수 없고 이보다 더 무… ㄷ    nng  \n## # … with 25 more rows\n\nenc2utf8(\"재밋었음10자는뭐여\") %>% pos()## $재밋었음10자는뭐여\n## [1] \"재/XPN\"    \"밋/NNG\"    \"었/EP\"     \"음/ETN\"   \n## [5] \"10/SN\"     \"자/NNG\"    \"는/JX\"     \"뭐/NP\"    \n## [9] \"여/VCP+EC\"\n\nrate_odds_df %>% \n  mutate(label = ifelse(posi_odds_ratio > 1, \"긍정평\", \"부정평\")) %>% \n  mutate(word = reorder(word, posi_odds_ratio)) %>% \n  \n  ggplot(aes(x = posi_odds_ratio, \n             y = word, \n             fill = label)) + \n  geom_col(show.legend = F) +\n  facet_wrap(~label, scales = \"free\")\nby1000_df %>% \n  unnest_tokens(word, document, token = pos) %>% \n  separate(word, c(\"word\", \"pos\"), sep = \"/\") %>% \n  filter(pos == \"nng\") %>% \n  filter(word != \"영화\") %>% \n  count(word) %>% \n  pivot_wider(names_from = label,\n              values_from = n, \n              values_fill = list(n = 0)) %>% \n  rename(posi = `1`, nega = `0`) %>% \n  mutate(odds_posi = ((posi+1)/sum(posi+1)),\n         odds_nega = ((nega+1)/sum(nega+1))) %>% \n  mutate(posi_odds_ratio = (odds_posi / odds_nega)) %>% \n  # 승산비 1을 중심으로 정렬\n  arrange(abs(1 - posi_odds_ratio)) %>% \n  head(20)## # A tibble: 20 × 6\n##   word    nega  posi odds_posi odds_nega posi_odds_ratio\n##   <chr>  <int> <int>     <dbl>     <dbl>           <dbl>\n## 1 이야기    14    13   0.00253   0.00250           1.01 \n## 2 정도      18    16   0.00307   0.00317           0.970\n## 3 결말       7     6   0.00127   0.00134           0.948\n## 4 느낌       7     6   0.00127   0.00134           0.948\n## 5 이상      15    13   0.00253   0.00267           0.948\n## 6 팬         7     6   0.00127   0.00134           0.948\n## # … with 14 more rows\n\nby1000_df %>% \n  unnest_tokens(word, document, token = pos) %>% \n  separate(word, c(\"word\", \"pos\"), sep = \"/\") %>% \n  filter(pos == \"nng\") %>% \n  filter(word != \"영화\") %>% \n  count(word) %>% \n  pivot_wider(names_from = label,\n              values_from = n, \n              values_fill = list(n = 0)) %>% \n  rename(posi = `1`, nega = `0`) %>% \n  # 긍정평과 부정평에서 각각 10회 초과한 단어 filter\n  filter(nega > 10 & posi > 10) %>%  \n  mutate(odds_posi = ((posi+1)/sum(posi+1)),\n         odds_nega = ((nega+1)/sum(nega+1))) %>% \n  mutate(posi_odds_ratio = (odds_posi / odds_nega)) %>% \n  arrange(abs(1 - posi_odds_ratio)) %>% \n  head(20)## # A tibble: 20 × 6\n##   word    nega  posi odds_posi odds_nega posi_odds_ratio\n##   <chr>  <int> <int>     <dbl>     <dbl>           <dbl>\n## 1 이야기    14    13    0.0201    0.0201           1.00 \n## 2 정도      18    16    0.0245    0.0254           0.963\n## 3 이상      15    13    0.0201    0.0214           0.942\n## 4 드라마    35    35    0.0518    0.0481           1.08 \n## 5 배우      27    22    0.0331    0.0374           0.884\n## 6 연출      14    11    0.0173    0.0201           0.861\n## # … with 14 more rows\n"},{"path":"tf-idf.html","id":"로그승산비","chapter":"6 .  단어 빈도수","heading":"6.3.2 로그승산비","text":"로그승산비(log odds ratio)\n승산비에 로그를 위한 값. 로그를 취하면 1보다 작은수는 음수가 되고, 1보다 큰수는 양수가 된다.\\[\n로그 승산비 = log(\\frac{승산A}{승산B})\n\\]로그 승산비를 이용하면 하나의 도표에 상대빈도를 표시할 수 있다. 먼저 로그승산비를 구한뒤, 0을 기준으로 긍정평과 부정평을 group으로 구분한다. 긍정평과 부정평 집단별로 구분돼 있으므로, 로그승산비의 절대값 상위 10개를 지정하면, 긍정평과 부정평 별로 각각 상위 10개 단어를 추출할 수 있다.막대도표로 시각화.","code":"\nby1000_df %>% \n  unnest_tokens(word, document, token = pos) %>% \n  separate(word, c(\"word\", \"pos\"), sep = \"/\") %>% \n  filter(pos == \"nng\") %>% \n  filter(word != \"영화\") %>% \n  count(word) %>% \n  pivot_wider(names_from = label,\n              values_from = n, \n              values_fill = list(n = 0)) %>% \n  rename(posi = `1`, nega = `0`) %>% \n  mutate(odds_posi = ((posi+1)/sum(posi+1)),\n         odds_nega = ((nega+1)/sum(nega+1))) %>% \n  # 승산비에 로그를 취한다.\n  mutate(log_odds_ratio = log(odds_posi / odds_nega)) ## # A tibble: 2,303 × 6\n##   word   nega  posi odds_posi odds_nega log_odds_ratio\n##   <chr> <int> <int>     <dbl>     <dbl>          <dbl>\n## 1 가가      1     0  0.000181  0.000334         -0.613\n## 2 가관      1     0  0.000181  0.000334         -0.613\n## 3 가능      1     4  0.000904  0.000334          0.997\n## 4 가든      1     0  0.000181  0.000334         -0.613\n## 5 가리      1     0  0.000181  0.000334         -0.613\n## 6 가문      1     0  0.000181  0.000334         -0.613\n## # … with 2,297 more rows\n\nrate_log_df <- \nby1000_df %>% \n  unnest_tokens(word, document, token = pos) %>% \n  separate(word, c(\"word\", \"pos\"), sep = \"/\") %>% \n  filter(pos == \"nng\") %>% \n  filter(word != \"영화\") %>% \n  count(word) %>% \n  pivot_wider(names_from = label,\n              values_from = n, \n              values_fill = list(n = 0)) %>% \n  rename(posi = `1`, nega = `0`) %>% \n  mutate(odds_posi = ((posi+1)/sum(posi+1)),\n         odds_nega = ((nega+1)/sum(nega+1))) %>% \n  # 승산비에 로그를 취한다.\n  mutate(log_odds_ratio = log(odds_posi / odds_nega)) \n\nrate_log_df  %>% \n  group_by(label  = ifelse(log_odds_ratio > 0, \"긍정평\", \"부정평\")) %>% \n  # 긍정평과 부정평별 각각 상위 10개 \n  slice_max(abs(log_odds_ratio), n = 10) ## # A tibble: 22 × 7\n## # Groups:   label [2]\n##   word   nega  posi odds_posi odds_nega log_odds_ratio label\n##   <chr> <int> <int>     <dbl>     <dbl>          <dbl> <chr>\n## 1 굿        0    18   0.00344  0.000167           3.02 긍정…\n## 2 밋        0    16   0.00307  0.000167           2.91 긍정…\n## 3 ㄷ        0    15   0.00289  0.000167           2.85 긍정…\n## 4 삶        0    10   0.00199  0.000167           2.48 긍정…\n## 5 마음      0     9   0.00181  0.000167           2.38 긍정…\n## 6 최고      8    83   0.0152   0.00150            2.31 긍정…\n## # … with 16 more rows\n\nrate_log_df %>% \n  group_by(label  = ifelse(log_odds_ratio > 0, \"긍정평\", \"부정평\")) %>% \n  slice_max(abs(log_odds_ratio), n = 10) %>% \n  \n  ggplot(aes(x = log_odds_ratio,\n             y = reorder(word, log_odds_ratio),\n             fill = label)) +\n  geom_col() "},{"path":"tf-idf.html","id":"가중로그승산비","chapter":"6 .  단어 빈도수","heading":"6.4 가중로그승산비","text":"로그승산비는 문서 2종에 대한 승산으로 비를 구하므로, 3종 이상의 문서로 구성된 말뭉치에 적용할 수 없는 한계가 있다. tf_idf는 3종 이상의 문서로 구성된 말뭉치에 적용할 수 있지만, tf_idf계산 방식에서 오는 한계가 있다. 모든 문서에 걸쳐 등장하는 단어라 해서 반드시 불용어처럼 문서의 의미파악에 기여하지 못하는 것이 아니기 때문이다.tf_idf의 한계를 Tyler Schnoebelen이 [Monroe, Colaresi, Quinn(2008)](Monroe, Colaresi, Quinn(2008)의 연구를 토대 블로그문서\n“dare say never use tf-idf ”을 통해 지적하며, 대안으로 로그승산비에 베이지언 확률모형을 적용한 가중로그승산비(weighted log odds ratio)를 제안했다.이에 tidytext 개발자인 Julia Silge가 tydylo패키지를 개발해 가중로그승산비를 간단하게 계산할 수 있도록 했다. (Silge의 패키지 소개 글)","code":"\ninstall.packages(\"tidylo\")"},{"path":"tf-idf.html","id":"bind_log_oddstbl-set-feature-n-uninformative-false-unweighted-false","chapter":"6 .  단어 빈도수","heading":"6.4.1 bind_log_odds(tbl, set, feature, n, uninformative = FALSE, unweighted = FALSE)","text":"tbl: 정돈데이터(feature와 set이 하나의 행에 저장).tbl: 정돈데이터(feature와 set이 하나의 행에 저장).set: feature를 비교하기 위한 set(group)에 대한 정보(예: 긍정 vs. 부정)이 저장된 열.set: feature를 비교하기 위한 set(group)에 대한 정보(예: 긍정 vs. 부정)이 저장된 열.feature: feature(단어나 바이그램 등의 텍스트자료)가 저장된 열.feature: feature(단어나 바이그램 등의 텍스트자료)가 저장된 열.n: feature-set의 빈도를 저장한 열.n: feature-set의 빈도를 저장한 열.uninformative: uninformative 디리슐레 분포 사용 여부. 기본값은 FALSE.uninformative: uninformative 디리슐레 분포 사용 여부. 기본값은 FALSE.unweighted: 비가중 로그승산 사용여부. 기본값은 FALSE. TRUE로 지정하면 비가중 로그승산비(log_odds) 열을 추가한다.unweighted: 비가중 로그승산 사용여부. 기본값은 FALSE. TRUE로 지정하면 비가중 로그승산비(log_odds) 열을 추가한다.가중로그승산비를 이용해 네이버영화 댓글 중 긍정평과 부정평에 사용된 단어의 상대빈도를 구해보자.긍정평과 부정평별 가중로그승산비 상위 10개 추출한다.막대도표로 시각화한다.부정평으로 분류된 내용 중 ’최고’와 ’감동’이 부정평에 들어 있다. 어떤 문장인지 확인해보자.‘억지로 끼워 맞추려는 감동 설정’ ‘감동 없음’ 등 단어 하나만을 토큰으로 사용했을 때의 한계를 잘 보여주는 사례.’감동적 그자체 영화’를 부정평으로 분류한 것은 라벨링 오류.","code":"\nlibrary(tidylo)\n\nweighted_log_odds_df <- \nby1000_df %>% \n  unnest_tokens(word, document, token = pos) %>% \n  separate(word, c(\"word\", \"pos\"), sep = \"/\") %>% \n  filter(pos == \"nng\") %>% \n  filter(word != \"영화\") %>% \n  count(word) %>% \n  bind_log_odds(set = label,\n                feature = word,\n                n = n) %>% \n  arrange(-log_odds_weighted)\n\nweighted_log_odds_df## # A tibble: 2,831 × 4\n## # Groups:   label [2]\n##   label word      n log_odds_weighted\n##   <dbl> <chr> <int>             <dbl>\n## 1     0 짜증     24              3.18\n## 2     0 최악     24              3.18\n## 3     1 굿       18              3.16\n## 4     1 최고     83              3.06\n## 5     1 밋       16              2.98\n## 6     1 ㄷ       15              2.88\n## # … with 2,825 more rows\n\nweighted_log_odds_df %>%   \n  group_by(label = ifelse(label > 0, \"긍정평\", \"부정평\")) %>% \n  slice_max(abs(log_odds_weighted), n = 10) # 긍정평과 부정평별 각각 상위 10개 ## # A tibble: 20 × 4\n## # Groups:   label [2]\n##   label  word      n log_odds_weighted\n##   <chr>  <chr> <int>             <dbl>\n## 1 긍정평 굿       18              3.16\n## 2 긍정평 최고     83              3.06\n## 3 긍정평 밋       16              2.98\n## 4 긍정평 ㄷ       15              2.88\n## 5 긍정평 삶       10              2.35\n## 6 긍정평 감동     58              2.28\n## # … with 14 more rows\n\nweighted_log_odds_df %>%   \n  group_by(label = ifelse(label > 0, \"긍정평\", \"부정평\")) %>% \n  slice_max(abs(log_odds_weighted), n = 10) %>%  # 긍정평과 부정평별 각각 상위 10개 \n  \n  ggplot(aes(x = log_odds_weighted,\n             y = reorder(word, log_odds_weighted),\n             fill = label)) +\n  geom_col(show.legend = F) +\n  facet_wrap(~label, scale = \"free\")\nby1000_df %>% \n  unnest_tokens(word, document, token = pos, drop = F) %>% \n  separate(word, c(\"word\", \"pos\"), sep = \"/\") %>% \n  filter(pos == \"nng\") %>% \n  filter(word != \"영화\") %>% \n  filter(label == 0) %>% \n  filter(word == \"최고\" | word == \"감동\")## # A tibble: 19 × 4\n## # Groups:   label [1]\n##   label document                                word  pos  \n##   <dbl> <chr>                                   <chr> <chr>\n## 1     0 \"액션이 부족한 가운데 이야기 자체가 허… 감동  nng  \n## 2     0 \"액션이 부족한 가운데 이야기 자체가 허… 감동  nng  \n## 3     0 \"액션이 부족한 가운데 이야기 자체가 허… 최고  nng  \n## 4     0 \"액션이 부족한 가운데 이야기 자체가 허… 최고  nng  \n## 5     0 \"액션이 부족한 가운데 이야기 자체가 허… 최고  nng  \n## 6     0 \"액션이 부족한 가운데 이야기 자체가 허… 최고  nng  \n## # … with 13 more rows\n"},{"path":"tf-idf.html","id":"지표-비교","chapter":"6 .  단어 빈도수","heading":"6.4.2 지표 비교","text":"","code":""},{"path":"tf-idf.html","id":"로그승산비-1","chapter":"6 .  단어 빈도수","heading":"6.4.2.1 로그승산비","text":"먼저 가중치를 사용하지않은 로그승산비를 구한다.","code":"\nrate_log_df  %>% # 승산비에 로그를 취한다.\n  group_by(label  = ifelse(log_odds_ratio > 0, \"긍정평\", \"부정평\")) %>% \n  slice_max(abs(log_odds_ratio), n = 10) %>%  # 긍정평과 부정평별 각각 상위 10개 \n  \n  ggplot(aes(x = log_odds_ratio,\n             y = reorder(word, log_odds_ratio),\n             fill = label)) +\n  geom_col(show.legend = F) +\n  facet_wrap(~label, scale = \"free\")"},{"path":"tf-idf.html","id":"tf_idf-3","chapter":"6 .  단어 빈도수","heading":"6.4.2.2 tf_idf","text":"tf_idf 계산긍정평과 부정평별 상위 10개 추출한다.막대도표로 시각화한다.","code":"\ntf_idf_df <- \nby1000_df %>% \n  unnest_tokens(word, document, token = pos) %>% \n  separate(word, c(\"word\", \"pos\"), sep = \"/\") %>% \n  filter(pos == \"nng\") %>% \n  filter(word != \"영화\") %>% \n  count(word) %>% \n  bind_tf_idf(term = word,\n              document = label,\n              n = n)\ntf_idf_df## # A tibble: 2,831 × 6\n## # Groups:   label [2]\n##   label word      n       tf   idf   tf_idf\n##   <dbl> <chr> <int>    <dbl> <dbl>    <dbl>\n## 1     0 가가      1 0.000271 0.693 0.000188\n## 2     0 가관      1 0.000271 0.693 0.000188\n## 3     0 가능      1 0.000271 0     0       \n## 4     0 가든      1 0.000271 0.693 0.000188\n## 5     0 가리      1 0.000271 0.693 0.000188\n## 6     0 가문      1 0.000271 0.693 0.000188\n## # … with 2,825 more rows\n\ntf_idf_df %>% \n  group_by(label = ifelse(label > 0, \"긍정평\", \"부정평\")) %>% \n  slice_max(tf_idf, n = 10) # 긍정평과 부정평별 각각 상위 10개 ## # A tibble: 22 × 6\n## # Groups:   label [2]\n##   label  word      n      tf   idf  tf_idf\n##   <chr>  <chr> <int>   <dbl> <dbl>   <dbl>\n## 1 긍정평 굿       18 0.00558 0.693 0.00387\n## 2 긍정평 밋       16 0.00496 0.693 0.00344\n## 3 긍정평 ㄷ       15 0.00465 0.693 0.00322\n## 4 긍정평 삶       10 0.00310 0.693 0.00215\n## 5 긍정평 마음      9 0.00279 0.693 0.00193\n## 6 긍정평 만족      8 0.00248 0.693 0.00172\n## # … with 16 more rows\n\ntf_idf_df %>% \n  group_by(label = ifelse(label > 0, \"긍정평\", \"부정평\")) %>% \n  slice_max(tf_idf, n = 10) %>% # 긍정평과 부정평별 각각 상위 10개 \n  \n  ggplot(aes(x = tf_idf,\n             y = reorder(word, tf_idf),\n             fill = label)) +\n  geom_col(show.legend = F) +\n  facet_wrap(~label, scale = \"free\")"},{"path":"tf-idf.html","id":"비교-2","chapter":"6 .  단어 빈도수","heading":"6.4.2.3 비교","text":"가중로그승산비, 로그승산비, tf_idf의 값을 비교해보자. 먼저 3개 데이터프레임을 행방향으로 결합해 하나의 데이터프레임으로 저장한다.","code":"\nwlo_df <- \nweighted_log_odds_df %>%   \n  group_by(label = ifelse(label > 0, \"긍정평\", \"부정평\")) %>% \n  slice_max(abs(log_odds_weighted), n = 10) %>% \n  select(label, word, score = log_odds_weighted)\n\nrlo_df <- \nrate_log_df  %>% \n  group_by(label  = ifelse(log_odds_ratio > 0, \"긍정평\", \"부정평\")) %>% \n  slice_max(abs(log_odds_ratio), n = 10) %>% \n  select(label, word, score = log_odds_ratio) \n\nti_df <- \ntf_idf_df %>% \n  group_by(label = ifelse(label > 0, \"긍정평\", \"부정평\")) %>% \n  slice_max(tf_idf, n = 10) %>% \n  select(label, word, score = tf_idf) %>% \n  mutate(score = score * 600)\n\nbind_rows(wlo_df, rlo_df, ti_df, .id = \"ID\") %>% tail(20)## # A tibble: 20 × 4\n## # Groups:   label [2]\n##   ID    label  word  score\n##   <chr> <chr>  <chr> <dbl>\n## 1 3     긍정평 ㄷ    1.93 \n## 2 3     긍정평 삶    1.29 \n## 3 3     긍정평 마음  1.16 \n## 4 3     긍정평 만족  1.03 \n## 5 3     긍정평 감정  0.902\n## 6 3     긍정평 행복  0.902\n## # … with 14 more rows\n\nbind_rows(wlo_df, rlo_df, ti_df, .id = \"ID\") %>% \n  mutate(ID = ifelse(ID == \"1\", \"1.가중로그승산비\", \n                     ifelse(ID == \"2\", \"2.로그승산비\", \"3.tf_idf\"))\n         ) %>% \n  \n  ggplot(aes(x = score,\n             y = reorder(word, score),\n             fill = label)\n         ) +\n  geom_col(show.legend = F) +\n  facet_wrap( ~ label+ID, scales = \"free\", ncol = 3)"},{"path":"tf-idf.html","id":"연습-3","chapter":"6 .  단어 빈도수","heading":"6.5 연습","text":"’백신’관련 보도의 내용을 빅카인즈의 자료를 이용해 탐색해보자.","code":""},{"path":"tf-idf.html","id":"빅카인즈","chapter":"6 .  단어 빈도수","heading":"6.5.1 빅카인즈","text":"빅카인즈는 언론진흥재단이 운영하는 뉴스빅데이터 분석서비스다. 종합일간지, 경제지, 지역일간지, 방송사 등의 기사를 분석할수 인터페이스를 제공한다. 빅카인즈인터페이스를 통해\n빅카인즈에서 제공하는 인터페이스으로 뉴스텍스트를 분석할 수 있다. 관계도, 키워드트렌드, 연관어분석 등이 가능하다. 사용자가 직접 분석할 수 있도록 데이터다운로드 서비스도 제공한다. 무료데이터는 일자, 언론사, 기고자, 제목, 분류, 인물, 위치, 본문(200자), 기사ULR 등이 포함돼 있다.데이터다운로도 방법:빅카인즈 접솝상단 메뉴의 ‘뉴스분석’ 클릭‘뉴스분석’ 메뉴가 아래로 펼쳐지면 ‘뉴스·검색 분석’ 클릭.‘STEP01·뉴스검색’ 창에서 다운로드할 기사의 범위 설정.‘STEP03·분석결과 및 시각화’를 클릭하면 ’데이터다운로드’ 메뉴가 나온다.오른쪽 하단의 ‘엑셀다운로드’ 클릭.","code":""},{"path":"tf-idf.html","id":"백신관련-보도-수집","chapter":"6 .  단어 빈도수","heading":"6.5.1.1 백신관련 보도 수집","text":"모든 기사를 분석하기 위해서는 시간이 많이 걸리므로, 데이터셋의 크기를 줄이기 위해, 분석 대상을 2021년 2월 한달간 ‘경향신문’ ‘한겨레’ ‘문화일보’ ‘조선일보’ 등 4개 일간지에서 보도된 내용에 국한하자.4. 'STEP01·뉴스검색' 창에서 다운로드할 기사의 범위 설정검색어: ‘백신’기간: 2021년 2월 1일 ~ 2월 28일통합분류: ‘사회’, ‘IT_과학’언론사: ‘경향신문’ ‘한겨레’ ‘문화일보’ ‘조선일보’ 선택작업디렉토리 아래 data폴더에 다운로드 받는다.파일명을 확인한다. NewsResult_20210201-20210228.xlsx다.readxl패키지의 read_excel()함수로 해당 파일을 R환경으로 이입한다.분석에 활용할 열만 추출해 vac_df에 할당한다.","code":"\nlist.files(\"data/.\")\nreadxl::read_excel(\"data/NewsResult_20210201-20210228.xlsx\") %>% glimpse()\nvac_df <- \n  readxl::read_excel(\"data/NewsResult_20210201-20210228.xlsx\") %>% \n  select(제목, 언론사, 본문, URL)\n\nvac_df %>% glimpse()## Rows: 602\n## Columns: 4\n## $ 제목   <chr> \"백신 접종 3일째 ‘큰 탈’ 없어 곳곳서 방역…\n## $ 언론사 <chr> \"경향신문\", \"경향신문\", \"경향신문\", \"경향신…\n## $ 본문   <chr> \"이틀간 2만322명에 접종 마쳐 중증 이상반응 …\n## $ URL    <chr> \"http://news.khan.co.kr/kh_news/khan_art_vi…\n"},{"path":"tf-idf.html","id":"정제","chapter":"6 .  단어 빈도수","heading":"6.5.2 정제","text":"먼저 drop_na()함수로 결측값을 제거하고, 토큰화한 다음, pos함수로 일반명사만 추출해, vac_tk로 저장한다.","code":"\nvac_tk <- \nvac_df %>% \n  drop_na() %>% \n  unnest_tokens(word, 제목, token = pos) %>% \n  separate(word, c(\"word\", \"pos\"), sep = \"/\") %>% \n  filter(pos == 'nng')\n\nvac_tk## # A tibble: 3,570 × 5\n##   언론사   본문                           URL    word  pos  \n##   <chr>    <chr>                          <chr>  <chr> <chr>\n## 1 경향신문 \"이틀간 2만322명에 접종 마쳐 … http:… 백신  nng  \n## 2 경향신문 \"이틀간 2만322명에 접종 마쳐 … http:… 접종  nng  \n## 3 경향신문 \"이틀간 2만322명에 접종 마쳐 … http:… 탈    nng  \n## 4 경향신문 \"이틀간 2만322명에 접종 마쳐 … http:… 곳곳  nng  \n## 5 경향신문 \"이틀간 2만322명에 접종 마쳐 … http:… 방역  nng  \n## 6 경향신문 \"이틀간 2만322명에 접종 마쳐 … http:… 해이  nng  \n## # … with 3,564 more rows\n"},{"path":"tf-idf.html","id":"분석-및-소통","chapter":"6 .  단어 빈도수","heading":"6.5.3 분석 및 소통","text":"총빈도, 감정어빈도, tf_idf, 로그승산비, 가중로그승산비 등 다양한 분석 방법을 배웠다. 총빈도와 가중로그승산비를 구해 언론사별 어떤 차이가 있는지 살펴보자. (분석 결과를 해석할 때는 2021년 2월 한달 기간에 국한한 자료를 이용한 점에 주의해야 한다.)","code":""},{"path":"tf-idf.html","id":"언론사별-총빈도","chapter":"6 .  단어 빈도수","heading":"6.5.3.1 언론사별 총빈도","text":"주요 단어가 사용된 내용(제목)을 살펴보자.’카’의 빈도가 높았던 이유는 ’아스트라제네카’를 형태소분석기가 하나의 일반명사로 추출하지 않았기 때문이다.","code":"\nvac_tk %>% \n  count(언론사, word, sort = T) %>% \n  filter(word != \"경향\") %>% \n  filter(word != \"포토\") %>% \n  filter(word != \"문화\") %>% \n  filter(word != \"조선\") %>% \n  filter(word != \"한겨레\") %>% \n  filter(word != \"백신\") %>%\n  filter(word != \"접종\") %>% \n  group_by(언론사) %>% \n  slice_max(n, n = 7) %>% \n  \n  ggplot(aes(x = n,\n             y = reorder(word, n),\n             fill = 언론사)\n         ) +\n  geom_col(show.legend = F) +\n  facet_wrap( ~ 언론사, scales = \"free\")\nvac_df %>% \n  drop_na() %>% \n  unnest_tokens(word, 제목, token = pos, drop = F) %>% \n  separate(word, c(\"word\", \"pos\"), sep = \"/\") %>% \n  filter(pos == 'nng') %>% \n  filter(word == \"카\") %>% .$제목##  [1] \"“독일, 65세 이상 아스트라제네카 백신 접종 곧 허가할 듯”\"                            \n##  [2] \"대통령은 아스트라제네카, 총리는 화이자 백신 접종 참관\"                                \n##  [3] \"접종 D-1  정세균 총리 “아스트라제네카 백신 의구심 근거 없어”\"                       \n##  [4] \"인천시, 26일부터 아스트라제네카 백신 접종 시작\"                                       \n##  [5] \"아스트라제네카 26일 화이자 27일 접종 시작\"                                            \n##  [6] \"‘스푸트니크-아스트라제네카 결합, 접종 기간 단축’\"                                   \n##  [7] \"아스트라제네카 백신 접종 '동의한다' 93.8%\"                                            \n##  [8] \"아스트라제네카 백신 국가출하승인 화이자는 3월초 허가 전망\"                            \n##  [9] \"호주, 아스트라제네카 백신 승인 “65세 이상도 사용 가능”\"                             \n## [10] \"WHO, 아스트라제네카 백신 승인\"                                                        \n## [11] \"[2 3월 백신접종]정부 \\\"아스트라제네카 백신 만 65세 이상 접종 보류\\\"\"                  \n## [12] \"아스트라제네카, 65살 이상 고령자 접종 연기 3월말 이후 결정\"                           \n## [13] \"아스트라제네카, 유럽 주요국도 65세 이상에겐 접종 제한\"                                \n## [14] \"아스트라제네카 백신 고령 접종여부 오늘 발표\"                                          \n## [15] \"아스트라제네카(AZ) 백신 고령층 접종 금지한 스웨덴도 \\\"AZ 밖에 없으면 당연히 맞혀야\\\"\" \n## [16] \"WHO 자문단 “아스트라제네카 백신, 65세 이상에게도 권고”\"                             \n## [17] \"아스트라제네카 백신 ‘만 65세 이상도 맞나’...16일 발표\"                              \n## [18] \"아스트라제네카 ‘모든 연령층 접종’ 허가 ‘65살 이상’은 의사가 판단\"                 \n## [19] \"아스트라제네카 백신, 만 65세 고령층에도 접종 허가  식약처 \\\"현장에서 의사가 판단\\\"\"   \n## [20] \"아스트라제네카 백신 허가... 만 65세 이상 접종여부는 ‘불투명'\"                        \n## [21] \"식약처, 아스트라제네카 백신 사용 허가 65살 이상도 포함\"                               \n## [22] \"질병청 ”아스트라제네카 백신 26일부터 접종 시작”\"                                    \n## [23] \"26일 아스트라제네카 백신 접종 시작 요양병원 시설 먼저\"                                \n## [24] \"“아스트라제네카 백신 75만명분, 24일부터 받아 곧바로 접종”\"                          \n## [25] \"남아공, 아스트라제네카 백신 접종 보류 변이 바이러스에 효과 낮아\"                      \n## [26] \"스위스 이어 남아공도 아스트라제네카 백신 접종 보류\"                                   \n## [27] \"“아스트라제네카 백신, 노인 접종에 신중해야”\"                                        \n## [28] \"식약처 “아스트라제네카 만 65세 접종 신중” 질병청에 공 넘겨\"                         \n## [29] \"노인에 대한 아스트라제네카 백신 투여, 최종 결정은 질병청에 넘겨\"                      \n## [30] \"식약처 “아스트라제네카, 65살 이상 접종 신중하게”\"                                   \n## [31] \"불투명해진 아스트라제네카 백신 고령층 접종  중앙약심위 \\\"만 65세 이상 접종 신중해야\\\"\"\n## [32] \"아스트라제네카 고령층 접종 제한하는 유럽 스위스는 “승인 보류”\"                      \n## [33] \"아스트라제네카 고령층 백신 효과 두고 유럽 시끌, 스위스는 승인 보류\"                   \n## [34] \"아스트라제네카 백신 논란 지속 스위스, 승인 보류\"                                      \n## [35] \"스위스, 아스트라제네카 백신 승인 거부\"                                                \n## [36] \"유럽 7개국, 아스트라제네카 고령자 접종 제한 우리나라는?\"                              \n## [37] \"프랑스, 스웨덴도 아스트라제네카 백신 65세 미만에만 접종 권고\"                         \n## [38] \"[만물상] 말 많고 탈 많은 아스트라제네카\"                                              \n## [39] \"아스트라제네카 접종, 65세 이상 고령층 포함 가능성 커졌다\"                             \n## [40] \"아스트라제네카 백신 ‘무용론’ 딛고 65세 이상도 접종할 듯\"                            \n## [41] \"[속보] 아스트라제네카 백신 예방효과 62% “고령층 접종 배제할 필요 없다”\"             \n## [42] \"“아스트라제네카, 고령층 투여 배제할 수 없어” 1차 자문 결과\"                         \n## [43] \"아스트라제네카, 조건부로 허용되나?\"\n\nenc2utf8(\"아스트라제네카 ‘모든 연령층 접종’ 허가 ‘65살 이상’은 의사가 판단\") %>% pos()## $`아스트라제네카 ‘모든 연령층 접종’ 허가 ‘65살 이상’은 의사가 판단`\n##  [1] \"아스트라/NNP\" \"제/NP\"        \"네/XSN\"      \n##  [4] \"카/NNG\"       \"‘/SY\"        \"모든/MM\"     \n##  [7] \"연령층/NNG\"   \"접종/NNG\"     \"’/SY\"       \n## [10] \"허가/NNG\"     \"‘/SY\"        \"65/SN\"       \n## [13] \"살/NNBC\"      \"이상/NNG\"     \"’/SY\"       \n## [16] \"은/JX\"        \"의사/NNG\"     \"가/JKS\"      \n## [19] \"판단/NNG\"\n\nvac_df %>% \n  drop_na() %>% \n  unnest_tokens(word, 제목, token = pos, drop = F) %>% \n  separate(word, c(\"word\", \"pos\"), sep = \"/\") %>% \n  filter(pos == 'nng') %>% \n  filter(word == \"이상\") %>% .$제목##  [1] \"독일, \\\"65세 이상 AZ백신 접종 검토\\\"\"                                                 \n##  [2] \"27일 백신 이상반응 97건 두통 발열 등 모두 경증\"                                       \n##  [3] \"“독일, 65세 이상 아스트라제네카 백신 접종 곧 허가할 듯”\"                            \n##  [4] \"[속보] 코로나 백신접종 첫날 이상반응 15건 두통 발열 구토 등 경증\"                     \n##  [5] \"백신 맞은 인천 간호사 2명도 이상증세... 숨차고 혈압 올라 병원행\"                      \n##  [6] \"포항서 아스트라 첫 이상증세 고혈압 어지럼증에 응급실 이송\"                            \n##  [7] \"백신 맞았다면 최소 15분이상 대기 ‘이상반응’ 관찰해야\"                               \n##  [8] \"백신 맞았다면 최소 15분이상 대기 ‘이상반응’ 관찰해야\"                               \n##  [9] \"중앙약심 “화이자 백신, 16세 이상 대상으로 접종 허가 타당”\"                          \n## [10] \"2차 자문에서도 화이자 백신 \\\"예방효과 안전성 충분... 16세 이상 허가 권고\\\"\"           \n## [11] \"서울 65세 이상 성인은 4월이후 코로나 백신 맞는다\"                                     \n## [12] \"시민 10명 중 7명 \\\"금고형 이상 범죄, 의사면허 취소 찬성\\\"\"                            \n## [13] \"“아스트라, 65세 이상도 입원 위험 80% 감소”\"                                         \n## [14] \"정 총리 “65세 이상 고령층에 화이자 백신 먼저 접종 가능성”\"                          \n## [15] \"식약처 “화이자 백신 16살 이상 사용 타당”\"                                           \n## [16] \"식약처 전문가 검증 자문단 “화이자 백신, 16살 이상 사용 허가 타당”\"                  \n## [17] \"식약처 자문단 “화이자 백신, 예방 효과 충분 16세 이상 접종도 타당”\"                  \n## [18] \"“화이자 백신 예방효과 충분  16세 이상 허가 타당”\"                                   \n## [19] \"졸업사진 찍을땐 5명 이상도 가능\"                                                      \n## [20] \"호주, 아스트라제네카 백신 승인 “65세 이상도 사용 가능”\"                             \n## [21] \"65세 이상, 아스트라 백신 접종 연기\"                                                   \n## [22] \"아스트라 백신 접종, 65살 이상 일단 보류 3월말께 재결정\"                               \n## [23] \"[2 3월 백신접종]정부 \\\"아스트라제네카 백신 만 65세 이상 접종 보류\\\"\"                  \n## [24] \"정은경 “65세 이상 접종 미루게 돼 안타까워”\"                                         \n## [25] \"아스트라제네카, 65살 이상 고령자 접종 연기 3월말 이후 결정\"                           \n## [26] \"아스트라제네카, 유럽 주요국도 65세 이상에겐 접종 제한\"                                \n## [27] \"AZ백신 ‘65세이상 접종’ 일단 후순위로\"                                               \n## [28] \"백신접종 계획 내일 발표 65세 이상도 아스트라 맞나\"                                    \n## [29] \"수도권 영업 오후10시까지, 5인 이상 모임 금지는 유지\"                                  \n## [30] \"WHO 자문단 “아스트라제네카 백신, 65세 이상에게도 권고”\"                             \n## [31] \"아스트라제네카 백신 ‘만 65세 이상도 맞나’...16일 발표\"                              \n## [32] \"아스트라제네카 ‘모든 연령층 접종’ 허가 ‘65살 이상’은 의사가 판단\"                 \n## [33] \"아스트라제네카 백신 허가... 만 65세 이상 접종여부는 ‘불투명'\"                        \n## [34] \"식약처, 아스트라제네카 백신 사용 허가 65살 이상도 포함\"                               \n## [35] \"65세 이상 고령층 대신 경찰 군인부터 백신 접종순위 바뀔 가능성\"                        \n## [36] \"식약처 “아스트라제네카, 65살 이상 접종 신중하게”\"                                   \n## [37] \"불투명해진 아스트라제네카 백신 고령층 접종  중앙약심위 \\\"만 65세 이상 접종 신중해야\\\"\"\n## [38] \"아스트라 백신 65세이상 접종 결정보류 일정 차질 불가피\"                                \n## [39] \"식약처 자문단 “화이자 백신, 만 16세 이상도 접종 타당”\"                              \n## [40] \"아스트라제네카 접종, 65세 이상 고령층 포함 가능성 커졌다\"                             \n## [41] \"의협회장 “만65세이상은 아스트라 백신 안돼, 화이자 모더나 접종을”\"                   \n## [42] \"아스트라제네카 백신 ‘무용론’ 딛고 65세 이상도 접종할 듯\"                            \n## [43] \"식약처 자문단, 아스트라 65세 이상 접종 결론 못내...다수 의견은 “가능”\"              \n## [44] \"단속 힘들고 불편만 ‘5인 이상 가족모임 금지’ 유지 논란\"                              \n## [45] \"[사이언스카페] 코로나 백신 2회중 1회만 맞아도 감염 전염력 60% 이상 떨어뜨려\"          \n## [46] \"설 차례, 직계가족이라도 주소 다르면 5인이상 금지\"\n"},{"path":"tf-idf.html","id":"언론사별-가중로그승산비","chapter":"6 .  단어 빈도수","heading":"6.5.3.2 언론사별 가중로그승산비","text":"주요 단어가 사용된 내용(제목)을 살펴보자.","code":"\nvac_tk %>% \n  count(언론사, word) %>% \n  filter(word != \"경향\") %>% \n  filter(word != \"포토\") %>% \n  filter(word != \"문화\") %>% \n  filter(word != \"조선\") %>% \n  filter(word != \"한겨레\") %>%\n  filter(word != \"백신\") %>%\n  filter(word != \"접종\") %>% \n  bind_log_odds(set = 언론사,\n                feature = word,\n                n = n) %>% \n  group_by(언론사) %>% \n  slice_max(abs(log_odds_weighted), n = 7) %>% \n  \n  ggplot(aes(x = log_odds_weighted,\n             y = reorder(word, log_odds_weighted),\n             fill = 언론사)\n         ) +\n  geom_col(show.legend = F) +\n  facet_wrap( ~ 언론사, scales = \"free\")\nvac_df %>% \n  drop_na() %>% \n  unnest_tokens(word, 제목, token = pos, drop = F) %>% \n  separate(word, c(\"word\", \"pos\"), sep = \"/\") %>% \n  filter(pos == 'nng') %>% \n  filter(언론사 == \"경향신문\") %>% \n  filter(word == \"현장\") %>% .$제목## [1] \"문 대통령, 접종 현장 방문 후 “일상 회복 머지않았다”\"                             \n## [2] \"'긴장 탓 울렁' 했지만 \\\"백신 접종 기쁠 뿐\\\"  첫 날 백신접종 현장은\"                \n## [3] \"국내 첫 백신 접종 시작 문 대통령 접종 현장 참관\"                                   \n## [4] \"아스트라제네카 백신, 만 65세 고령층에도 접종 허가  식약처 \\\"현장에서 의사가 판단\\\"\"\n## [5] \"[오늘은 이런 경향] 2월 9일 정 총리 답신 한달 지냈지만...현장 간호사들은...\"\n\nvac_df %>% \n  drop_na() %>% \n  unnest_tokens(word, 제목, token = pos, drop = F) %>% \n  separate(word, c(\"word\", \"pos\"), sep = \"/\") %>% \n  filter(pos == 'nng') %>% \n  filter(언론사 == \"문화일보\") %>% \n  filter(word == \"확정\") %>% .$제목## [1] \"백신 접종 D-7 명단 확정 65세미만 27만명으로 줄어\"       \n## [2] \"국내 1호 AZ백신 첫 접종 대상은 누구? 19일까지 명단 확정\"\n\nvac_df %>% \n  drop_na() %>% \n  unnest_tokens(word, 제목, token = pos, drop = F) %>% \n  separate(word, c(\"word\", \"pos\"), sep = \"/\") %>% \n  filter(pos == 'nng') %>% \n  filter(언론사 == \"조선일보\") %>% \n  filter(word == \"국민\") %>% .$제목## [1] \"文 “백신 관리 접종 과정, 국민 신뢰 주기에 충분”\"                    \n## [2] \"국민 열에 일곱은 “코로나 백신 접종할 것”\"                           \n## [3] \"백신 불안감 여전 국민 30% “안 맞거나 미룰래요”\"                     \n## [4] \"국민 41%가 백신 맞았다, 이스라엘 제치고 세계 1위된 이 나라\"           \n## [5] \"“과학자 입 막으면 국민 건강권 침해” 기자협회 과학기자협회 공동 발표\"\n\nvac_df %>% \n  drop_na() %>% \n  unnest_tokens(word, 제목, token = pos, drop = F) %>% \n  separate(word, c(\"word\", \"pos\"), sep = \"/\") %>% \n  filter(pos == 'nng') %>% \n  filter(언론사 == \"한겨레\") %>% \n  filter(word == \"뒤\") %>% .$제목## [1] \"백신 ‘1호 접종’은 야간근무 뒤 일찍 온 요양보호사\"            \n## [2] \"통합관제센터서 “온도 벗어났다” 제주행 백신 전량 회수 뒤 교체\"\n## [3] \"백신접종 뒤 입원 위험, 아스트라가 화이자보다 낮았다\"           \n## [4] \"예진 통과하면 간호사 4명 배치된 주사실로 접종 뒤 15~30분 관찰\"\n"},{"path":"tf-idf.html","id":"총빈도와-상대빈도-비교","chapter":"6 .  단어 빈도수","heading":"6.5.4 총빈도와 상대빈도 비교","text":"언론사별로 각 기사에서 사용된 단어의 총빈도와 상대빈도를 비교하자.먼저 총빈도를 구한다.상대빈도총빈도와 상대빈도 데이터프레임을 행방향으로 결합한다.전반적으로 4개 언론사는 백신과 관련하여 2021년 2월 한달간 ‘이상’(특정 연령 이상)과 아스트라제네카 백신에 대한 내용을 주로 다뤘다.언론사별로는 경향신문은 ‘오늘’과 ’현장’, 문화일보는 ‘미국’과 ’누적’, 조선일보는 ‘국민’과 ’속도’, 한겨레는 ‘뒤’와 ’팔’ 등에 관련된 기사를 다른 언론사에 비해 상대적으로 더 다뤘다.","code":"\ntotal_fq <- \nvac_tk %>% \n  count(언론사, word, sort = T) %>% \n  filter(word != \"경향\") %>% \n  filter(word != \"포토\") %>% \n  filter(word != \"문화\") %>% \n  filter(word != \"조선\") %>% \n  filter(word != \"한겨레\") %>% \n  filter(word != \"백신\") %>%\n  filter(word != \"접종\") %>% \n  group_by(언론사) %>% \n  slice_max(n, n = 7) \n\ntotal_fq## # A tibble: 32 × 3\n## # Groups:   언론사 [4]\n##   언론사   word      n\n##   <chr>    <chr> <int>\n## 1 경향신문 오늘     19\n## 2 경향신문 카       14\n## 3 경향신문 시작     13\n## 4 경향신문 집단     13\n## 5 경향신문 고령     10\n## 6 경향신문 이상     10\n## # … with 26 more rows\n\nwlo_fq <- \nvac_tk %>% \n  count(언론사, word) %>% \n  filter(word != \"경향\") %>% \n  filter(word != \"포토\") %>% \n  filter(word != \"문화\") %>% \n  filter(word != \"조선\") %>% \n  filter(word != \"한겨레\") %>%\n  filter(word != \"백신\") %>%\n  filter(word != \"접종\") %>% \n  bind_log_odds(set = 언론사,\n                feature = word,\n                n = n) %>% \n  group_by(언론사) %>% \n  slice_max(abs(log_odds_weighted), n = 7)\n\nwlo_fq ## # A tibble: 38 × 4\n## # Groups:   언론사 [4]\n##   언론사   word      n log_odds_weighted\n##   <chr>    <chr> <int>             <dbl>\n## 1 경향신문 현장      5              2.74\n## 2 경향신문 오늘     19              2.46\n## 3 경향신문 국산      4              2.45\n## 4 경향신문 발        3              2.12\n## 5 경향신문 사회      3              2.12\n## 6 경향신문 삶        3              2.12\n## # … with 32 more rows\n\nbind_rows(\n  select(total_fq, 언론사, word, score = n),\n  select(wlo_fq, 언론사, word, score = log_odds_weighted),\n  .id = \"ID\") %>% \n  mutate(ID = ifelse(ID == 1, \"total\", \"wlo\")) %>% \n  \n  ggplot(aes(x = score,\n             y = reorder(word, score),\n             fill = 언론사)\n         ) +\n  geom_col(show.legend = F) +\n  facet_wrap(~ID+언론사, scales = \"free\", ncol = 4)"},{"path":"tf-idf.html","id":"과제-2","chapter":"6 .  단어 빈도수","heading":"6.5.4.1 과제","text":"빅카인즈에서 언론사별로 관심 분야의 기사를 추출해 보도 내용을 분석할 수 있도록 시각화하시오.보도에 사용된 단어의 총빈도에 대해 선택한 언론사별 비교 시각화보도에 사용된 단어의 총빈도에 대해 선택한 언론사별 비교 시각화보도에 사용된 단어의 상대빈도에 대해 선택한 언론사별 비교 시각화보도에 사용된 단어의 상대빈도에 대해 선택한 언론사별 비교 시각화총빈도와 상대빈도에 대해 언론사별로 비교 시각화총빈도와 상대빈도에 대해 언론사별로 비교 시각화","code":""},{"path":"sentiment.html","id":"sentiment","chapter":"7 .  감성분석","heading":"7 .  감성분석","text":"","code":""},{"path":"sentiment.html","id":"anal1freq","chapter":"7 .  감성분석","heading":"7.1 감정어 빈도","text":"단어(term) - 문서(document) - 말뭉치(corpus)사전방식\n말뭉치에서 많이 사용된 일군의 단어집단의 빈도 계산. 감정사전을 이용하면 감정어 빈도를 구할 수 있다.\n말뭉치에서 많이 사용된 일군의 단어집단의 빈도 계산. 감정사전을 이용하면 감정어 빈도를 구할 수 있다.tf_idf\n말뭉치의 문서별로 중요하게 사용된 단어의 빈도 계산\n말뭉치의 문서별로 중요하게 사용된 단어의 빈도 계산주제모형(topic modeling)\n말뭉치에서 주제를 나타내는 단어의 분포 계산\n말뭉치에서 주제를 나타내는 단어의 분포 계산사전방식은 말뭉치에서 많이 사용된 일군의 단어를 사전으로 만들어 해당 단어 집단의 빈도 계산.감정사전\n각 단어를 감정에 따라 분류도덕기반사전\n각 단어를 도덕감정에 따라 분류LIWC(Linguistic Inquiry Word Count)\n일상에서 사용하는 단어를 통해 생각, 느낌 등의 심리 측정. 상용.","code":""},{"path":"sentiment.html","id":"국문-감정사전","chapter":"7 .  감성분석","heading":"7.2 국문 감정사전","text":"영문감정사전은 tidytext에 포함된 3종의 감정사전이 있다.KNU한국어감성사전군산대 소프트웨어융합공학과 Data Intelligence Lab에서 개발한 한국어감정사전이다. 표준국어대사전을 구성하는 각 단어의 뜻풀이를 분석하여 긍부정어를 추출했다.보편적인 기본 감정 표현을 나타내는 긍부정어로 구성된다. 보편적인 긍정 표현으로는 ‘감동받다’, ‘가치 있다’, ‘감사하다’와 보편적인 부정 표현으로는 ‘그저 그렇다’, ‘도저히 ~수 없다’, ‘열 받다’ 등이 있다. 이 사전을 토대로 각 영역(음식, 여행지 등)별 감정사전을 구축하는 기초 자료로 활용할 수 있다.KNU한국어감성사전은 다음 자료를 통합해 개발했다.국립국어원 표준국어대사전의 뜻풀이(glosses) 분석을 통한 긍부정 추출(이 방법을 통해 대부분의 긍부정어 추출)김은영(2004)의 긍부정어 목록SentiWordNet 및 SenticNet-5.0에서 주로 사용되는 긍부정어 번역최근 온라인에서 많이 사용되는 축약어 및 긍부정 이모티콘 목록위 자료를 통합해 총 1만4천여개의 1-gram, 2-gram, 관용구, 문형, 축약어, 이모티콘 등에 대한 긍정, 중립, 부정 판별 및 정도(degree)의 값 계산했다.","code":""},{"path":"sentiment.html","id":"사전-데이터프레임-만들기","chapter":"7 .  감성분석","heading":"7.2.1 사전 데이터프레임 만들기","text":"KNU한국어감정사전을 다운로드 받아 압축을 풀어 데이터프레임 “senti_dic_df”에 할당하자.먼저 파일을 data폴더에 다운로드해 knusenti.zip파일로 저장한다.다운로드받은 knusenti.zip파일을 압축해제해 필요한 사전자료의 위치를 파악한다.data/KnuSentiLex-master 폴더에 있는 파일의 종류를 탐색한다.SentiWord_Dict.txt가 사전내용이 들어있는 파일이다. 9번째 있으므로, 이를 선택해 R환경으로 이입하자.개별 요소와 요소 사이가 공백문자로 구분돼 있다.요소를 구분한 공백문자는 탭(\\t)이다. read_tsv로 이입해 내용을 검토하자.senti_dic_df에 할당하자.열의 이름을 내용에 맞게 변경하자. 감정단어의 열은 word로, 감정점수의 열은 sScore로 변경.이제 KNU한국어감성사전을 R환경에서 사용할 수 있다.","code":"\npkg_v <- c(\"tidyverse\", \"tidytext\", \"epubr\", \"RcppMeCab\")\nlapply(pkg_v, require, ch = T)## [[1]]\n## [1] TRUE\n## \n## [[2]]\n## [1] TRUE\n## \n## [[3]]\n## [1] TRUE\n## \n## [[4]]\n## [1] TRUE\n\nurl_v <- \"https://github.com/park1200656/KnuSentiLex/archive/refs/heads/master.zip\"\ndest_v <- \"data/knusenti.zip\"\n\ndownload.file(url = url_v, \n              destfile = dest_v,\n              mode = \"wb\")\n\nlist.files(\"data/.\")\nunzip(\"knusenti.zip\", exdir = \"data\")\nlist.files(\"data/.\")\nlist.files(\"data/KnuSentiLex-master/.\")## [1] \"data\"                     \"KnuSentiLex\"             \n## [3] \"knusl.py\"                 \"neg_pol_word.txt\"        \n## [5] \"obj_unknown_pol_word.txt\" \"pos_pol_word.txt\"        \n## [7] \"README.md\"                \"ReadMe.txt\"              \n## [9] \"SentiWord_Dict.txt\"\n\nsenti_name_v <- list.files(\"data/KnuSentiLex-master/.\")[9]\nsenti_name_v## [1] \"SentiWord_Dict.txt\"\n\nread_lines(str_c(\"data/KnuSentiLex-master/\", senti_name_v)) %>% head(10)##  [1] \"(-;\\t1\"    \"(;_;)\\t-1\" \"(^^)\\t1\"   \"(^-^)\\t1\" \n##  [5] \"(^^*\\t1\"   \"(^_^)\\t1\"  \"(^_^;\\t-1\" \"(^o^)\\t1\" \n##  [9] \"(-_-)\\t-1\" \"(T_T)\\t-1\"\n\nread_lines(str_c(\"data/KnuSentiLex-master/\", senti_name_v)) %>% \n  head(10) %>% str_extract(\"\\t|\\n| \")\nread_tsv(str_c(\"data/KnuSentiLex-master/\", senti_name_v)) %>% head(10)\nread_tsv(str_c(\"data/KnuSentiLex-master/\", senti_name_v), col_names = F) %>% head(10)\nsenti_dic_df <- read_tsv(str_c(\"data/KnuSentiLex-master/\", senti_name_v), col_names = F)\nglimpse(senti_dic_df)## Rows: 14,855\n## Columns: 2\n## $ X1 <chr> \"(-;\", \"(;_;)\", \"(^^)\", \"(^-^)\", \"(^^*\", \"(^_^)…\n## $ X2 <dbl> 1, -1, 1, 1, 1, 1, -1, 1, -1, -1, -1, -1, 1, 1,…\n\nsenti_dic_df[1-5, ]## # A tibble: 14,854 × 2\n##   X1       X2\n##   <chr> <dbl>\n## 1 (-;       1\n## 2 (;_;)    -1\n## 3 (^^)      1\n## 4 (^^*      1\n## 5 (^_^)     1\n## 6 (^_^;    -1\n## # … with 14,848 more rows\n\nsenti_dic_df <- senti_dic_df %>% rename(word = X1, sScore = X2)\nglimpse(senti_dic_df)## Rows: 14,855\n## Columns: 2\n## $ word   <chr> \"(-;\", \"(;_;)\", \"(^^)\", \"(^-^)\", \"(^^*\", \"(…\n## $ sScore <dbl> 1, -1, 1, 1, 1, 1, -1, 1, -1, -1, -1, -1, 1…\n"},{"path":"sentiment.html","id":"사전내용","chapter":"7 .  감성분석","heading":"7.2.1.0.1 사전내용","text":"KNU감성사전에 포함된 긍정단어와 부정단어를 살펴보자.각 감정점수별로 단어의 수를 계산해보자.감정점수가 -2 ~ 2까지 정수로 부여돼 있다. 이를 3단계로 줄여 각각 ’긍정, 중립, 부정’으로 명칭을 바꾸자.sScore 열에 결측값이 한 행에 있다. 감정단어와 감정값 사이에 \\t이 아닌 기호로 분리돼 있었기 때문이다. 결측값이 있는 행을 제거하고, 해당 내용을 추가하자.","code":"\nsenti_dic_df %>% filter(sScore == 2) %>% arrange(word)## # A tibble: 2,603 × 2\n##   word              sScore\n##   <chr>              <dbl>\n## 1 가능성이 늘어나다      2\n## 2 가능성이 있다고        2\n## 3 가능하다               2\n## 4 가볍고 상쾌하다        2\n## 5 가볍고 상쾌한          2\n## 6 가볍고 시원하게        2\n## # … with 2,597 more rows\n\nsenti_dic_df %>% filter(sScore == -2) %>% arrange(word)## # A tibble: 4,799 × 2\n##   word         sScore\n##   <chr>         <dbl>\n## 1 가난             -2\n## 2 가난뱅이         -2\n## 3 가난살이         -2\n## 4 가난살이하다     -2\n## 5 가난설음         -2\n## 6 가난에           -2\n## # … with 4,793 more rows\n\nsenti_dic_df %>% count(sScore)## # A tibble: 6 × 2\n##   sScore     n\n##    <dbl> <int>\n## 1     -2  4799\n## 2     -1  5030\n## 3      0   154\n## 4      1  2268\n## 5      2  2603\n## 6     NA     1\n\nsenti_dic_df %>% \n  mutate(emotion = ifelse(sScore >= 1, \"positive\", \n                          ifelse(sScore <= -1, \"negative\", \"neutral\"))) %>% \n  count(emotion)## # A tibble: 4 × 2\n##   emotion      n\n##   <chr>    <int>\n## 1 negative  9829\n## 2 neutral    154\n## 3 positive  4871\n## 4 NA           1\n\nsenti_dic_df$sScore %>% unique()## [1]  1 -1  0 -2  2 NA\n\nsenti_dic_df %>% \n  filter(is.na(sScore)) ## # A tibble: 1 × 2\n##   word    sScore\n##   <chr>    <dbl>\n## 1 갈등 -1     NA\n\nsenti_dic_df %>% \n  filter(!is.na(sScore)) %>% \n  add_row(word = \"갈등\", sScore = -1) -> senti_dic_df \n\nsenti_dic_df %>% \n  filter(!is.na(sScore)) %>% count(sScore)## # A tibble: 5 × 2\n##   sScore     n\n##    <dbl> <int>\n## 1     -2  4799\n## 2     -1  5031\n## 3      0   154\n## 4      1  2268\n## 5      2  2603\n\nknu_dic_df <- senti_dic_df %>% \n  filter(!is.na(sScore))"},{"path":"sentiment.html","id":"연습-4","chapter":"7 .  감성분석","heading":"7.2.2 연습","text":"헌번전문을 토큰화한 다음, 긍정어와 부정어의 빈도를 계산하려고 한다. 이를 위해 토큰화한 헌법전문과 KNU한국어감성사전을 결합해 감정 단어만 추려내려 한다. 다음 중 어느 결합을 이용해야 하는가? 왜?semi_joinanti_joininner_joinleft_joinright_joinfull_join위에서 선택한 _join으로 KNU한국어감성사전을 이용해 헌법전문에서 긍정어와 부정어를 추출하자.형태소를 추출해 분석해보자. RcppMeCab 사용KoNLP로 분석해보자.감정사전에 없는 단어를 제거하지 말고 결합해보자. left_join을 이용한다. 감성사전에 없는 단어에는 sScore열의 NA를 0으로 바꾼다.헌법 전문의 감정점수를 구해보자.","code":"\ncon_v <- \"유구한 역사와 전통에 빛나는 우리 대한국민은 3·1운동으로 건립된 대한민국임시정부의 법통과 불의에 항거한 4·19민주이념을 계승하고, 조국의 민주개혁과 평화적 통일의 사명에 입각하여 정의·인도와 동포애로써 민족의 단결을 공고히 하고, 모든 사회적 폐습과 불의를 타파하며, 자율과 조화를 바탕으로 자유민주적 기본질서를 더욱 확고히 하여 정치·경제·사회·문화의 모든 영역에 있어서 각인의 기회를 균등히 하고, 능력을 최고도로 발휘하게 하며, 자유와 권리에 따르는 책임과 의무를 완수하게 하여, 안으로는 국민생활의 균등한 향상을 기하고 밖으로는 항구적인 세계평화와 인류공영에 이바지함으로써 우리들과 우리들의 자손의 안전과 자유와 행복을 영원히 확보할 것을 다짐하면서 1948년 7월 12일에 제정되고 8차에 걸쳐 개정된 헌법을 이제 국회의 의결을 거쳐 국민투표에 의하여 개정한다.\"\n\ntibble(text = con_v) %>% \n  unnest_tokens(output = word, \n                input = text) %>% \n  inner_join(knu_dic_df)## # A tibble: 2 × 2\n##   word   sScore\n##   <chr>   <dbl>\n## 1 조화를      2\n## 2 행복을      2\n\ncon_v %>% enc2utf8 %>% tibble(text = .) %>% \n  unnest_tokens(output = word, \n                input = text,\n                token = pos) %>%\n  separate(col = word, \n           into = c(\"word\", \"morph\"),\n           sep = \"/\") %>% \n  inner_join(knu_dic_df)## # A tibble: 7 × 3\n##   word  morph sScore\n##   <chr> <chr>  <dbl>\n## 1 조화  nng        2\n## 2 능력  nng        1\n## 3 최고  nng        2\n## 4 향상  nng        1\n## 5 안전  nng        2\n## 6 행복  nng        2\n## # … with 1 more row\n\ncon_v %>% SimplePos09() %>% flatten_dfc() %>% \n  pivot_longer(everything(), \n               names_to = \"header\", \n               values_to = \"value\") %>% \n  separate_rows(value, sep = \"\\\\+\") %>% \n  separate(value, into = c(\"word\", \"pos\"), sep = \"/\") %>% \n  inner_join(knu_dic_df)\ncon_v %>% SimplePos09() %>% flatten_dfc() %>% \n  pivot_longer(everything(), \n               names_to = \"header\", \n               values_to = \"value\") %>% \n  separate_rows(value, sep = \"\\\\+\") %>% \n  separate(value, into = c(\"word\", \"pos\"), sep = \"/\") %>% \n  left_join(knu_dic_df) %>% \n  mutate(sScore = ifelse(is.na(sScore), 0, sScore)) %>% \n  arrange(desc(sScore))\ncon_v %>% SimplePos09() %>% flatten_dfc() %>% \n  pivot_longer(everything(), \n               names_to = \"header\", \n               values_to = \"value\") %>% \n  separate_rows(value, sep = \"\\\\+\") %>% \n  separate(value, into = c(\"word\", \"pos\"), sep = \"/\") %>% \n  left_join(knu_dic_df) %>% \n  mutate(sScore = ifelse(is.na(sScore), 0, sScore)) %>% \n  summarise(score = sum(sScore))"},{"path":"sentiment.html","id":"감정분석","chapter":"7 .  감성분석","heading":"7.3 감정분석","text":"긴 문서를 이용해 분석해보자.","code":""},{"path":"sentiment.html","id":"수집-1","chapter":"7 .  감성분석","heading":"7.3.1 수집","text":"한글소설을 모아놓은 직지프로젝트 파일을 다운로드 받는다.다운로드받은 epub파일을 R환경으로 이입해 데이터구조를 살핀다.데이터는 리스트구조의 data열 안의 93행 4열의 데이터프레임에 저장돼 있다. 리스트구조의 첬재 요소이므로 [[ ]]로 부분선택해 구조를 살펴보자.","code":"\ndir.create(\"data\")\nfile_url <- 'https://www.dropbox.com/s/ug3pi74dnxb66wj/jikji.epub?dl=1'\ndownload.file(url = file_url, \n              destfile = \"data/jikji.epub\",\n              mode=\"wb\")\nepub(\"data/jikji.epub\") %>% glimpse()## Rows: 1\n## Columns: 8\n## $ identifier <chr> \"urn:uuid:15037ed1-7721-4f33-8534-f6a74…\n## $ title      <chr> \"직지 프로젝트|직지프로텍트\"\n## $ creator    <chr> \"수학방\"\n## $ language   <chr> \"ko\"\n## $ date       <chr> \"2013-05-10\"\n## $ rights     <chr> \"크리에이티브 커먼즈 저작자표시-비영리-…\n## $ source     <chr> \"직지프로젝트(http://www.jikji.org)\"\n## $ data       <list> [<tbl_df[93 x 4]>]\n\njikji_all_df <- epub(\"data/jikji.epub\") %>% .$data %>% .[[1]] %>% \n  arrange(desc(nchar))"},{"path":"sentiment.html","id":"정제-1","chapter":"7 .  감성분석","heading":"7.3.2 정제","text":"text열에 93편의 소설 본문이 들어있다. sosul_v에 할당해 구조를 살펴보자.소설 제목, 지은이, 출전, 본문을 추출해보자.데이터프레임에 저장하자. 마지막 4행은 소설본문이 아니므로 제외하고 저장.비슷한 길이의 소설 1편씩 추출해보자.9번째와 10번째 소설의 길이가 둘다 21000자로 비슷하다. 이상의 ’권태’와 나혜석의 ’규원’이다.두 편의 소설은 웹에서도 볼수 있다.이상의 ‘권태’일제식민지 시기 지식인의 무기력함과 좌절을 여름날 벽촌에서의 권태로운 생활에 대한 묘사를 통해 나타낸 수필.나혜석의 ‘규원’1921년 여성 최초의 서양화가이자 소설가 나혜석의 단편소설. 양반집 규수였던 여인이 사랑하는 이에에 버림받은 한을 여성 특유의 섬세한 표현으로 그린 단편. 규수의 원한이라 규원.","code":"\nsosul_v <- epub(\"data/jikji.epub\") %>% .$data %>% .[[1]] %>% .$text\nsosul_v %>% glimpse##  Named chr [1:93] \"17원 50전(十七圓五十錢)--- 젋은 화가 A의 눈물 한 방울\\n\\n  지은이: 나도향\\n\\n    출전: 개벽, <1923>\\n\\n    공개\"| __truncated__ ...\n##  - attr(*, \"names\")= chr [1:93] \"C:/Users/STATKC~1/AppData/Local/Temp/RtmpoXb4Un/jikji/OEBPS/Text/2.xhtml\" \"C:/Users/STATKC~1/AppData/Local/Temp/RtmpoXb4Un/jikji/OEBPS/Text/3.xhtml\" \"C:/Users/STATKC~1/AppData/Local/Temp/RtmpoXb4Un/jikji/OEBPS/Text/4.xhtml\" \"C:/Users/STATKC~1/AppData/Local/Temp/RtmpoXb4Un/jikji/OEBPS/Text/5.xhtml\" ...\n\nsosul_v %>% str_extract(\".*\\\\b\")##  [1] \"17원 50전(十七圓五十錢)--- 젋은 화가 A의 눈물 한 방울\"\n##  [2] \"B사감(舍監)과 러브레터\"                               \n##  [3] \"경희(瓊姬\"                                            \n##  [4] \"계집 하인\"                                            \n##  [5] \"고향\"                                                 \n##  [6] \"공포(恐怖)의 기록(記錄\"                               \n##  [7] \"광화사(狂畵師\"                                        \n##  [8] \"구운몽(완판 105장본) 현대문\"                          \n##  [9] \"권태\"                                                 \n## [10] \"규원\"                                                 \n## [11] \"그립은 흘긴 눈\"                                       \n## [12] \"금따는 콩밭\"                                          \n## [13] \"금붕어\"                                               \n## [14] \"김탄실과 그 아들\"                                     \n## [15] \"까막잡기\"                                             \n## [16] \"깨뜨려지는 홍등(紅燈\"                                 \n## [17] \"꿈\"                                                   \n## [18] \"낙엽기\"                                               \n## [19] \"날개\"                                                 \n## [20] \"노령근해(露領近海\"                                    \n## [21] \"눈 내리는 오후\"                                       \n## [22] \"단발\"                                                 \n## [23] \"도시와 유령\"                                          \n## [24] \"동정(同情\"                                            \n## [25] \"동해\"                                                 \n## [26] \"돼지(豚\"                                              \n## [27] \"들\"                                                   \n## [28] \"만년 샤쓰\"                                            \n## [29] \"말없는 사람\"                                          \n## [30] \"메밀꽃 필 무렵\"                                       \n## [31] \"무지개\"                                               \n## [32] \"물레방아\"                                             \n## [33] \"바람 부는 저녁\"                                       \n## [34] \"바리데기(바리공주\"                                    \n## [35] \"발가락이 닯았다\"                                      \n## [36] \"배따라기\"                                             \n## [37] \"벙어리 삼룡(三龍)이\"                                  \n## [38] \"별을 안거든 우지나 말걸\"                              \n## [39] \"병상 이후\"                                            \n## [40] \"봉별기(逢別記\"                                        \n## [41] \"북국사신(北國私信\"                                    \n## [42] \"붉은산\"                                               \n## [43] \"빈처(貧妻\"                                            \n## [44] \"뽕\"                                                   \n## [45] \"사립 정신병원장\"                                      \n## [46] \"사진\"                                                 \n## [47] \"산\"                                                   \n## [48] \"산골 나그네\"                                          \n## [49] \"상륙\"                                                 \n## [50] \"소 - 전 영택 소설\"                                    \n## [51] \"소낙비\"                                               \n## [52] \"수탉\"                                                 \n## [53] \"술 권하는 사회\"                                       \n## [54] \"실락원\"                                               \n## [55] \"실화(失花\"                                            \n## [56] \"약령기(弱齡記\"                                        \n## [57] \"약수\"                                                 \n## [58] \"옛날 꿈은 창백하더이다\"                               \n## [59] \"용(龍)과 용(龍)의 대격전(大激戰\"                      \n## [60] \"운명\"                                                 \n## [61] \"운수좋은 날\"                                          \n## [62] \"운현궁(雲峴宮)의 봄\"                                  \n## [63] \"인간산문(人間散文\"                                    \n## [64] \"자기를 찾기 전\"                                       \n## [65] \"젊은이의 시절\"                                        \n## [66] \"정조(貞操)와 약가(藥價\"                               \n## [67] \"종생기\"                                               \n## [68] \"주리면.... ---어떤 생활의 단면\"                       \n## [69] \"지도(地圖)의 암실(暗室\"                               \n## [70] \"지주회시(踟蛛會豕\"                                    \n## [71] \"지팽이 역사(轢死\"                                     \n## [72] \"지형근(池亨根\"                                        \n## [73] \"차돌멩이\"                                             \n## [74] \"천치(天痴)? 천재(天才\"                                \n## [75] \"총각과 맹꽁이\"                                        \n## [76] \"크리스마스 전야의 풍경\"                               \n## [77] \"타락자 (墮落者\"                                       \n## [78] \"퇴별가 -완판본\"                                       \n## [79] \"하늘을 바라보는 여인\"                                 \n## [80] \"한 마리 양\"                                           \n## [81] \"할머니의 죽음\"                                        \n## [82] \"행랑 자식\"                                            \n## [83] \"혈의 누(血の淚\"                                       \n## [84] \"화수분\"                                               \n## [85] \"환시기(幻視記\"                                        \n## [86] \"황소와 도깨비\"                                        \n## [87] \"회생한 손녀에게\"                                      \n## [88] \"휴업(休業)과 사정(事情\"                               \n## [89] \"흰 닭\"                                                \n## [90] \"차례 - 작가순\"                                        \n## [91] \"차례 - 가나다 순\"                                     \n## [92] \"12월 12일(十二月 十二日\"                              \n## [93] \"직지프로젝트\"\n\nsosul_v %>% str_extract(\"지은이.*\\\\b\") %>% str_remove(\"지은이: \")##  [1] \"나도향\" \"현진건\" \"나혜석\" \"나도향\" \"현진건\" \"이상\"  \n##  [7] \"김동인\" \"김만중\" \"이상\"   \"나혜석\" \"현진건\" \"김유정\"\n## [13] \"전영택\" \"전영택\" \"현진건\" \"이효석\" \"나도향\" \"이효석\"\n## [19] \"이상\"   \"이효석\" \"전영택\" \"이상\"   \"이효석\" \"현진건\"\n## [25] \"이상\"   \"이효석\" \"이효석\" \"방정환\" \"전영택\" \"이효석\"\n## [31] \"김동인\" \"나도향\" \"전영택\" \"지은이\" \"김동인\" \"김동인\"\n## [37] \"나도향\" \"나도향\" \"이상\"   \"이상\"   \"이효석\" \"김동인\"\n## [43] \"현진건\" \"나도향\" \"현진건\" \"전영택\" \"이효석\" \"김유정\"\n## [49] \"이효석\" \"전영택\" \"김유정\" \"이효석\" \"현진건\" \"이상\"  \n## [55] \"이상\"   \"이효석\" \"이상\"   \"나도향\" \"신채호\" \"전영택\"\n## [61] \"현진건\" \"김동인\" \"이효석\" \"나도향\" \"나도향\" \"현진건\"\n## [67] \"이상\"   \"이효석\" \"이상\"   \"이상\"   \"이상\"   \"나도향\"\n## [73] \"전영택\" \"전영택\" \"김유정\" \"전영택\" \"현진건\" NA      \n## [79] \"전영택\" \"전영택\" \"현진건\" \"나도향\" \"이인직\" \"전영택\"\n## [85] \"이상\"   \"이상\"   \"나혜석\" \"이상\"   \"전영택\" NA      \n## [91] NA       \"이상\"   NA\n\nsosul_v %>% str_extract(\"출전.*\\\\b\") %>% \n  str_remove(\":\") %>% str_remove(\"\\\\)\") %>% str_remove(\"출전 \")##  [1] \"개벽, <1923\"                    \n##  [2] \"조선문단 5호 <1925\"             \n##  [3] NA                               \n##  [4] \"조선 문단 5월호 <1925\"          \n##  [5] \"조선의 얼굴 <1926.3\"            \n##  [6] \"매일신보 , 1937.4.15~5.15\"      \n##  [7] \"야담 1(1935.12\"                 \n##  [8] \"출전\"                           \n##  [9] NA                               \n## [10] \"신가정1, 1921년 7월\"            \n## [11] \"폐허이후 1 <1923\"               \n## [12] \"<개벽> 3월호, 1935\"             \n## [13] \"?, <1959\"                       \n## [14] \"?, <1955\"                       \n## [15] \"개벽 43호 <1923\"                \n## [16] \"대중공론(大衆公論 》4, 1930. 4\" \n## [17] \"조선문단 11월호 <1925\"          \n## [18] \"백광(白光》 1호. 1937. 1\"       \n## [19] \"조광 11, 1936.9\"                \n## [20] \"조선강단(朝鮮講壇》1., 1930. 1\" \n## [21] \"?, <1959\"                       \n## [22] \"조선문학 17, 1939.4\"            \n## [23] \"조선지광(朝鮮之光》79., 1928. 7\"\n## [24] \"조선의 얼굴 <1926\"              \n## [25] \"조광 16, 1936.10\"               \n## [26] \"조선지광(朝鮮之光》1933. 10\"    \n## [27] \"신동아》 1936.3\"                \n## [28] \"어린이, 1927년 3월\"             \n## [29] \"?, <1964\"                       \n## [30] \"조광(朝光》 1936.10\"            \n## [31] \"매일신보』 (1930.9\"             \n## [32] \"조선문단 11 <1925.9\"            \n## [33] \"?, <1925\"                       \n## [34] \"출전\"                           \n## [35] \"동광 29(1924.1\"                 \n## [36] \"창조 9(1921.6\"                  \n## [37] \"현대평론, <1926\"                \n## [38] \"백조 2호, <1922\"                \n## [39] \"청색지 5, 1939.5\"               \n## [40] \"여성 9, 1936.12\"                \n## [41] \"신소설》5호, 1930.9\"            \n## [42] \"조선 문단 4(1925.1\"             \n## [43] \"개벽 12월호 <1921\"              \n## [44] \"개벽 12월호 <1925\"              \n## [45] \"개벽 <1926\"                     \n## [46] \"?, <1924\"                       \n## [47] \"삼천리》69호, 1936.1-3\"         \n## [48] \"제1선지 3월호, 1933\"            \n## [49] \"대중공론(大衆公論》, 1930.6\"    \n## [50] \"?, <1947\"                       \n## [51] \"조선일보 신춘문예, 1935\"        \n## [52] \"《삼천리》, 1933.11\"            \n## [53] \"개벽 17호 <1921\"                \n## [54] \"조광, 1939.2\"                   \n## [55] \"출전\"                           \n## [56] \"삼천리》, 1930\"                 \n## [57] \"중앙, 1936.7\"                   \n## [58] \"개벽 30, <1922.12\"              \n## [59] \"출전\"                           \n## [60] \"창조 3호, <1920\"                \n## [61] \"개벽 48호 <1924\"                \n## [62] \"출전\"                           \n## [63] \"조광》 9호, 1936.7\"             \n## [64] \"개벽, <1924\"                    \n## [65] \"백조, <1922.1\"                  \n## [66] \"신소설 1 <1929\"                 \n## [67] \"조광 19, 1937.5\"                \n## [68] \"청년》, 1927.3\"                 \n## [69] \"조선과 건축, 1932.3\"            \n## [70] \"<중앙>, 1936년 6월\"             \n## [71] \"월간매신, 1934.8\"               \n## [72] \"조선문단 3, 4, 5월호, <1926\"    \n## [73] \"???, <1960\"                     \n## [74] \"창조 2호, <1919\"                \n## [75] \"신여성 9월호, 1933\"             \n## [76] \"?, <1960\"                       \n## [77] \"개벽 19-22, <1922\"              \n## [78] NA                               \n## [79] \"?, <1948\"                       \n## [80] \"?, <1959\"                       \n## [81] \"백조 3호 <1923\"                 \n## [82] \"개벽 40호, <1923\"               \n## [83] \"만세보(1906.7.22~1906.10.10\"    \n## [84] \"?, <1925\"                       \n## [85] \"청색지 1, 1938.6\"               \n## [86] \"매일신보, 1937.3.5~9\"           \n## [87] \"여자계1, 1918년 9월\"            \n## [88] \"조선 4, 1931.4\"                 \n## [89] \"?, <1925\"                       \n## [90] NA                               \n## [91] NA                               \n## [92] NA                               \n## [93] NA\n\nsosul_v %>% str_squish() %>% \n  str_extract(\"본문.*\") %>%\n  str_remove(\"본문|:\") %>% .[5]## [1] \" 대구에서 서울로 올라오는 차중에서 생긴 일이다. 나는 나와 마주 앉은 그를 매우 흥미있게 바라보고 또 바라보았다. 두루마기 격으로 기모노를 둘렀고, 그 안에서 옥양목 저고리가 내어 보이며 아랫도리엔 중국식 바지를 입었다. 그것은 그네들이 흔히 입는 유지 모양으로 번질번질한 암갈색 피륙으로 지은 것이었다. 그리고 발은 감발을 하였는데 짚신을 신었고, 고무가리로 깎은 머리엔 모자도 쓰지 않았다. 우연히 이따금 기묘한 모임을 꾸민 것이다. 우리가 자리를 잡은 찻간에는 공교롭게 세 나라 사람이 다 모였으니, 내 옆에는 중국 사람이 기대었다. 그의 옆에는 일본 사람이 앉아 있었다. 그는 동양 삼국옷을 한몸에 감은 보람이 있어 일본말도 곧잘 철철 대이거니와 중국말에도 그리 서툴지 않은 모양이었다. \\\"고꼬마데 오이데 데스까?(어디까지 가십니까?)\\\"하고 첫마디를 걸더니만, 도꼬가 어떠니, 오사까가 어떠니, 조선 사람은 고추를 끔찍이 많이 먹는다는 둥, 일본 음식은 너무 싱거워서 처음에는 속이 뉘엿걸다는 둥, 횡설수설 지껄이다가 일본 사람이 엄지와 검지 손가락으로 짧게 끊은 꼿꼿한 윗수염을 비비면서 마지못해 까땍까땍하는 고개와 함께 \\\"소데스까(그렇습니까)\\\"란 한 마디로 코대답을 할 따름이요, 잘 받아 주지 않으매, 그는 또 중국인을 붙들고서 실랑이를 하였다. \\\"니상나열취......\\\" \\\"니싱섬마\\\"하고 덤벼 보았으나 중국인 또한 그 기름낀 뚜우한 얼굴에 수수께끼 같은 웃음을 띨 뿐이요 별로 대구를 하지 않았건만, 그래도 무어라고 연해 웅얼거리면서 나를 보고 웃어 보였다. 그것은 마치 짐승을 놀리는 요술장이가 구경꾼을 바라볼 때처럼 훌륭한 재주를 갈채해 달라는 웃음이었다. 나는 쌀쌀하게 그의 시선을 피해 버렸다. 그 주적대는 꼴이 어줍지 않고 밉살스러웠다. 그는 잠깐 입을 닫치고 무료한 듯이 머리를 덕억덕억 긁기도 하며, 손톱을 이로 물어뜯기도 하고, 멀거니 창 밖을 내다보기도 하다가, 암만해도 중절대지 않고는 못 참겠던지 문득 나에게로 향하며, \\\"어디꺼정 가는 기오?\\\"라고 경상도 사투리로 말을 붙인다. \\\"서울까지 가요.\\\" \\\"그런기오. 참 반갑구마. 나도 서울꺼정 가는데. 그러면 우리 동행이 되겠구마.\\\" 나는 이 지나치게 반가와하는 말씨에 대하여 무어라고 대답할 말도 없고, 또 굳이 대답하기도 싫기에 덤덤히 입을 닫쳐 버렸다. \\\"서울에 오래 살았는기요?\\\" 그는 또 물었다. \\\"육칠년이나 됩니다.\\\" 조금 성가시다 싶었으되, 대꾸 않을 수도 없었다. \\\"에이구, 오래 살았구마, 나는 처음길인데 우리 같은 막벌이군이 차를 내려서 어디로 찾아가야 되겠는기요? 일본으로 말하면 기전야도 같은 것이 있는기오?\\\" 하고 그는 답답한 제 신세를 생각했던지 찡그려 보았다. 그때 나는 그의 얼굴이 웃기보다 찡그리기에 가장 적당한 얼굴임을 발견하였다. 군데군데 찢어진 겅성드뭇한 눈썹이 올올이 일어서며, 아래로 축 처지는 서슬에 양미간에는 여러 가닥 주름이 잡히고, 광대뼈 위로 뺨살이 실룩실룩 보이자 두 볼은 쪽 빨아든다. 입은 소태나 먹은 것처럼 왼편으로 삐뚤어지게 찢어 올라가고, 죄던 눈엔 눈물이 괸 듯 삼십 세밖에 안되어 보이는 그 얼굴이 10년 가량은 늙어진 듯하였다. 나는 그 신산스러운 표정에 얼마쯤 감동이 되어서 그에게 대한 반감이 풀려지는 듯하였다. \\\"글쎄요, 아마 노동 숙박소란 것이 있지요.\\\" 노동 숙박소에 대해서 미주알고주알 묻고 나서, \\\"시방 가면 무슨 일자리를 구하겠는기오?\\\" 라고 그는 매달리는 듯이 또 꽤쳤다. \\\"글쎄요, 무슨 일자리를 구할 수 있을는지요.\\\" 나는 내 대답이 너무 냉랭하고 불친절한 것이 죄송스러웠다. 그러나 일자리에 대하여 아무 지식이 없는 나로서는 이외에 더 좋은 대답을 해 줄 수가 없었던 것이다. 그 대신 나는 은근하게 물었다. \\\"어디서 오시는 길입니까?\\\" \\\"흠, 고향에서 오누마.\\\" 하고 그는 휘 한숨을 쉬었다. 그러자, 그의 신세타령의 실마리는 풀려 나왔다. 그의 고향은 대구에서 멀지 않은 K군 H란 외따른 동리였다. 한 백호 남짓한 그곳 주님은 전부가 역둔토를 파먹고 살았는데, 역둔토로 말하면 사삿집 땅을 부치는 것보다 떨어지는 것이 후하였다. 그러므로 넉넉지는 못할망정 평화로운 농촌으로 남부럽지 않게 지낼 수 있었다. 그러나 세상이 뒤바뀌자 그 땅은 전부가 동양 척식 회사의 소유에 들어가고 말았다. 직접으로 회사에 소작료를 바치게 되었으면 그래도 나으련만 소위 중간 소작인이란 것이 생겨나서 저는 손에 흙 한 번 만져 보지도 않고 동척엔 소작인 노릇을 하며, 실지인에게는 지주 행세를 하게 되었다. 동척에 소작료를 물고 나서 또 중간 소작료인에게 긁히고 보니, 실작인의 손에는 소출이 3할도 떨어지지 않았다. 그후로 <죽겠다, 못 살겠다>하는 소리는 중이 염불하듯 그들의 입길에서 오르내리게 되었다. 남부여대하고 타처로 유리하는 사람만 늘고 동리는 점점 쇠진해갔다. 지금으로부터 9년 전, 그가 열일곱 살 되던 해 봄에(그의 나이는 실상 스물여섯이었다. 가난과 고생이 얼마나 사람을 늙히는가?) 그의 집안은 살기 좋다는 바람에 서간도로 이사를 갔었다. 쫓겨가는 운명이거든 어디를 간들 신신하랴. 그곳의 비옥한 전야도 그들을 위하여 열려질 리 없었다. 조금 좋은 땅은 먼저 간 이가 모조리 차지하였고 황무지는 비록 많다 하나 그곳 당도하던 날부터 아침거리 저녁거리 걱정이랴. 무슨 행세로 적어도 1년이란 장구한 세월을 먹고 입어 가며 거친 땅을 풀 수가 있으랴. 남의 밑천을 얻어서 농사를 짓고 보니, 가을이 되어 얻는 것은 빈주먹뿐이었다. 이태 동안을 사는 것이 아니라 억지로 버티어 갈 제, 그의 아버지는 망연히 병을 얻어 타국의 외로운 혼이 되고 말았다. 열아홉 살밖에 안된 그가 홀어머니를 보시고 악으로 악으로 모진 목숨을 이어가는 중 4년이 못되어 영양 부족한 몸이 심한 노동에 지친 탓으로 그의 어머니 또한 죽고 말았다. \\\"모친까장 돌아갔구마.\\\" \\\"돌아가실 때 흰죽 한 모금도 못 자셨구마.\\\"하고 이야기하던 이는 문득 말을 뚝 끊는다. 나는 무엇이라고 위로할 말을 몰랐다. 한동안 머뭇머뭇이 있다가 나는 차를 탈 때에 친구들이 사준 정종병 마개를 빼었다. 찻잔에 부어서 그도 마시고 나도 마셨다. 악착한 운명이 던져 준 깊은 슬픔을 술로 녹이려는 듯이 연거푸 다섯 잔을 마시는 그는 다시 말을 계속하였다. 그후 그는 부모 잃은 땅에 오래 머물기 싫었다. 신의주로, 안동현으로 품을 팔다가 일본으로 또 벌이를 찾아가게 되었다. 규슈 탄광에 있어도 보고, 오사까 철공장에도 몸을 담아 보았다. 벌이는 조금 나았으나 외롭고 젊은 몸은 자연히 방탕해졌다. 돈을 모으려야 모을 수 없고 이따금 울화만 치받치기 때문에 한곳에 주접을 하고 있을 수 없었다. 화도 나고 고국 산천이 그립기도 하여서 훌쩍 뛰어나왔다가 오래간만에 고향을 둘러보고 벌이를 구할 겸 서울로 올라가는 길이라 했다. \\\"고향에 가시니 반가워하는 사람이 있습디까?\\\" 나는 탄식하였다. \\\"반가워하는 사람이 다 뮌기오, 고향이 통 없어졌더마.\\\" \\\"그렇겠지요. 9년 동안이나 퍽 변했겠지요.\\\" \\\"변하고 뭐고 간에 아무것도 없더마. 집도 없고, 사람도 없고, 개 한 마리도 얼씬을 않더마.\\\" \\\"그러면, 아주 폐농이 되었단 말씀이오?\\\" \\\"흥, 그렇구마. 무너지다 만 담만 즐비하게 남았드마. 우리 살던 집도 터야 안 남았는기오, 암만 찾아도 못 찾겠더마. 사람 살던 동리가 그렇게 된 것을 혹 구경했는기오?\\\" 하고 그의 짜는 듯 한 목은 높아졌다. \\\"썩어 넘어진 서까래, 뚤뚤 구르는 주추는! 꼭 무덤을 파서 해골을 헐어 젖혀놓은 것 같더마. 세상에 이런 일도 있는기오? 백여호 살던 동리가 10년이 못 되어 통 없어지는 수도 있는기오, 후!\\\" 하고 그는 한숨을 쉬며, 그때의 광경을 눈앞에 그리는 듯이 멀거니 먼산을 보다가 내가 따라 준 술을 꿀꺽 들이켜고, \\\"참! 가슴이 터지더마, 가슴이 터져\\\" 하자마자 굵직한 눈물 둬 방울이 뚝뚝 떨어진다. 나는 그 눈물 가운데 음산하고 비참한 조선의 얼굴을 똑똑히 본 듯 싶었다. 이윽고 나는 이런 말을 물었다. \\\"그래, 이번 길에 고향 사람은 하나도 못 만났습니까?\\\" \\\"하나 만났구마, 단지 하나.\\\" \\\"친척되는 분이던가요?\\\" \\\"아니구마, 한 이웃에 살던 사람이구마.\\\" 하고 그의 얼굴은 더욱 침울했다. \\\"여간 반갑지 않으셨지어요.\\\" \\\"반갑다마다, 죽은 사람을 만난 것 같더마. 더구나 그 사람은 나와 까닭도 좀 있던 사람인데......\\\" \\\"까닭이라니?\\\" \\\"나와 혼인 말이 있던 여자구마.\\\" \\\"하아!\\\" 나는 놀란 듯이 벌린 입이 닫혀지지 않았다. \\\"그 신세도 내 신세만 하구마.\\\" 하고 그는 또 이야기를 계속하였다. 그 여자는 자기보다 나이 두 살 위였는데, 한이웃에 사는 탓으로 같이 놀기도 하고 싸우기도 하며 자라났다. 그가 열 네살 적부터 그들 부모들 사이에 혼인 말이 있었고 그도 어린 마음에 매우 탐탁하게 생각하였었다. 그런데 그 처녀가 열일곱 살 된 겨울에 별안간 간 곳을 모르게 되었다. 알고 보니, 그 아버지되는 자가 20원을 받고 대구 유곽에 팔아먹은 것이었다. 그 소문이 퍼지자 그 처녀 가족은 그 동리에서 못 살고 멀리 이사를 갔는데 그 후로는 물론 피차에 한 번 만나 보지도 못하였다. 이번에야 빈터만 남은 고향을 구경하고 돌아오는 길에 읍내에서 그 아내될 뻔한 댁과 마주치게 되었다. 처녀는 어떤 일본 사람 집에서 아이를 보고 있었다. 궐녀는 20원 몸값을 10년을 두고 갚았건만 그래도 주인에게 빚이 60원이나 남았었는데, 몸에 몹쓸 병이 들어 나이 늙어져서 산송장이 되니까. 주인되는 자가 특별히 빚을 탕감해 주고, 작년 가을에야 놓아 준 것이었다. 궐녀도 자기와 같이 10년 동안이나 그리던 고향에 찾아오니까 거기에는 집도 없고, 부모도 없고 쓸쓸한 돌무더기만 눈물을 자아낼 뿐이었다. 하루해를 울어 보내고 읍내로 들어와서 돌아다니다가, 10년 동안에 한 마디 두 마디 배워 두었던 일본말 덕택으로 그 일본 집에 있게 되었던 것이다. \\\"암만 사람이 변하기로 어째 그렇게도 변하는기오? 그 숱 많던 머리가 훌렁 다 벗을졌두마. 눈을 푹 들어가고 그 이들이들하던 얼굴빛도 마치 유산을 끼얹은 듯하더마.\\\" \\\"서로 붙잡고 많이 우셨겠지요\\\" \\\"눈물도 안 나오더마. 일본 우동집에 들어가서 둘이서 정종만 열병 때려뉘고 헤어졌구마.\\\" 하고 가슴을 짜는 듯한 괴로운 한숨을 쉬더니만 그는 지난 슬픔을 새록새록 자아내어 마음을 새기기에 지쳤음이더라. \\\"이야기를 다하면 뭐하는기오.\\\" 하고 쓸쓸하게 입을 다문다. 나 또한 너무도 참혹한 사람살이를 듣기에 쓴물이 났다. \\\"자, 우리 술이나 마자 먹읍시다.\\\" 하고 우리는 주거니받거니 한되 병을 다 말리고 말았다. 그는 취흥에 겨워서 우리가 어릴 때 멋모르고 부르던 노래를 읊조렸다. 볏섬이나 나는 전토는 신작로가 되고요...... 말마디나 하는 친구는 감옥소로 가고요...... 담뱃대나 떠는 노인은 공동묘지 가고요...... 인물이나 좋은 계집은 유곽으로 가고요...... <1926>\"\n\ntitle_v <- sosul_v %>% str_extract(\".*\\\\b\")\nauthor_v <- sosul_v %>% str_extract(\"지은이.*\\\\b\") %>% str_remove(\"지은이: \")\nsource_v <- sosul_v %>% str_extract(\"출전.*\\\\b\") %>% \n  str_remove(\":\") %>% str_remove(\"\\\\)\") %>% str_remove(\"출전 \")\nmain_v <- sosul_v %>% str_squish() %>% \n  str_extract(\"본문.*\") %>% str_remove(\"본문|:\")\n\nsosul_df <- tibble(title = title_v, \n       author = author_v,\n       source = source_v,\n       main = main_v) %>% .[1:89,]\nsosul_df %>% glimpse()## Rows: 89\n## Columns: 4\n## $ title  <chr> \"17원 50전(十七圓五十錢)--- 젋은 화가 A의 …\n## $ author <chr> \"나도향\", \"현진건\", \"나혜석\", \"나도향\", \"현…\n## $ source <chr> \"개벽, <1923\", \"조선문단 5호 <1925\", NA, \"…\n## $ main   <chr> \" 첫 째 사랑하시는 C선생님께 어린 심정에서 …\n\njikji_all_df$nchar[1:10]##  C:/Users/STATKC~1/AppData/Local/Temp/RtmpoXb4Un/jikji/OEBPS/Text/1.xhtml \n##                                                                    102011 \n##  C:/Users/STATKC~1/AppData/Local/Temp/RtmpoXb4Un/jikji/OEBPS/Text/9.xhtml \n##                                                                     86078 \n## C:/Users/STATKC~1/AppData/Local/Temp/RtmpoXb4Un/jikji/OEBPS/Text/90.xhtml \n##                                                                     61151 \n## C:/Users/STATKC~1/AppData/Local/Temp/RtmpoXb4Un/jikji/OEBPS/Text/84.xhtml \n##                                                                     51379 \n## C:/Users/STATKC~1/AppData/Local/Temp/RtmpoXb4Un/jikji/OEBPS/Text/78.xhtml \n##                                                                     36357 \n## C:/Users/STATKC~1/AppData/Local/Temp/RtmpoXb4Un/jikji/OEBPS/Text/85.xhtml \n##                                                                     32444 \n##  C:/Users/STATKC~1/AppData/Local/Temp/RtmpoXb4Un/jikji/OEBPS/Text/4.xhtml \n##                                                                     27801 \n## C:/Users/STATKC~1/AppData/Local/Temp/RtmpoXb4Un/jikji/OEBPS/Text/20.xhtml \n##                                                                     23159 \n## C:/Users/STATKC~1/AppData/Local/Temp/RtmpoXb4Un/jikji/OEBPS/Text/68.xhtml \n##                                                                     21976 \n## C:/Users/STATKC~1/AppData/Local/Temp/RtmpoXb4Un/jikji/OEBPS/Text/71.xhtml \n##                                                                     21821\n\nsosul2_df <- sosul_df[9:10, ] \nsosul2_df## # A tibble: 2 × 4\n##   title author source              main                    \n##   <chr> <chr>  <chr>               <chr>                   \n## 1 권태  이상   NA                  \" 1 어서, 차라리 어둬버…\n## 2 규원  나혜석 신가정1, 1921년 7월 \" 때는 정히 오월 중순이…\n"},{"path":"sentiment.html","id":"분석-및-소통-1","chapter":"7 .  감성분석","heading":"7.3.3 분석 및 소통","text":"감정어휘 빈도를 계산해 두 문서에 감정을 분석해보자.형태소를 분석해 감정을 분석해 보자. RcppMeCab의 pos()함수를 이용하자.형태소를 분석한 결과를 통해 나타난 두 문서의 감정이 조금 더 긍정적으로 변했다. 형태소분석하면서 부정어휘로 구분됐던 단어들이 한글자로 분리돼 불용어로 제거된 결과다.","code":"\nsosul2_senti_df <- sosul2_df %>% \n  unnest_tokens(word, main) %>% \n  inner_join(knu_dic_df) %>% \n  group_by(author) %>% \n  count(word, sScore, sort = T) %>% \n  filter(str_length(word) > 1) %>% \n  mutate(word = reorder(word, n)) %>% \n  slice_head(n = 20)\n\nsosul2_senti_df %>% \n  ggplot() + geom_col(aes(n, word, fill = sScore), show.legend = F) +\n  facet_wrap(~author, scales = \"free\")\nsosul2_senti_tk <- sosul2_df %>% \n  unnest_tokens(word, main, token = pos) %>% \n  separate(col = word, \n           into = c(\"word\", \"morph\"), \n           sep = \"/\" ) %>% \n  inner_join(knu_dic_df) %>% \n  group_by(author) %>% \n  count(word, sScore, sort = T) %>% \n  filter(str_length(word) > 1) %>% \n  mutate(word = reorder(word, n)) %>% \n  slice_head(n = 20)\n\nsosul2_senti_tk %>% \n  ggplot() + geom_col(aes(n, word, fill = sScore), show.legend = F) +\n  facet_wrap(~author, scales = \"free\")"},{"path":"sentiment.html","id":"문서의-감정점수-계산","chapter":"7 .  감성분석","heading":"7.3.4 문서의 감정점수 계산","text":"’권태’와 ’규원’의 감정점수를 비교해 어느 글이 더 부정적인지 살펴보자. 각 토큰에 감정점수 부여한다. 감정사전에 없는 단어를 남겨두기 위해 left_join()이용감정단어를 극성별로 분류하자. 사전의 감정점수가 5수준으로 돼 있는 것을 3수준으로 줄인다.’권태’와 ’규원’의 감정점수 계산","code":"\nsosul2_df %>% \n  unnest_tokens(word, main) %>% \n  left_join(knu_dic_df) %>% \n  mutate(sScore = ifelse(is.na(sScore), 0, sScore)) %>% \n  arrange(sScore)## # A tibble: 5,959 × 5\n##   title author source word   sScore\n##   <chr> <chr>  <chr>  <chr>   <dbl>\n## 1 권태  이상   NA     위험한     -2\n## 2 권태  이상   NA     아프게     -2\n## 3 권태  이상   NA     어려운     -2\n## 4 권태  이상   NA     밉다       -2\n## 5 권태  이상   NA     거칠고     -2\n## 6 권태  이상   NA     아니다     -2\n## # … with 5,953 more rows\n\nsosul2_df %>% \n  unnest_tokens(word, main) %>% \n  left_join(knu_dic_df) %>% \n  mutate(sScore = ifelse(sScore >= 1, \"긍정\",\n                         ifelse(sScore <= -1, \"부정\", \"중립\"))) %>% \n  count(sScore)## # A tibble: 4 × 2\n##   sScore     n\n##   <chr>  <int>\n## 1 긍정      39\n## 2 부정     174\n## 3 중립      11\n## 4 NA      5735\n\nsosul2_df %>% \n  unnest_tokens(word, main) %>% \n  left_join(knu_dic_df) %>% \n  mutate(sScore = ifelse(is.na(sScore), 0, sScore)) %>% \n  summarise(emotion = sum(sScore))## # A tibble: 1 × 1\n##   emotion\n##     <dbl>\n## 1    -181\n"},{"path":"sentiment.html","id":"감정-극성별-단어빈도","chapter":"7 .  감성분석","heading":"7.3.5 감정 극성별 단어빈도","text":"감정사전에서 긍정감정 단어와 부정단어를 각각 긍정단어 사전과 부정단어 사전 만들자","code":"\nknu_pos_df <- knu_dic_df %>% \n  filter(sScore > 0)\nknu_pos_df$sScore %>% unique()## [1] 1 2\n\nknu_neg_df <- knu_dic_df %>% \n  filter(sScore < 0)\nknu_neg_df$sScore %>% unique()## [1] -1 -2\n\nsosul2_df %>% \n  unnest_tokens(word, main) %>% \n  inner_join(knu_pos_df) %>% \n  count(author, word, sort = T)## # A tibble: 32 × 3\n##   author word         n\n##   <chr>  <chr>    <int>\n## 1 이상   귀여운       3\n## 2 이상   잘           3\n## 3 나혜석 사랑으로     2\n## 4 나혜석 잘           2\n## 5 이상   좋다         2\n## 6 나혜석 고쳐         1\n## # … with 26 more rows\n\nsosul2_df %>% \n  unnest_tokens(word, main) %>% \n  inner_join(knu_neg_df) %>% \n  count(author, word, sort = T)## # A tibble: 93 × 3\n##   author word       n\n##   <chr>  <chr>  <int>\n## 1 이상   없다      30\n## 2 나혜석 해        11\n## 3 이상   권태       7\n## 4 나혜석 못하고     5\n## 5 이상   답답한     4\n## 6 이상   모르는     4\n## # … with 87 more rows\n"},{"path":"sentiment.html","id":"단어구름에-극성별-단어빈도-표시","chapter":"7 .  감성분석","heading":"7.3.6 단어구름에 극성별 단어빈도 표시","text":"긍정단어와 부정단어 분리","code":"\ninstall.packages(\"wordcloud\")\nlibrary(wordcloud)\nsosul2_df %>% \n  unnest_tokens(word, main) %>% \n  count(word) %>% \n  filter(str_length(word) > 1) %>% \n  with(wordcloud(word, n, max.words = 50))\nlibrary(wordcloud)\nsosul2_df %>% \n  unnest_tokens(word, main) %>% \n  inner_join(knu_dic_df) %>% \n  mutate(emotion = ifelse(sScore > 0, \"긍정\", ifelse(sScore < 0, \"부정\", \"중립\"))) %>% \n  filter(emotion != \"중립\") %>% \n  count(word, emotion, sort = T) %>% \n  filter(str_length(word) > 1) %>% \n  reshape2::acast(word ~ emotion, value.var = \"n\", fill = 0) %>% \n  comparison.cloud(colors = c(\"blue\", \"red\"), max.words = 50)"},{"path":"sentiment.html","id":"문장-전개에-따른-감정-변화","chapter":"7 .  감성분석","heading":"7.3.7 문장 전개에 따른 감정 변화","text":"문장 단위로 토큰화하고, row_number()를 이용해 각 문장단위로 번호를 부여한다.5수준인 감정사전의 감정점수를 긍정과 부정 등 2수준으로 축소하자.긍정감정과 부정감정의 차이를 계산한다. 지은이를 기준으로 각 문장(행)별로 감정의 빈도를 계산하면, 글의 진행에 따른 감정의 변화를 나타낼 수 있다. 그런데, 개별 문장에는 감정어가 들어있는 빈도가 적기 때문에, 여러 문장을 하나의 단위로 묶을 필요가 있다. 이를 위해 몫만을 취하는 정수 나눗셈(%/%)활용.여러 행을 묶지 않고 지은이 별로 감정어의 빈도를 계산하면 334행에는 긍정단어 2개, 부정단어가 1개 있음을 알수 있다.각 행을 10으로 정수나눗셈을 하면 10개 행씩 모두 같은 값으로 묶을 수 있다. 334행부터 339행인 33이 된다. 긍정단어와 부정단어도 각각 5개가 된다.긍정감정 점수에서 부정감정 점수를 빼 감정의 상대강도를 계산하기 위해 감정의 값을 열의 제목으로 만들고, 각 감정어 빈도를 해당 열의 값으로 할당한다. 감정어가 없어 결측값이 나올수 있으므로, values_fill =인자로 ’0’을 할당한다.막대도표로 시각화한다.","code":"\nsosul2_df %>% \n  unnest_tokens(sentence, main, token = \"sentences\") %>% \n  mutate(linenumber = row_number()) %>% \n  unnest_tokens(word, sentence)## # A tibble: 5,959 × 5\n##   title author source linenumber word        \n##   <chr> <chr>  <chr>       <int> <chr>       \n## 1 권태  이상   NA              1 1           \n## 2 권태  이상   NA              1 어서        \n## 3 권태  이상   NA              1 차라리      \n## 4 권태  이상   NA              1 어둬버리기나\n## 5 권태  이상   NA              1 했으면      \n## 6 권태  이상   NA              1 좋겠는데    \n## # … with 5,953 more rows\n\nsosul2_df %>% \n  unnest_tokens(sentence, main, token = \"sentences\") %>% \n  mutate(linenumber = row_number()) %>% \n  unnest_tokens(word, sentence) %>% \n  inner_join(knu_dic_df) %>% \n  mutate(emotion = ifelse(sScore > 0, \"긍정\", \n                          ifelse(sScore < 0, \"부정\", \"중립\")))## # A tibble: 224 × 7\n##   title author source linenumber word  sScore emotion\n##   <chr> <chr>  <chr>       <int> <chr>  <dbl> <chr>  \n## 1 권태  이상   NA              7 바른       2 긍정   \n## 2 권태  이상   NA             14 사랑       2 긍정   \n## 3 권태  이상   NA             15 좋다       2 긍정   \n## 4 권태  이상   NA             19 잘         1 긍정   \n## 5 권태  이상   NA             24 없다      -1 부정   \n## 6 권태  이상   NA             25 권태      -1 부정   \n## # … with 218 more rows\n\nc(1, 10, 20, 30, 40, 50) / 20## [1] 0.05 0.50 1.00 1.50 2.00 2.50\n\nc(1, 10, 20, 30, 40, 50) %/% 20## [1] 0 0 1 1 2 2\n\nsosul2_df %>% \n  unnest_tokens(sentence, main, token = \"sentences\") %>% \n  mutate(linenumber = row_number()) %>% \n  unnest_tokens(word, sentence) %>% \n  inner_join(knu_dic_df) %>% \n  mutate(emotion = ifelse(sScore > 0, \"긍정\", \n                          ifelse(sScore < 0, \"부정\", \"중립\"))) %>% \n  count(author, index = linenumber, emotion)## # A tibble: 190 × 4\n##   author index emotion     n\n##   <chr>  <int> <chr>   <int>\n## 1 나혜석   334 긍정        2\n## 2 나혜석   334 부정        1\n## 3 나혜석   335 부정        1\n## 4 나혜석   336 부정        1\n## 5 나혜석   336 중립        1\n## 6 나혜석   338 긍정        1\n## # … with 184 more rows\n\nsosul2_df %>% \n  unnest_tokens(sentence, main, token = \"sentences\") %>% \n  mutate(linenumber = row_number()) %>% \n  unnest_tokens(word, sentence) %>% \n  inner_join(knu_dic_df) %>% \n  mutate(emotion = ifelse(sScore > 0, \"긍정\", \n                          ifelse(sScore < 0, \"부정\", \"중립\"))) %>% \n  count(author, index = linenumber %/% 10, emotion)## # A tibble: 89 × 4\n##   author index emotion     n\n##   <chr>  <dbl> <chr>   <int>\n## 1 나혜석    33 긍정        5\n## 2 나혜석    33 부정        5\n## 3 나혜석    33 중립        2\n## 4 나혜석    34 긍정        2\n## 5 나혜석    34 부정        8\n## 6 나혜석    35 긍정        1\n## # … with 83 more rows\n\nsosul2_emo_df <- sosul2_df %>% \n  unnest_tokens(sentence, main, token = \"sentences\") %>% \n  mutate(linenumber = row_number()) %>% \n  unnest_tokens(word, sentence) %>% \n  inner_join(knu_dic_df) %>% \n  mutate(emotion = ifelse(sScore > 0, \"긍정\", \n                          ifelse(sScore < 0, \"부정\", \"중립\"))) %>% \n  count(author, index = linenumber %/% 10, emotion) %>% \n  pivot_wider(names_from = emotion, values_from = n, values_fill = 0) %>%   mutate(sentiment = 긍정 - 부정) \n\nsosul2_emo_df## # A tibble: 59 × 6\n##   author index  긍정  부정  중립 sentiment\n##   <chr>  <dbl> <int> <int> <int>     <int>\n## 1 나혜석    33     5     5     2         0\n## 2 나혜석    34     2     8     0        -6\n## 3 나혜석    35     1     4     0        -3\n## 4 나혜석    37     2     2     1         0\n## 5 나혜석    38     0     1     0        -1\n## 6 나혜석    39     1     6     0        -5\n## # … with 53 more rows\n\nsosul2_emo_df %>% \n  ggplot() + \n  geom_col(aes(index, sentiment, fill = author), show.legend = F) +\n  facet_wrap(~author, scales = \"free_x\")"},{"path":"topic-modeling.html","id":"topic-modeling","chapter":"8 .  주제모형","heading":"8 .  주제모형","text":"","code":""},{"path":"topic-modeling.html","id":"개관","chapter":"8 .  주제모형","heading":"8.1 개관","text":"문서에 빈번하는 등장하는 단어를 통해 그 문서의 주제를 추론할 수 있다. 한 문서에는 다양한 주제가 들어있다.예를 들어, 아래 문장은 AP가 보도한 1988년 허스트재단의 링컨센터 기부 기사다.단어의 총빈도와 상대빈도를 계산하면, 이 문서의 주요 내용이 무엇인지 파악할 수 있다.\n먼저 총빈도 상위단어를 찾아보자.상대빈도를 구해보자.먼저 문장 단위로 토큰화해 문장별 ID를 구한다음 단어 단위로 토큰화한다. 대문자 앞의 마침표와 공백(\"\\\\.\\\\s(?=[:upper:])\")을 기준으로 구분하면 된다.추출한 상위 10대 빈도 단어를 비교해 보자.총빈도와 상대빈도를 보면, 허스트재단이 링컨아트센터에 기부금 발표한 내용이란 것을 추론할 수 있지만, 기사에는 보다 다양한 주제를 담고 있다. 기사의 내용을 읽어보면 다양한 주제가 있음을 알수 있다 (Figure 8.1).\n그림 8.1: AP 기사\n기사에 포함된 단어 중 같은 색으로 구부된 단어들을 모아보면 예술, 재정, 아동, 교육 등의 주제를 나타내는 일관된 단어로 구성됐음을 알수 있다 (Figure 8.2).\n그림 8.2: AP 기사 주제\nBlei 등 일군의 전산학자들은 문서 내 단어의 확률분포를 계산해 찾아낸 일련의 단어 군집을 통해 문서의 주제를 추론하는 방법으로서 LDA(Latent Dirichlet Allocation)을 제시했다.Blei, D. M., Ng, . Y., & Jordan, M. . (2003). Latent dirichlet allocation. Journal Machine Learning Research, 3, 993-1022.19세기 독일 수학자 러죈 디리클레(Lejeune Dirichlet, 1805 ~ 1859)가 제시한 디리클래 분포(Dirichlet distribution)를 이용해 문서에 잠재된 주제를 추론하기에 잠재 디리클레 할당(LDA: Latent Dirichlet Allocation)이라고 했다. 문서의 주제를 추론하는 방법이므로 주제모형(topic models)이라고 한다.Beli(2012)가 설명한 LDA에 대한 직관적인 이해는 다음과 같다. Figure8.3에 제시된 논문 “Seeking life’s bare (genetics) necessities”은 진화의 틀에서 유기체가 생존하기 위해 필요한 유전자의 수를 결정하기 위한 데이터분석에 대한 내용이다. 문서(documents)에 파란색으로 표시된 ‘computer’ ‘prediction’ 등은 데이터분석에 대한 단어들이다. 분홍색으로 표시된 ‘life’ ‘organism’은 진화생물학에 대한 내용이다. 노란색으로 표시된 ’sequenced’ ’genes’는 유전학에 대한 내용이다. 이 논문의 모든 단어를 이런 식으로 분류하면 아마도 이 논문은 유전학, 데이터분석, 진화생물학 등이 상이한 비율로 혼합돼 있음을 알게 된다.Blei, D. M. (2012). Probabilistic topic models. Communications ACM, 55(4), 77-84.\n그림 8.3: LDA의 직관적 예시\nLDA에는 다음과 같은 전제가 있다.말뭉치에는 단어를 통해 분포된 다수의 주제가 있다 (위 그림의 가장 왼쪽).각 문서에서 주제를 생성하는 과정은 다음과 같다.주제에 대해 분포 선택(오른쪽 히스토그램)각 단어에 대해 주제의 분포 선택(색이 부여된 동그라미)해당 주제를 구성하는 단어 선택(가운데 화살표)LDA에서 정의하는 주제(topic)는 특정 단어에 대한 분포다. 예를 들어, 유전학 주제라면 유전학에 대하여 높은 확률로 분포하는 단어들이고, 진화생물학 주제라면 진화생물학에 대하여 높은 확률로 분포하는 단어들이다.LDA에서는 문서를 주머니에 무작위로 섞여 있는 임의의 혼합물로 본다(Bag words). 일반적으로 사용하는 문장처럼 문법이라는 짜임새있는 구조로 보는 것이 아니다. 임의의 혼합물이지만 온전하게 무작위로 섞여 있는 것은 아니다. 서로 함께 모여 있는 군집이 확률적으로 존재한다. 즉, 주제모형에서 접근하는 문서는 잠재된 주제의 혼합물로서, 각 주제를 구성하는 단어 단위가 확률적으로 혼합된 주머니(bag)인 셈이다.개별 문서: 여러 주제(topic)가 섞여 있는 혼합물\n문서마다 주제(예술, 교육, 예산 등)의 분포 비율 상이\n주제(예: 예술)마다 단어(예: 오페라, 교향악단)의 분포 상이\n문서마다 주제(예술, 교육, 예산 등)의 분포 비율 상이주제(예: 예술)마다 단어(예: 오페라, 교향악단)의 분포 상이주제모형의 목표는 말뭉치에서 주제의 자동추출이다. 문서 자체는 관측가능하지만, 주제의 구조(문서별 주제의 분포와 문서별-단어별 주제할당)는 감춰져 있다. 감춰진 주제의 구조를 찾아내는 작업은 뒤집어진 생성과정이라고 할 수 있다. 관측된 말뭉치를 생성하는 감춰진 구조를 찾아내는 작업이기 때문이다. 문서에 대한 사전 정보없이 문서의 주제를 분류하기 때문에 주제모형은 비지도학습(unsupervised learning) 방식의 기계학습(machine learning)이 된다.기계학습(machine learing)\n인공지능 작동방식. 투입한 데이터에서 규칙성 탐지해 분류 및 예측. 지도학습, 비지도학습, 강화학습 등으로 구분.\n인공지능 작동방식. 투입한 데이터에서 규칙성 탐지해 분류 및 예측. 지도학습, 비지도학습, 강화학습 등으로 구분.지도학습(supervised learning)\n인간이 사전에 분류한 결과를 학습해 투입한 자료에서 규칙성 혹은 경향 발견\n인간이 사전에 분류한 결과를 학습해 투입한 자료에서 규칙성 혹은 경향 발견비지도학습(unsupervised learning)\n사전분류한 결과 없이 기계 스스로 투입한 자료에서 규칙성 혹은 경향 발견\n사전분류한 결과 없이 기계 스스로 투입한 자료에서 규칙성 혹은 경향 발견강화학습(reinforced learning)\n행동의 결과에 대한 피드백(보상, 처벌 등)을 통해 투입한 자료에서 규칙성 혹은 경향 발견\n행동의 결과에 대한 피드백(보상, 처벌 등)을 통해 투입한 자료에서 규칙성 혹은 경향 발견주제모형의 효용은 대량의 문서에서 의미구조를 닮은 주제구조를 추론해 주석을 자동으로 부여할 수 있다는데 있다.주제모형은 다양한 패지키가 있다.ldatopicmodelsstm여기서는 구조적 주제모형(structural topic model)이 가능한 stm패키지를 이용한다. stm은 메타데이터를 이용한 추출한 주제에 대하여 다양한 분석을 할 수 있는 장점이 있다.","code":"\nap_v <- c(\"The William Randolph Hearst Foundation will give $1.25 million to Lincoln Center, Metropolitan Opera Co., New York Philharmonic and Juilliard School. “Our board felt that we had a\nreal opportunity to make a mark on the future of the performing arts with these grants an act\nevery bit as important as our traditional areas of support in health, medical research, education\nand the social services,” Hearst Foundation President Randolph A. Hearst said Monday in\nannouncing the grants. Lincoln Center’s share will be $200,000 for its new building, which\nwill house young artists and provide new public facilities. The Metropolitan Opera Co. and\nNew York Philharmonic will receive $400,000 each. The Juilliard School, where music and\nthe performing arts are taught, will get $250,000. The Hearst Foundation, a leading supporter\nof the Lincoln Center Consolidated Corporate Fund, will make its usual annual $100,000\ndonation, too.\")\npkg_v <- c(\"tidyverse\", \"tidytext\", \"tidylo\")\nlapply(pkg_v, require, ch = T)## [[1]]\n## [1] TRUE\n## \n## [[2]]\n## [1] TRUE\n## \n## [[3]]\n## [1] TRUE\n\nap_count <- \nap_v %>% tibble(text = .) %>% \n  unnest_tokens(word, text, drop = F) %>% \n  anti_join(stop_words) %>% \n  count(word, sort = T) %>% head(10)\nap_tfidf <- \nap_v %>% tibble(text = .) %>% \n  mutate(text = str_squish(text)) %>% \n  unnest_tokens(sentence, text, token = \"regex\", pattern = \"\\\\.\\\\s(?=[:upper:])\") %>% \n  mutate(ID = row_number()) %>% \n  unnest_tokens(word, sentence, drop = F) %>% \n  anti_join(stop_words) %>% \n  count(ID, word, sort = T) %>% \n  bind_tf_idf(term = word, document = ID, n = n) %>% \n  arrange(-tf_idf) %>% head(10)\n\nap_wlo <- \nap_v %>% tibble(text = .) %>% \n  mutate(text = str_squish(text)) %>% \n  unnest_tokens(sentence, text, token = \"regex\", pattern = \"\\\\.\\\\s(?=[:upper:])\") %>% \n  mutate(ID = row_number()) %>% \n  unnest_tokens(word, sentence, drop = F) %>% \n  anti_join(stop_words) %>% \n  count(ID, word, sort = T) %>% \n  bind_log_odds(feature = word, set = ID, n = n) %>% \n  arrange(-log_odds_weighted) %>% head(10)\nbind_cols(\n  select(ap_count,  총빈도  = word),\n  select(ap_tfidf, tf_idf = word),\n  select(ap_wlo,  가중승산비  = word)\n)## # A tibble: 10 × 3\n##   총빈도     tf_idf     가중승산비  \n##   <chr>      <chr>      <chr>       \n## 1 hearst     announcing hearst      \n## 2 foundation monday     grants      \n## 3 lincoln    400,000    announcing  \n## 4 arts       receive    monday      \n## 5 center     grants     metropolitan\n## 6 grants     250,000    opera       \n## # … with 4 more rows\n"},{"path":"topic-modeling.html","id":"자료-준비-1","chapter":"8 .  주제모형","heading":"8.2 자료 준비","text":"빅카인즈에서 다음의 조건으로 기사를 추출한다.검색어: 인공지능기간: 2010.1.1 ~ 2020.12.31언론사: 중앙일보, 조선일보, 한겨레, 경향신문, 한국경제, 매일경제분석: 분석 기사모두 5145건이다.","code":""},{"path":"topic-modeling.html","id":"자료-이입","chapter":"8 .  주제모형","heading":"8.2.1 자료 이입","text":"다운로드한 기사를 작업디렉토리 아래 data폴더에 복사한다.파일명이 ’NewsResult_20200101-20201201.xlsx’다.분석에 필요한 열을 선택해 데이터프레임으로 저장한다. 분석 텍스트는 제목과 본문이다. 빅카인즈는 본문을 200자까지만 무료로 제공하지만, 학습 목적을 달성하기에는 충분하다. 제목은 본문의 핵심 내용을 반영하므로, 제목과 본문을 모두 주제모형 분석에 투입한다. 시간별, 언론사별, 분류별로 분석할 계획이므로, 해당 열을 모두 선택한다.시간열에는 연월일의 값이 있다. 월별 추이에 따른 주제를 분석할 것이므로 열의 값을 월에 대한 값으로 바꾼다. tidyverse패키지에 함께 설치되는 lubridate패키지를 이용해 문자열을 날짜형으로 변경하고 월 데이터 추출해 새로운 열 month 생성한다. lubridate는 tidyverse에 포함돼 있으나 함께 부착되지 않으므로 별도로 실행해야 한다.DB에 같은 기사가 중복 등록되는 경우가 있으므로, dplyr패키지의 distinct()함수를 이용해 중복된 문서를 제거한다. .keep_all =인자의 기본값은 FALSE다. 투입한 열 이외의 열은 유지하지 않는다. 다른 변수(열)도 분석에 필요하므로 .keep_all =인자를 TRUE로 지정한다.분석 목적에 맞게 열을 재구성한다.기사의 분류된 종류, 월 등 새로 생성한 열의 내용을 확인해보자.분류별, 월별로 기사의 양을 계산해보자.","code":"\nlist.files(\"data/.\")##  [1] \"bunn.epub\"                            \n##  [2] \"dfm.rds\"                              \n##  [3] \"jikji.epub\"                           \n##  [4] \"knusenti.zip\"                         \n##  [5] \"KnuSentiLex-master\"                   \n##  [6] \"KoreanPOStags.xlsx\"                   \n##  [7] \"meta_fit.rds\"                         \n##  [8] \"moby.epub\"                            \n##  [9] \"newsData\"                             \n## [10] \"newsData.zip\"                         \n## [11] \"NewsResult_20200101-20201231.xlsx\"    \n## [12] \"NewsResult_20200601-20210531_hdm.xlsx\"\n## [13] \"NewsResult_20200601-20210531_tsl.xlsx\"\n## [14] \"NewsResult_20210101-20210330.xlsx\"    \n## [15] \"NewsResult_20210201-20210228.xlsx\"    \n## [16] \"NewsResult_20210330-20210330.xlsx\"    \n## [17] \"ogamdo.txt\"                           \n## [18] \"ogamdo2.txt\"                          \n## [19] \"poliblogs2008.csv\"                    \n## [20] \"ratings.txt\"                          \n## [21] \"sense.epub\"                           \n## [22] \"topic.csv\"                            \n## [23] \"yoon.txt\"                             \n## [24] \"취임사_문재인.txt\"                    \n## [25] \"취임사_윤석열.txt\"\n\nreadxl::read_excel(\"data/NewsResult_20200101-20201231.xlsx\") %>% names()##  [1] \"뉴스 식별자\"                 \n##  [2] \"일자\"                        \n##  [3] \"언론사\"                      \n##  [4] \"기고자\"                      \n##  [5] \"제목\"                        \n##  [6] \"통합 분류1\"                  \n##  [7] \"통합 분류2\"                  \n##  [8] \"통합 분류3\"                  \n##  [9] \"사건/사고 분류1\"             \n## [10] \"사건/사고 분류2\"             \n## [11] \"사건/사고 분류3\"             \n## [12] \"인물\"                        \n## [13] \"위치\"                        \n## [14] \"기관\"                        \n## [15] \"키워드\"                      \n## [16] \"특성추출(가중치순 상위 50개)\"\n## [17] \"본문\"                        \n## [18] \"URL\"                         \n## [19] \"분석제외 여부\"\n\nai_df <- \nreadxl::read_excel(\"data/NewsResult_20200101-20201231.xlsx\") %>% \n  select(일자, 제목, 본문, 언론사, cat = `통합 분류1`) \nai_df %>% head()## # A tibble: 6 × 5\n##   일자     제목                           본문  언론사 cat  \n##   <chr>    <chr>                          <chr> <chr>  <chr>\n## 1 20201231 \"`사모펀드 책임` 놓고 새해부…  \"은…  매일…  경제…\n## 2 20201231 \"[이광석의 디지털 이후](25)시… \"ㆍ…  경향…  IT_… \n## 3 20201231 \"\\\"소처럼 묵묵하게 경제 통합 … \"역…  매일…  지역…\n## 4 20201231 \"GS건설, `강릉자이 파인베뉴` … \"GS…  매일…  경제…\n## 5 20201231 \"디지털 강조한 금융협회장들 …  \"\\\"…  매일…  경제…\n## 6 20201231 \"정치인 거짓말도 진실로 둔갑 … \"◆ …  매일…  IT_…\n\nlibrary(lubridate)\nas_date(\"20201231\") %>% month()## [1] 12\n\nymd(\"20201231\") %>% month()## [1] 12\n\nlibrary(lubridate) \n\nai2_df <- \nai_df %>% \n  # 중복기사 제거\n  distinct(제목, .keep_all = T) %>% \n  # 기사별 ID부여\n  mutate(ID = factor(row_number())) %>% \n  # 월별로 구분한 열 추가\n  mutate(month = month(ymd(일자))) %>% \n  # 기사 제목과 본문 결합\n  unite(제목, 본문, col = \"text\", sep = \" \") %>% \n  # 중복 공백 제거\n  mutate(text = str_squish(text)) %>% \n  # 언론사 분류: 보수 진보 경제 %>% \n  mutate(press = case_when(\n    언론사 == \"조선일보\" ~ \"종합지\",\n    언론사 == \"중앙일보\" ~ \"종합지\",\n    언론사 == \"경향신문\" ~ \"종합지\",\n    언론사 == \"한겨레\" ~ \"종합지\",\n    언론사 == \"한국경제\" ~ \"경제지\",\n    TRUE ~ \"경제지\") ) %>% \n  # 기사 분류 구분 \n  separate(cat, sep = \">\", into = c(\"cat\", \"cat2\")) %>% \n  # IT_과학, 경제, 사회 만 선택\n  filter(str_detect(cat, \"IT_과학|경제|사회\")) %>% \n  select(-cat2)  \n\nai2_df %>% head(5)## # A tibble: 5 × 7\n##   일자     text               언론사 cat   ID    month press\n##   <chr>    <chr>              <chr>  <chr> <fct> <dbl> <chr>\n## 1 20201231 \"`사모펀드 책임` … 매일…  경제  1        12 경제…\n## 2 20201231 \"[이광석의 디지털… 경향…  IT_…  2        12 종합…\n## 3 20201231 \"GS건설, `강릉자…  매일…  경제  4        12 경제…\n## 4 20201231 \"디지털 강조한 금… 매일…  경제  5        12 경제…\n## 5 20201231 \"정치인 거짓말도 … 매일…  IT_…  6        12 경제…\n\nai2_df %>% names()## [1] \"일자\"   \"text\"   \"언론사\" \"cat\"    \"ID\"     \"month\" \n## [7] \"press\"\n\nai2_df$cat %>% unique()## [1] \"경제\"    \"IT_과학\" \"사회\"\n\nai2_df$month %>% unique()##  [1] 12 11 10  9  8  7  6  5  4  3  2  1\n\nai2_df$press %>% unique()## [1] \"경제지\" \"종합지\"\n\nai2_df %>% count(cat, sort = T)## # A tibble: 3 × 2\n##   cat         n\n##   <chr>   <int>\n## 1 IT_과학  1922\n## 2 경제     1409\n## 3 사회      460\n\nai2_df %>% count(month, sort = T)## # A tibble: 12 × 2\n##   month     n\n##   <dbl> <int>\n## 1     1   382\n## 2     9   379\n## 3    12   366\n## 4     7   345\n## 5     6   335\n## 6     5   334\n## # … with 6 more rows\n\nai2_df %>% count(press, sort = T)## # A tibble: 2 × 2\n##   press      n\n##   <chr>  <int>\n## 1 경제지  2049\n## 2 종합지  1742\n"},{"path":"topic-modeling.html","id":"정제-2","chapter":"8 .  주제모형","heading":"8.2.2 정제","text":"","code":""},{"path":"topic-modeling.html","id":"토큰화-1","chapter":"8 .  주제모형","heading":"8.2.2.1 토큰화","text":"KoNLP패키지의 extractNoun()함수로 명사만 추출해 토큰화한다. 명사가 문서의 주제를 잘 나타내므로 주제모형에서는 주로 명사를 이용하지만, 목적에 따라서는 다른 품사(용언 등)를 분석에 투여하기도 한다.(* 형태소 추출전에 문자 혹은 공백 이외의 요소(예: 구둣점)를 먼저 제거한다.)","code":"\n# library(KoNLP)\n# ai_tk <- \n# ai2_df %>% \n#   mutate(text = str_remove_all(text, \"[^(\\\\w+|\\\\s)]\")) %>%  # 문자 혹은 공백 이외 것 제거\n#   unnest_tokens(word, text, token = extractNoun, drop = F) \n# ai_tk %>% glimpse()"},{"path":"topic-modeling.html","id":"불용어-제거","chapter":"8 .  주제모형","heading":"8.2.2.2 불용어 제거","text":"’인공지능’으로 검색한 기사이므로, ’인공지능’관련 단어는 제거한다.\n문자가 아닌 요소를 모두 제거한다. (숫자를 반드시 제거해야 하는 것은 아니다.)단어의 총빈도와 상대빈도를 살펴보자상대빈도가 높은 단어와 낮은 단어를 확인한다.한글자 단어는 문서의 주제를 나타내는데 기여하지 못하는 경우도 있고, 고유명사인데 형태소로 분리돼 있는 경우도 있다. 상대빈도가 높은 단어를 살펴 특이한 단어가 있으면 형태소 추출전 단어가 무엇인지 확인한다. 특이한 경우가 없으면 한글자 단어는 모두 제거한다.\ntibble데이터프레임은 문자열의 일부만 보여준다. pull()함수로 열에 포함된 문자열을 벡터로 출력하므로, 모든 내용을 확인할 수 있다.‘기업’ ‘기술’ 등의 단어는 총사용빈도가 높지만, 상대빈도는 낮다. 대부분의 분류에서 널리 사용된 단어다. 지금 제거할필요는 없지만, 제거가능성을 염두에 둔다.상대빈도를 다시 확인하자.","code":"\nai_tk <- \nai_tk %>% \n  filter(!word %in% c(\"인공지능\", \"AI\", \"ai\", \"인공지능AI\", \"인공지능ai\")) %>% \n  filter(str_detect(word, \"[:alpha:]+\")) \nai_tk %>% count(word, sort = T)\nai_tk %>% count(cat, word, sort = T) %>% \n  bind_log_odds(set = cat, feature = word, n = n) %>% \n  arrange(log_odds_weighted)\n\nai_tk %>% count(cat, word, sort = T) %>% \n  bind_tf_idf(term = word, document = word, n = n) %>% \n  arrange(idf)\nai_tk %>% \n  filter(word == \"하\") %>% pull(text) %>% head(3)\nai_tk %>% \n  filter(str_length(word) > 1) -> ai2_tk\n\nai2_tk %>% \n  count(word, sort = T) \nai2_tk %>% count(cat, word, sort = T) %>% \n  bind_log_odds(set = cat, feature = word, n = n) %>% \n  arrange(-log_odds_weighted)\n\nai2_tk %>% count(cat, word, sort = T) %>% \n  bind_tf_idf(term = word, document = word, n = n) %>% \n  arrange(tf_idf)"},{"path":"topic-modeling.html","id":"stm-말뭉치","chapter":"8 .  주제모형","heading":"8.2.3 stm 말뭉치","text":"토큰화한 데이터프레임을 stm패키지 형식의 말뭉치로 변환한다. 이를 위해 먼저 분리된 토큰을 원래 문장에 속한 하나의 열로 저장한다.str_flatten()은 str_c()함수와 달리, 문자열을 결합해 단일 요소로 산출한다.\nstr_c()함수에 collapse =인자를 사용한 경우에 해당한다.\nstr_c()는 R기본함수 paste0()와 비슷하다. 결측값 처리방법이 서로 다르다.\nstr_c()함수에 collapse =인자를 사용한 경우에 해당한다.str_c()는 R기본함수 paste0()와 비슷하다. 결측값 처리방법이 서로 다르다.textProcessor()함수로 리스트 형식의 stm말뭉치로 변환한다. ‘documents’ ‘vacab’ ’meta’등의 하부요소가 생성된다. ’meta’에 텍스트데이터가 저장돼 있다.영문문서의 경우, textProcessor()함수로 정제과정을 수행하므로, 한글문서처럼 별도의 형태소 추출과정 바로 영문데이터프레임을 투입하면 된다.’text2’열에 토큰화한 단어가 저장돼 있다.만일 tm패키지나 SnowballC패키지가 설치돼 있지 않으면prepDocuments()함수로 주제모형에 사용할 데이터의 인덱스(wordcounts)를 만든다.제거할수 있는 단어와 문서의 수를 plotRemoved()함수로 확인할 수 있다.lower.thresh =로 최소값 설정하면 빈도가 낮아 제거할 용어의 수를 설정할 수 있다. 설정값을 너무 높게 잡으면 분석의 정확도가 떨어진다. 여기서는 계산 편의를 위해 설정값을 높게 잡았다.산출결과를 개별 객체로 저장한다. 이 객체들은 이후 모형구축에 사용된다.","code":"\ninstall.packages(\"stm\", dependencies = T)\ncombined_df <-\n  ai2_tk %>%\n  group_by(ID) %>%\n  summarise(text2 = str_flatten(word, \" \")) %>%\n  ungroup() %>% \n  inner_join(ai2_df, by = \"ID\")\n\ncombined_df %>% glimpse()\nlibrary(stm)\nprocessed <- \n  ai2_df %>% textProcessor(documents = combined_df$text2, metadata = .)\nout <- \n  prepDocuments(processed$documents,\n                     processed$vocab,\n                     processed$meta)\nplotRemoved(processed$documents, lower.thresh = seq(0, 100, by = 5))\nout <-\n  prepDocuments(processed$documents,\n                processed$vocab,\n                processed$meta, \n                lower.thresh = 15)\ndocs <- out$documents\nvocab <- out$vocab\nmeta <- out$meta"},{"path":"topic-modeling.html","id":"분석-1","chapter":"8 .  주제모형","heading":"8.3 분석","text":"","code":""},{"path":"topic-modeling.html","id":"주제의-수k-설정","chapter":"8 .  주제모형","heading":"8.3.1 주제의 수(K) 설정","text":"주제를 몇개로 설정할지 탐색한다. 7개와 10개를 놓고 비교해보자. searchK()함수는 주제의 수에 따라 4가지 계수를 제공한다.배타성(exclusivity): 특정 주제에 등장한 단어가 다른 주제에는 나오지 않는 정도. 확산타당도에 해당.배타성(exclusivity): 특정 주제에 등장한 단어가 다른 주제에는 나오지 않는 정도. 확산타당도에 해당.의미 일관성(semantic coherence): 특정 주제에 높은 확률로 나타나는 단어가 동시에 등장하는 정도. 수렴타당도에 해당.의미 일관성(semantic coherence): 특정 주제에 높은 확률로 나타나는 단어가 동시에 등장하는 정도. 수렴타당도에 해당.지속성(heldout likelihood): 데이터 일부가 존재하지 않을 때의 모형 예측 지속 정도.지속성(heldout likelihood): 데이터 일부가 존재하지 않을 때의 모형 예측 지속 정도.잔차(residual): 투입한 데이터로 모형을 설명하지 못하는 오차.잔차(residual): 투입한 데이터로 모형을 설명하지 못하는 오차.배타성, 의미 일관성, 지속성이 높을 수록, 그리고 잔차가 작을수록 모형의 적절성 증가.보통 10개부터 100개까지 10개 단위로 주제의 수를 구분해 연구자들이 정성적으로 최종 주제의 수 판단한다.학습 상황이므로 계산시간을 줄이기 위해 주제의 수를 3개와 10개의 결과만 비교한다. (iteration을 200회 이상 수행하므로 계산시간이 오래 걸린다.)백그라운드에서 계산하도록 하면, RStudio에서 다른 작업을 병행할 수 있다.RStudio 코드 백그라운드에서 돌리기배타성, 지속성, 잔차 등 3개 지표에서 모두 주제의 수가 10개인 모형이 3개인 모형보다 우수하고, 3개인 모형은 의미일관성만 높다. 따라서 이미 10개로 분석한 모형을 그대로 이용한다.","code":"\n# topicN <- seq(from = 10, to = 100, by = 10)\ntopicN <- c(3, 10)\n\nstorage <- searchK(out$documents, out$vocab, K = topicN)\nplot(storage)"},{"path":"topic-modeling.html","id":"주제모형-구성","chapter":"8 .  주제모형","heading":"8.3.2 주제모형 구성","text":"docs, vocab, meta에 저장된 문서와 텍스트정보를 이용해 주제모형을 구성한다. 추출한 주제의 수는 K =인자로 설정한다. 처음에는 임의의 값을 투입한다. 이후 적절한 주제의 수를 다시 추정하는 단계가 있다.\n모형 초기값은 init.type =인자에 “Spectral”을 투입한다. 같은 결과가 나오도록 하려면 seed =인자를 지정한다. 투입하는 값은 개인의 선호대로 한다.stm패키지는 4종의 가중치를 이용해 주제별로 주요 단어를 제시한다.Highest probability: 각 주제별로 단어가 등장할 확률이 높은 정도. 베타() 값.FREX: 전반적인 단어빈도의 가중 평균. 해당 주제에 배타적으로 포함된 정도.Lift: 다른 주제에 덜 등장하는 정도. 해당 주제에 특정된 정도.score: 다른 주제에 등장하는 로그 빈도. 해당 주제에 특정된 정도.FREX와 Lift는 드문 빈도의 단어도 분류하는 경향이 있으므로, 주로 Highest probability(베타)를 이용한다.stm패키지의 자세한 내용은 패키지 저자 Roberts의 stm홈페이지와 해설 논문을 참조한다.stm: R Package Structural Topic Modelsstm: R Package Structural Topic ModelsRoberts, M. E., Stewart, B. M., & Airoldi, E. M. (2016). model text experimentation social sciences. Journal American Statistical Association, 111(515), 988-1003.Roberts, M. E., Stewart, B. M., & Airoldi, E. M. (2016). model text experimentation social sciences. Journal American Statistical Association, 111(515), 988-1003.","code":"\nstm_fit <-\n  stm(\n    documents = docs,\n    vocab = vocab,\n    K = 10,    # 토픽의 수\n    data = meta,\n    init.type = \"Spectral\",\n    seed = 37 # 반복실행해도 같은 결과가 나오게 난수 고정\n  )\n\nsummary(stm_fit) %>% glimpse()\nsummary(stm_fit)"},{"path":"topic-modeling.html","id":"소통-1","chapter":"8 .  주제모형","heading":"8.4 소통","text":"분석한 모형을 통해 말뭉치에 포함된 주제와 주제별 단어의 의미가 무엇인지 전달하기 위해서는 우선 모형에 대한 해석을 제시할 수 있는 시각화가 필요하다. 이를 위해서는 먼저 주요 계수의 의미를 이해할 필요가 있다. 주제모형에서 주제별 확률분포를 나타내는 베타와 감마다.베타 \\(\\beta\\): 단어가 각 주제에 등장할 확률. 각 단어별로 베타 값 부여. stm모형 요약에서 제시한 Highest probability의 지표다.베타 \\(\\beta\\): 단어가 각 주제에 등장할 확률. 각 단어별로 베타 값 부여. stm모형 요약에서 제시한 Highest probability의 지표다.감마 \\(\\gamma\\): 문서가 각 주제에 등장할 확률. 각 문서별로 감마 값 부여.감마 \\(\\gamma\\): 문서가 각 주제에 등장할 확률. 각 문서별로 감마 값 부여.즉, 베타와 감마 계수를 이용해 시각화하면 주제와 주제단어의 의미를 간명하게 나타낼 수 있다.먼저 tidy()함수를 이용해 stm()함수로 주제모형을 계산한 결과를 정돈텍스트 형식으로 변환한다(줄리아 실기의 시각화 참고)학습편의를 위해 주제의 수롤 6개로 조정해 다시 모형을 구성하자.","code":"\nstm_fit <-\n  stm(\n    documents = docs,\n    vocab = vocab,\n    K = 6,    # 토픽의 수\n    data = meta,\n    init.type = \"Spectral\",\n    seed = 37, # 반복실행해도 같은 결과가 나오게 난수 고정\n    verbose = F\n  )\n\nsummary(stm_fit) %>% glimpse()\nsummary(stm_fit)"},{"path":"topic-modeling.html","id":"주제별-단어-분포","chapter":"8 .  주제모형","heading":"8.4.1 주제별 단어 분포","text":"베타 값을 이용해 주제별로 단어의 분포를 막대도표로 시각화하자.","code":"\ntd_beta <- stm_fit %>% tidy(matrix = 'beta') \n\ntd_beta %>% \n  group_by(topic) %>% \n  slice_max(beta, n = 7) %>% \n  ungroup() %>% \n  mutate(topic = str_c(\"주제\", topic)) %>% \n  \n  ggplot(aes(x = beta, \n             y = reorder(term, beta),\n             fill = topic)) +\n  geom_col(show.legend = F) +\n  facet_wrap(~topic, scales = \"free\") +\n  labs(x = expression(\"단어 확률분포: \"~beta), y = NULL,\n       title = \"주제별 단어 확률 분포\",\n       subtitle = \"각 주제별로 다른 단어들로 군집\") +\n  theme(plot.title = element_text(size = 20))"},{"path":"topic-modeling.html","id":"주제별-문서-분포","chapter":"8 .  주제모형","heading":"8.4.2 주제별 문서 분포","text":"감마 값을 이용해 주제별로 문서의 분포를 히스토그램으로 시각화한다. x축과 y축에 각각 변수를 투입하는 막대도표와 달리, 히스토그램은 x축에만 변수를 투입하고, y축에는 x축 값을 구간(bin)별로 요약해 표시한다.각 문서가 각 주제별로 등장할 확률인 감마(\\(\\gamma\\))의 분포가 어떻게 히스토그램으로 표시되는지 살펴보자.td_gamma에는 문서별로 감마 값이 부여돼 있다. 1번 문서는 주제1에 포함될 확률(감마)이 0.4이고, 2번 문서는 주제1에 포함될 확률이 0.2다. 감마 값은 최저 0.04에서 최고 0.74까지 있다. 감마는 연속적인 값이므로 이 감마의 값을 일정한 구간(bin)으로 나누면, 각 감마의 구간에 문서(document)가 몇개 있는지 계산해, 감마 값에 따른 각 문서의 분포를 구할 수 있다. 연속하는 값을 구간(bin)으로 구분해 분포를 표시한 도표가 히스토그램이다.주제별로 문서의 분포를 감마 값에 따라 히스토그램으로 시각해하자. geom_histogram()함수에서 bins =인자의 기본값은 30이다. 즉, bin을 30개로 나눠 분포를 그린다.감마가 높은 문서(기사)가 많지 않고, 대부분 낮은 값에 치우쳐 있다. ‘인공지능’ 단일 검색어로 추출한 말뭉치이기 때문이다.","code":"\ntd_gamma <- stm_fit %>% tidy(matrix = \"gamma\") \ntd_gamma %>% glimpse()\ntd_gamma %>% \n  mutate(max = max(gamma),\n         min = min(gamma),\n         median = median(gamma))\ntd_gamma %>% \n  ggplot(aes(x = gamma, fill = as.factor(topic))) +\n  geom_histogram(bins = 100, show.legend = F) +\n  facet_wrap(~topic) + \n  labs(title = \"주제별 문서 확률 분포\",\n       y = \"문서(기사)의 수\", x = expression(\"문서 확률분포: \"~(gamma))) +\n  theme(plot.title = element_text(size = 20))"},{"path":"topic-modeling.html","id":"주제별-단어-문서-분포","chapter":"8 .  주제모형","heading":"8.4.3 주제별 단어-문서 분포","text":"주제별로 감마의 평균값을 구하면 비교적 각 주제와 특정 문서와 관련성이 높은 순서로 주제를 구분해 표시할 수 있다. 또한 각 주제 별로 대표 단어를 표시할 수 있다. 가장 간단하게 주제별로 단어와 문서의 분포를 표시하는 방법은 stm패키지에서 제공하는 plot()함수다.stm plot()함수는 type =인자에 ‘summary’ ‘labels’ ‘perspective’ ‘hist’ 등을 투입해 다양한 방식으로 결과를 탐색할 수 있다. 기본적인 정보는 ’summary’를 통해 제시한다.위 결과를 ggplot2 패키지로 시각화하는 방법은 다음과 같다.주제별 상위 5개 단어 추출해 데이터프레임에 저장.문서의 감마 평균값 주제별로 계산해 주제별 상위 단어 데이터프레임과 결합","code":"\nplot(stm_fit, type = \"summary\", n = 5)"},{"path":"topic-modeling.html","id":"주제별-상위-5개-단어-추출","chapter":"8 .  주제모형","heading":"8.4.3.1 주제별 상위 5개 단어 추출","text":"td_beta에서 주제별로 상위 5개 단어 추출해 top_terms에 할당한다. 각 주제별로 그룹을 묶어 list형식으로 각 상위단어 5개를 각 주제에 리스트로 묶어준 다음, 다시 데이터프레임의 열로 바꿔준다.","code":"\ntop_terms <- \ntd_beta %>% \n  group_by(topic) %>% \n  slice_max(beta, n = 5) %>% \n  select(topic, term) %>% \n  summarise(terms = str_flatten(term, collapse = \", \")) "},{"path":"topic-modeling.html","id":"주제별-감마-평균-계산","chapter":"8 .  주제모형","heading":"8.4.3.2 주제별 감마 평균 계산","text":"td_gamma에서 각 주제별 감마 평균값 계산해 top_terms(주제별로 추출한 상위 5개 단어 데이터프레임)와 결합해 gamma_terms에 할당한다.결합한 데이터프레임을 막대도표에 표시한다. 문서 확률분포 평균값과 주제별로 기여도가 높은 단어를 표시한다. 주제별로 문서의 확률분포와 단어의 확률분포를 한눈에 볼수 있다.\nX축을 0에서 1까지 설정한 이유는 구간을 국소로 설정할 경우, 막대도표가 크기가 상대적으로 크게 보여 결과적으로 데이터의 왜곡이 되기 때문이다.","code":"\ngamma_terms <- \ntd_gamma %>% \n  group_by(topic) %>% \n  summarise(gamma = mean(gamma)) %>% \n  left_join(top_terms, by = 'topic') %>% \n  mutate(topic = str_c(\"주제\", topic),\n         topic = reorder(topic, gamma))\ngamma_terms %>% \n  \n  ggplot(aes(x = gamma, y = topic, fill = topic)) +\n  geom_col(show.legend = F) +\n  geom_text(aes(label = round(gamma, 2)), # 소수점 2자리 \n            hjust = 1.4) +                # 라벨을 막대도표 안쪽으로 이동\n  geom_text(aes(label = terms), \n            hjust = -0.05) +              # 단어를 막대도표 바깥으로 이동\n  scale_x_continuous(expand = c(0, 0),    # x축 막대 위치를 Y축쪽으로 조정\n                     limit = c(0, 1)) +   # x축 범위 설정\n  labs(x = expression(\"문서 확률분포\"~(gamma)), y = NULL,\n       title = \"인공지능 관련보도 상위 주제어\",\n       subtitle = \"주제별로 기여도가 높은 단어 중심\") +\n  theme(plot.title = element_text(size = 20))"},{"path":"topic-modeling.html","id":"과제-3","chapter":"8 .  주제모형","heading":"8.4.4 과제","text":"관심있는 검색어를 이용해 빅카인즈에서 기사를 검색해 수집한 기사의 주제모형을 구축한다.관심있는 검색어를 이용해 빅카인즈에서 기사를 검색해 수집한 기사의 주제모형을 구축한다.구축한 주제 모형에 대해 시각화한다(주제별 단어분포, 주제별 문서분포, 주제별 단어-문서 분포)구축한 주제 모형에 대해 시각화한다(주제별 단어분포, 주제별 문서분포, 주제별 단어-문서 분포)","code":""},{"path":"anal4topic.html","id":"anal4topic","chapter":"9 .  주제모형(공변인)","heading":"9 .  주제모형(공변인)","text":"","code":""},{"path":"anal4topic.html","id":"주제-명명과-공변인-주제모형","chapter":"9 .  주제모형(공변인)","heading":"9.1 주제 명명과 공변인 주제모형","text":"","code":""},{"path":"anal4topic.html","id":"개관-1","chapter":"9 .  주제모형(공변인)","heading":"9.1.1 개관","text":"주제모형은 기계학습의 비지도학습에 해당한다. 기계가 인간의 ‘지도’를 받지 않고 ’스스로’ 자료에서 일정한 규칙을 찾아 비슷한 유형끼리 군집하는 학습방식이다. 인간이 데이터셋을 미리 분류한 정보를 투입하지 않기 때문에, 기계가 분류한 군집에 대해 인간이 사후적으로 의미를 추론해야 한다. 이번 장에서는 기계가 도출한 각 주제의 주요 단어와 문서를 통해 주제의 의미를 추론하는 방법으로서의 주제 명명에 대해 학습한다.이와 함께, 메타데이터를 이용한 공변인(covariate) 주제모형 분석에 대해서도 학습한다. 메타데이터는 데이터에 대한 데이터다. 예를 들어, 말뭉치에 포함된 문서의 유형(예: 소설, 논설), 분류(예: 사회면, 정치면), 소속(예: 언론사), 시기(예: 연, 월, 주)에 대한 정보가 메타데이터다. 이 메타데이터를 변수로서 투입해 분석하면 말뭉치의 주제에 대해 보다 의미있는 분석이 가능하다.예를 들어, 기간대별로 말뭉치의 주제가 어떻게 변하는지, 혹은 문서의 분류에 따라 주제가 어떻게 다른지 등을 분석할 수 있다. 주제의 구조적인 측면은 다룬다고 해서 구조적 주제모형(structural topic models)이라고 한다.공변인을 투입한 주제모형 분석이므로 여기서는 동적 주제모형과 구조적 주제모형을 모두 공변인 주제모형(Covariates topic models)이라고 하겠다.stm패키지와 keyATM패키지는 말뭉치의 메타데이터를 공변인으로 투입해 말뭉치의 주제에 대한 회귀분석 기능을 제공한다. keyATM이 보다 최근의 패키지라 보다 다양한 기능이 있지만, 윈도에서 멀티바이트문자를 지원하지 않아 윈도에서는 한글문서를 분석할 수 없다는 단점이 있다.","code":"\npkg_v <- c(\"tidyverse\", \"tidytext\", \"stm\", \"lubridate\")\npurrr::map(pkg_v, require, ch = T)## [[1]]\n## [1] TRUE\n## \n## [[2]]\n## [1] TRUE\n## \n## [[3]]\n## [1] TRUE\n## \n## [[4]]\n## [1] TRUE\n"},{"path":"anal4topic.html","id":"자료-준비-2","chapter":"9 .  주제모형(공변인)","heading":"9.2 자료 준비","text":"","code":""},{"path":"anal4topic.html","id":"수집-2","chapter":"9 .  주제모형(공변인)","heading":"9.2.1 수집","text":"빅카인즈의 ‘뉴스분석’ 메뉴에서 ‘뉴스검색·분석’을 선택한 다음, ’상세검색’을 클릭한다. 상세검색은 다양한 기준으로 검색할 수 있다. 검색유형 기본값은 ’뉴스’, 검색어처리 기본값은 ‘형태소분석’, 검색어범위 기본값은 ’제목+본문’이다. 모두 기본값으로 검색한다.기간: 2021-01-01 - 2021-03-31검색어: ‘백신’, ‘코로나19’, ‘신종 코로나’, ‘신종코로나’, ‘우한 폐렴’, ‘우한폐렴’, ‘바이러스’ (쉼표로 분리하면 각 검색어를 ‘’ 연산자로 검색이 된다.)언론사: 경향신문, 조선일보, 중앙일보, 한겨레통합분류: 정치, 경제, 사회, 국제, 지역, IT_과학분석: 분석기사 (분석기사를 선택하면 중복(반복되는 유사도 높은 기사)과 예외(인사 부고 동정 포토)가 검색에서 제외된다.모두 14,097건이다.다운로드한 엑셀파일을 작업디렉토리 아래 data폴더로 복사한다. data폴더에서 ‘News’로 시작해서’.xlsx’로 끝나는 파일명만 표시해 보자.데이터셋의 파일명이 ’NewsResult_20210101-20210330.xlsx’이다.분석에 필요한 열을 선택해 데이터프레임으로 저장한다. 분석 텍스트는 제목과 본문이다. 제목은 본문의 핵심 내용을 반영하므로, 제목과 본문을 모두 주제모형 분석에 투입한다. 시간별, 언론사별, 분류별로 분석할 계획이므로, 해당 열을 모두 선택한다. 키워드 열은 빅카인즈가 본문에서 추출한 키워드 중 단순 숫자, 이메일주소, 시간이 아닌 단어 등이다. 빅카인즈의 형태소분석결과를 이용할 계획이므로 키워드 열도 선택한다. 빅카인즈는 본문을 200자까지만 무료로 제공하지만, 빅카인즈에서 형태소분석을 통해 추출한 키워드는 기사 전문에서 추출한 결과다. 키워드를 이용하면 기사 전문을 이용하는 효과가 있다.","code":"\nlist.files(path = 'data', pattern = '^News.*20210101.*\\\\.xlsx$')## [1] \"NewsResult_20210101-20210330.xlsx\"\n\nreadxl::read_excel(\"data/NewsResult_20210101-20210330.xlsx\") %>% names()##  [1] \"뉴스 식별자\"                 \n##  [2] \"일자\"                        \n##  [3] \"언론사\"                      \n##  [4] \"기고자\"                      \n##  [5] \"제목\"                        \n##  [6] \"통합 분류1\"                  \n##  [7] \"통합 분류2\"                  \n##  [8] \"통합 분류3\"                  \n##  [9] \"사건/사고 분류1\"             \n## [10] \"사건/사고 분류2\"             \n## [11] \"사건/사고 분류3\"             \n## [12] \"인물\"                        \n## [13] \"위치\"                        \n## [14] \"기관\"                        \n## [15] \"키워드\"                      \n## [16] \"특성추출(가중치순 상위 50개)\"\n## [17] \"본문\"                        \n## [18] \"URL\"                         \n## [19] \"분석제외 여부\"\n\nvac_df <- \nreadxl::read_excel(\"data/NewsResult_20210101-20210330.xlsx\") %>% \n  select(일자, 제목, 본문, 언론사, cat = `통합 분류1`, 키워드) \nvac_df %>% head(3)## # A tibble: 3 × 6\n##   일자     제목                    본문  언론사 cat   키워드\n##   <chr>    <chr>                   <chr> <chr>  <chr> <chr> \n## 1 20210330 美 국채 금리 상승에 뉴… \"정…  조선…  경제… 국채,…\n## 2 20210330 코로나 변이 바이러스 …  \"전…  경향…  IT_…  코로… \n## 3 20210330 “숲 통해 국민 심신 치…  \"ㆍ…  경향…  지역… 치유,…\n"},{"path":"anal4topic.html","id":"선택-표집","chapter":"9 .  주제모형(공변인)","heading":"9.2.2 (선택) 표집","text":"LDA 모형은 베이지언 모형이므로 사후확률의 근사치를 주어진 자료로부터 반복적으로 계산해 주제를 추론한다. 복잡한 계산을 반복적으로 수행하기 때문에 컴퓨터 사양이 낮은 경우 분석이 매우 느릴 수 있다. 데이터 크기와 컴퓨터 서능에 따라 수십분 혹은 수시간 이상 소요될 수 있다. 주제모형 분석 방법 학습 맥락에서 시간 절약을 위해 데이터셋의 일부만 추려 분석에 활용한다. 아래 코드는 1만4천개 행에서 3천개행 표집.기사 본문과 키워드를 비교해보자.분석 목적에 맞게 열을 재구성한다. 언론사는 ’여당지’와 ’야당지’로 구분한다. 분류는 ’사회면’와 ’비사회면’로 나눈다.새로 생성된 열의 기사 양을 계산해보자.문화와 스포츠를 검색단계서 선택하지 않았음에도 데이터셋에 포함된 이유는 하부 분류에 포함돼 있었기 때문이다.비사회면 8436건, 사회면 5653건으로 문서 수에서 큰 차이가 나지 않는다.월 및 언론사 구분에서도 문서 수에서 큰 차이가 나지 않는다.","code":"\nset.seed(37)\nvac_sample_df <-   \n  vac_df %>% \n  sample_n(size = 3000) \nvac_df %>% glimpse()\nvac_df %>% pull(키워드) %>% head(1)## [1] \"국채,금리,상승,출발,뉴욕,증시,소폭,하락,정체중,미국,국채,금리,상승세,미국,증시,소폭,약세,출발,마무리,트레이더들,미국,대통령,인프라,투자,계획,매도,국채,30일,현지시간,기준,미국,뉴욕,증시,지수,하락,거래,다우평균,0.24%,S&P,0.35%,나스닥,0.81%,거래,0.81%,채권시장,이날,뉴욕,채권,시장,개장,전자상,거래,만기,국채,금리,하루,0.06%,포인트,여파,코로나,바이러스,1월,수준,기록,미국,정부,사회,인프라,투자,패키지,법안,공개,예정,법안,사회,인프라,투자,패키지,3조,달러,사회,인프라,투자,전망,인플레이션,가능,부각,국채,금리,상승,압력,작용,가능성,국채,금리,상승,이외,매니저,한국,해지펀드,Bill,Hwang,운용,펀드,이케고스,투자,주식,하락,여파,이케고스,지난주,마진콜,계약,가격,변화,부족,증거금,추가,납부,요구,블록딜,일반,주식,거래,양측,거래,상대방,대량,거래,시장,영향,가격,합의,정규장,거래,비아콤,CBS,디스커버리,주식,주식,낙차,발생,전문가들,증시,예상,예측,야후,파이낸스,30일,시황,보도,고용,동향,금요일,전망,이달,63만,일자리,창출,예상,수치,시작,코로나,펜데믹,수치\"\n\nvac_df %>% pull(제목) %>% head(1)## [1] \"美 국채 금리 상승에 뉴욕 증시 소폭 하락 출발\"\n\nvac_df %>% pull(본문) %>% head(1)## [1] \"정체중이었던 미국 국채금리가 다시 상승세를 보이면서 미국 증시가 소폭 약세를 보이며 출발했다. 1분기 마무리를 앞둔 트레이더들이 조 바이든 미국 대통령의 대규모 인프라 투자 계획 발표 전에 미 국채 매도를 늘리고 있다는 분석이 나온다. \\n \\n30일 오전 9시 40분(현지시간) 기준 미국 뉴욕 증시 3대 지수는 하락한채 거래 중이다. 다우평균은 전날보다 ..\"\n\nvac2_df <- \nvac_df %>% \n  # 중복기사 제거\n  distinct(제목, .keep_all = T) %>% \n  # 기사별 ID부여\n  mutate(ID = factor(row_number())) %>% \n  # 월별로 구분한 열 추가(lubridate 패키지)\n  mutate(week = week(ymd(일자))) %>%       \n  # 기사 제목과 본문 결합\n  unite(제목, 본문, col = \"text\", sep = \" \") %>% \n  # 중복 공백 제거\n  mutate(text = str_squish(text)) %>% \n  # 언론사 구분: 야당지, 여당지 %>% \n  mutate(press = case_when(\n    언론사 == \"조선일보\" ~ \"야당지\",\n    언론사 == \"중앙일보\" ~ \"야당지\",\n    언론사 == \"경향신문\" ~ \"여당지\",\n    TRUE ~ \"여당지\") ) %>% \n  # 기사 분류 구분 \n  separate(cat, sep = \">\", into = c(\"cat\", \"cat2\")) %>% \n  # IT_과학, 경제, 사회 만 선택\n  select(-cat2) %>% \n  # 분류 구분: 사회, 비사회\n  mutate(catSoc = case_when(\n    cat == \"사회\" ~ \"사회면\",\n    cat == \"지역\" ~ \"사회면\",\n    TRUE ~ \"비사회면\") )\n\nvac2_df %>% glimpse()## Rows: 14,090\n## Columns: 9\n## $ 일자   <chr> \"20210330\", \"20210330\", \"20210330\", \"202103…\n## $ text   <chr> \"美 국채 금리 상승에 뉴욕 증시 소폭 하락 출…\n## $ 언론사 <chr> \"조선일보\", \"경향신문\", \"경향신문\", \"경향신…\n## $ cat    <chr> \"경제\", \"IT_과학\", \"지역\", \"IT_과학\", \"지역…\n## $ 키워드 <chr> \"국채,금리,상승,출발,뉴욕,증시,소폭,하락,정…\n## $ ID     <fct> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, …\n## $ week   <dbl> 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,…\n## $ press  <chr> \"야당지\", \"여당지\", \"여당지\", \"여당지\", \"여…\n## $ catSoc <chr> \"비사회면\", \"비사회면\", \"사회면\", \"비사회면…\n\nvac2_df %>% count(cat, sort = T)## # A tibble: 8 × 2\n##   cat         n\n##   <chr>   <int>\n## 1 사회     3733\n## 2 경제     2891\n## 3 국제     2164\n## 4 정치     2088\n## 5 지역     1921\n## 6 IT_과학   915\n## # … with 2 more rows\n\nvac2_df %>% count(catSoc, sort = T)## # A tibble: 2 × 2\n##   catSoc       n\n##   <chr>    <int>\n## 1 비사회면  8436\n## 2 사회면    5654\n\nvac2_df %>% count(week)## # A tibble: 13 × 2\n##    week     n\n##   <dbl> <int>\n## 1     1  1491\n## 2     2  1299\n## 3     3  1229\n## 4     4  1379\n## 5     5  1219\n## 6     6   927\n## # … with 7 more rows\n\nvac2_df %>% count(press, sort = T)## # A tibble: 2 × 2\n##   press      n\n##   <chr>  <int>\n## 1 야당지  8043\n## 2 여당지  6047\n"},{"path":"anal4topic.html","id":"정제-3","chapter":"9 .  주제모형(공변인)","heading":"9.2.3 정제","text":"","code":""},{"path":"anal4topic.html","id":"토큰화-2","chapter":"9 .  주제모형(공변인)","heading":"9.2.3.1 토큰화","text":"빅카인즈의 형태소분석이 된 키워드를 이용하므로 이미 토큰화가 된 상태이나, 본 분석에 앞서 단어의 빈도 등을 검토하기 위해 정돈텍스트 형식으로 변경하기 위해 토큰화를 진행한다. ‘text’열이 아니라 ’키워드’열의’,’를 기준으로 토큰화한다. 토큰화하기 전 문자, 숫자, 쉼표 이외의 요소를 제거한다. 전각문자는 문자가 아님에도 정규표현식으로 걸리지지 않으므로 추가로 제거한다.추가로 지워야 하는 주요 기호 :\nㆍㅣ‘’“” ○ ● ◎ ◇ ◆ □ ■ △ ▲ ▽ ▼ 〓 ◁ ◀ ▷ ▶ ♤ ♠ ♡ ♥ ♧ ♣ ⊙ ◈ ▣\nㆍㅣ‘’“” ○ ● ◎ ◇ ◆ □ ■ △ ▲ ▽ ▼ 〓 ◁ ◀ ▷ ▶ ♤ ♠ ♡ ♥ ♧ ♣ ⊙ ◈ ▣이외에도 빈도 분석을 통해 정규표현식으로 걸러지지 않은 기호가 나오면 추가로 제거한다.","code":"\n\"!@#$... 전각ㆍㅣ문자 %^&*()\" %>% str_remove(\"\\\\w+\")## [1] \"!@#$...  %^&*()\"\n\nfullchar_v <- \"ㆍ|ㅣ|‘|’|“|”|○|●|◎|◇|◆|□|■|△|▲|▽|▼|〓|◁|◀|▷|▶|♤|♠|♡|♥|♧|♣|⊙|◈|▣\"\n\nvac_tk <- \nvac2_df %>% \n  mutate(키워드 = str_remove_all(키워드, \"[^(\\\\w+|\\\\d+|,)]\")) %>% \n  mutate(키워드 = str_remove_all(키워드, fullchar_v)) %>% \n  unnest_tokens(word, 키워드, token = \"regex\", pattern = \",\") \n\nvac_tk %>% arrange(ID) %>% head(30)## # A tibble: 30 × 9\n##   일자     text  언론사 cat   ID     week press catSoc word \n##   <chr>    <chr> <chr>  <chr> <fct> <dbl> <chr> <chr>  <chr>\n## 1 20210330 美 …  조선…  경제  1        13 야당… 비사…  국채 \n## 2 20210330 美 …  조선…  경제  1        13 야당… 비사…  금리 \n## 3 20210330 美 …  조선…  경제  1        13 야당… 비사…  상승 \n## 4 20210330 美 …  조선…  경제  1        13 야당… 비사…  출발 \n## 5 20210330 美 …  조선…  경제  1        13 야당… 비사…  뉴욕 \n## 6 20210330 美 …  조선…  경제  1        13 야당… 비사…  증시 \n## # … with 24 more rows\n\nvac_tk %>% arrange(ID) %>% tail(30)## # A tibble: 30 × 9\n##   일자     text  언론사 cat   ID     week press catSoc word \n##   <chr>    <chr> <chr>  <chr> <fct> <dbl> <chr> <chr>  <chr>\n## 1 20210101 \"\\\"…  경향…  사회  14090     1 여당… 사회면 온라…\n## 2 20210101 \"\\\"…  경향…  사회  14090     1 여당… 사회면 자가 \n## 3 20210101 \"\\\"…  경향…  사회  14090     1 여당… 사회면 진단 \n## 4 20210101 \"\\\"…  경향…  사회  14090     1 여당… 사회면 제출 \n## 5 20210101 \"\\\"…  경향…  사회  14090     1 여당… 사회면 선생…\n## 6 20210101 \"\\\"…  경향…  사회  14090     1 여당… 사회면 부재…\n## # … with 24 more rows\n"},{"path":"anal4topic.html","id":"불용어-처리","chapter":"9 .  주제모형(공변인)","heading":"9.2.3.2 불용어 처리","text":"빅카인즈의 형태소분석 결과를 이용하므로 별도의 불용어처리는 불필요하나, 의미없는 고빈도 단어를 선별할 필요가 있다. 단어의 총빈도를 계산해본다. ’백신’과 ’코로나19’의 빈도가 높다. 어느 한 단어가 압도적인 비중을 차지하는 것이 아니므로 제거하지 않고 그대로 둔다. ’코로나’는 ’코로나19’라는 질병의 이름으로 사용됐을수 있고, ’코로나바이러스’라는 병인으로 사용됐을수도 있으므로, ’코로나19’와 병합하지 말고 그대로 둔다.","code":"\ncount_df <- \nvac_tk %>% count(word, sort = T)\n\ncount_df %>% head(40)## # A tibble: 40 × 2\n##   word         n\n##   <chr>    <int>\n## 1 백신     34189\n## 2 코로나19 32204\n## 3 접종     24660\n## 4 정부     14813\n## 5 미국     12322\n## 6 코로나   12120\n## # … with 34 more rows\n\ncount_df %>% tail(40)## # A tibble: 40 × 2\n##   word               n\n##   <chr>          <int>\n## 1 히팅               1\n## 2 힉스               1\n## 3 힌덴버그리서치     1\n## 4 힌두경전           1\n## 5 힌두교식           1\n## 6 힌두트바           1\n## # … with 34 more rows\n"},{"path":"anal4topic.html","id":"stm말뭉치","chapter":"9 .  주제모형(공변인)","heading":"9.2.4 stm말뭉치","text":"stm()함수에서 처리하는 데이터는 각 기사의 토큰이 하나의 열에 함께 있어야 한다. 정돈텍스트형식은 한개의 열에 하나의 토큰만 있으므로 str_flatten()함수로 하나의 열에 결합한다.textProcessor()함수는 영문처리를 기본값으로 하고 있다. 영문은 두 글자 단어가 거의 없기 때문에 기본값이 세글자 이상만 분석에 투입하도록 기본값이 설정돼 있다. 국문은 두 글자 단어도 의미있는 단어가 많기 때문에, 단어의 길이를 두 글자 이상으로 설정한다.prepDocuments()함수로 주제모형에 사용할 데이터의 인덱스(wordcounts)를 만든다. 이후 stm말뭉치와 기사 본문을 연결해 확인해야 하므로, 단어와 문서를 제거하지 않는다.산출결과를 개별 객체로 저장한다. 이 객체들은 이후 모형구축에 사용된다.","code":"\ncombined_df <-\n  vac_tk %>%\n  group_by(ID) %>%\n  summarise(text2 = str_flatten(word, \" \")) %>%\n  ungroup() %>% \n  inner_join(vac2_df, by = \"ID\")\n\ncombined_df %>% glimpse()## Rows: 14,090\n## Columns: 10\n## $ ID     <fct> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, …\n## $ text2  <chr> \"국채 금리 상승 출발 뉴욕 증시 소폭 하락 정…\n## $ 일자   <chr> \"20210330\", \"20210330\", \"20210330\", \"202103…\n## $ text   <chr> \"美 국채 금리 상승에 뉴욕 증시 소폭 하락 출…\n## $ 언론사 <chr> \"조선일보\", \"경향신문\", \"경향신문\", \"경향신…\n## $ cat    <chr> \"경제\", \"IT_과학\", \"지역\", \"IT_과학\", \"지역…\n## $ 키워드 <chr> \"국채,금리,상승,출발,뉴욕,증시,소폭,하락,정…\n## $ week   <dbl> 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,…\n## $ press  <chr> \"야당지\", \"여당지\", \"여당지\", \"여당지\", \"여…\n## $ catSoc <chr> \"비사회면\", \"비사회면\", \"사회면\", \"비사회면…\n\nprocessed <-\n  combined_df %>% textProcessor(\n    documents = combined_df$text2,\n    metadata = .,\n    wordLengths = c(2, Inf)\n  )## Building corpus... \n## Converting to Lower Case... \n## Removing punctuation... \n## Removing stopwords... \n## Removing numbers... \n## Stemming... \n## Creating Output...\n\nsummary(processed)## A text corpus with 14090 documents, and an 121938 word dictionary. Use str() to inspect object or see documentation\n\nout <-\n  prepDocuments(processed$documents,\n                processed$vocab,\n                processed$meta,\n                lower.thresh = 0)\nsummary(out)##                Length Class      Mode     \n## documents       14090 -none-     list     \n## vocab          121938 -none-     character\n## meta               10 data.frame list     \n## words.removed       0 -none-     character\n## docs.removed        0 -none-     NULL     \n## tokens.removed      1 -none-     numeric  \n## wordcounts     121938 -none-     numeric\n\ndocs <- out$documents\nvocab <- out$vocab\nmeta <- out$meta"},{"path":"anal4topic.html","id":"분석-2","chapter":"9 .  주제모형(공변인)","heading":"9.3 분석","text":"주제모형 구축에 앞서 먼저 도출한 주제의 수를 정한다.","code":""},{"path":"anal4topic.html","id":"주제topic의-수-설정","chapter":"9 .  주제모형(공변인)","heading":"9.3.1 주제(topic)의 수 설정","text":"보통 10개부터 100개까지 10개 단위로 주제의 수를 구분해 연구자들이 정성적으로 최종 주제의 수 판단한다.학습 상황이므로 계산시간을 줄이기 위해 주제의 수를 3개, 9개, 100개의 결과를 비교해보자.주제모형 분석은 사후 확률의 근사치를 주어진 자료로부터 반복적으로 최적화하는 계산을 수행하기 때문에 자료가 크면 계산시간이 오래 걸린다. 컴퓨터 성능에 따라 수십분 이상 소요될 수 있다.","code":"\ntopicN <- c(3, 9, 100)\n\nstorage <- searchK(docs, vocab, K = topicN)\nstorage\nplot(storage)"},{"path":"anal4topic.html","id":"모형-구성","chapter":"9 .  주제모형(공변인)","heading":"9.3.2 모형 구성","text":"stm패키지가 추출한 주제에 대하여 메타데이터를 변수로 투입하는 방식은 2가지다. ‘topical prevalence’와 ’topical content’를 이용하는 방식이다. ’topical prevalence’ 공변인은 prevalence =인자를 통해 투입하고, ’topical content’는 cotent =인자를 통해 투입한다. 모형에 따라 prevalence와 content 중 하나만 투입하기도 하고 둘다 투입하기도 한다.Topical prevalence: 공변인에 따른 문서별 주제 분포의 비율Topical prevalence: 공변인에 따른 문서별 주제 분포의 비율Topical content: 공변인에 따른 단어별 주제 분포Topical content: 공변인에 따른 단어별 주제 분포연속변수를 투입할 때는 s()함수를 이용해 구간의 수를 지정한다. s()함수는 공변인을 연속변수로 투입할때 spline으로 추정하도록 한다. 즉, 구간을 지정해 각 구간별로 따로 회귀식을 구하면서 각 구간을 연속적인 형태로 만들어준다. 구간의 수는 df =인자를 통해 투입한다. 기본값은 10이다.주제모형분석을 위해서는 주제어 선정과 분포계산을 반복적으로 수행한다. 이 과정을 화면에 출력되지 않게 verbose = 인자를 FALSE로 설정할 수 있다.주의: 메타데이터 인자(prevalence =~ 와 content =~)에 투입할 때 =가 아니라 =~ !!!아래 모형에서는 언론사의 정치성향과 시간을 공변인으로 투입했다.","code":"\nt1 <- Sys.time()\nmeta_fit <-\n  stm(\n    documents = docs,\n    vocab = vocab,\n    data = meta,\n    K = 9,         \n    prevalence =~ press + s(week, 6), # 투입하는 공변인\n    max.em.its = 75,                # 최대 반복계산 회수 \n    verbose = F,                    # 반복계산결과 화면출력 여부\n    init.type = \"Spectral\",\n    seed = 37 \n  )\nt2 <- Sys.time()\nt2-t1\n\nmeta_fit %>% \n  write_rds(\"data/meta_fit.rds\")\nmeta_fit <-  \n  read_rds(\"data/meta_fit.rds\")\n\nsummary(meta_fit)## A topic model with 9 topics, 14090 documents and a 121938 word dictionary.\n## Topic 1 Top Words:\n##       Highest Prob: 백신, 접종, 코로나, 아스트라제네카, 바이러스, 예방, 영국 \n##       FREX: 접종, 아스트라제네카, 임상, az, 면역, 화이자, 혈전 \n##       Lift: aah, abv, acip, acut, adapt, adcov, ade \n##       Score: 접종, 백신, 아스트라제네카, 접종자, az, 임상, 혈전 \n## Topic 2 Top Words:\n##       Highest Prob: 코로나, 지원, 지급, 정부, 지원금, 재난, 매출 \n##       FREX: 추경안, 가액, 예비비, 지원대상, 부가세, 본예산, 하이트진로 \n##       Lift: bgf, blt, carrier, cj오쇼핑, covideigokr, db손해보험, e영업제한 \n##       Score: 지원금, 매출, 지급, 소상공인, 소득, 대출, 자영업자 \n## Topic 3 Top Words:\n##       Highest Prob: 달러, 투자, 기업, 코로나, 시장, 미국, 경제 \n##       FREX: 주식, 반도체, 주가, 증시, 투자자, 코스피, 공매도 \n##       Lift: a기업, bev, carbon, cbi, ceo스코어, cpng, c쇼크 \n##       Score: 금리, 증시, 달러, 주식, 반도체, 코스피, 공매도 \n## Topic 4 Top Words:\n##       Highest Prob: 코로나, 병원, 경찰, 환자, a씨, 의료, 법무부 \n##       FREX: 교도소, 서울동부구치소, 의료법, 재판부, 재소자, 서울구치소, 정인이 \n##       Lift: 교도소, 서울구치소, 총회장, abus, admit, aids감염인연합회, aids인권활동가네트워크 \n##       Score: 수용자, 구치소, 경찰, 병원, 법무부, 동부구치소, 혐의 \n## Topic 5 Top Words:\n##       Highest Prob: 미국, 중국, 일본, 한국, 코로나, 북한, 정부 \n##       FREX: 올림픽, 도쿄, 미얀마, 스가, 쿼드, 군부, 도쿄올림픽 \n##       Lift: 관영, 도쿄, aaaj, aaip, aapi, aapp, aaron \n##       Score: 북한, 중국, 바이든, 외교, 트럼프, 이란, 미국 \n## Topic 6 Top Words:\n##       Highest Prob: 온라인, 교육, 코로나, 서비스, 지원, 사업, 학교 \n##       FREX: 클럽하우스, 학년도, 학급, 메타버스, vr, 특강, 명예의 \n##       Lift: 학급, aadhaar, aapex, abet, abf, abf제도, abl생명보험 \n##       Score: 브랜드, 수업, 학생, 등교, 학년, 고객, 학교 \n## Topic 7 Top Words:\n##       Highest Prob: 대통령, 국민, 의원, 후보, 대표, 정부, 민주당 \n##       FREX: 선거, 지지율, 출마, 탄핵, 사면, 보궐, 경선 \n##       Lift: sica, 고전역학, 공정사단, 기록관, 기초의원, 김해신공항, 난타전 \n##       Score: 대통령, 민주당, 후보, 선거, 대선, 출마, 의원 \n## Topic 8 Top Words:\n##       Highest Prob: 확진자, 코로나, 감염, 검사, 방역, 확진, 발생 \n##       FREX: 진단검사, 비수도권, 입국자, 열방, btj, 선교회, 다중이용시설 \n##       Lift: abrimosomorimo, accept, agejspnurl, antibodi, anvisa, a관세법, a관세법인 \n##       Score: 확진자, 확진, 판정, 검사, 감염, 방대본, 집단감염 \n## Topic 9 Top Words:\n##       Highest Prob: 코로나, 여성, 사람, 사회, 생각, 사람들, 서울 \n##       FREX: 출생아, 층간소음, 천사, 빙어, 칫솔, 장미, 빨대 \n##       Lift: a무리, bj파이, brt, cesd, cmip, connectus, contact \n##       Score: 플라스틱, 쓰레기, 결혼, 축제, 일자리, 가구, 여성\n"},{"path":"anal4topic.html","id":"주제-이름짓기","chapter":"9 .  주제모형(공변인)","heading":"9.4 주제 이름짓기","text":"","code":""},{"path":"anal4topic.html","id":"주제별-단어와-원문-결합","chapter":"9 .  주제모형(공변인)","heading":"9.4.1 주제별 단어와 원문 결합","text":"주제 단어가 추출된 원문을 살펴보면 해당 주제를 보다 명확하게 파악할 수 있다. 모형 구성에 투입한 데이터와 문서(이 경우 개별 기사) 본문이 포함된 데이터를 결합해야 한다. stm패키지는 findThoughts()함수를 통해 각 모형별로 전형적인 문서를 확인할 수 있도록 한다.구성한 stm모형과 구성전 데이터프레임을 결합하면 보다 다양한 방식으로 문서의 원문을 탐색할 수 있다.결합하는 두 데이터프레임의 기준이 되는 열에 포함된 자료의 유형을 통일시킨다. 여기서는 정수(integer)로 통일시켰다.각 주제는 독립된 열로 분리한다.각 주제별로 확률분포가 높은 문서를 확인해 보자. 각 문서에서 감마가 높은 순서로 정열하면, 해당 주제에 속할 확률이 높은 문서 순서대로 볼수 있다. pull()함수를 이용하면 해당 열의 모든 내용을 볼수 있다.각 주제별로 대표 단어를 선택해 원문을 살펴보자.","code":"\nfindThoughts(\n  model = meta_fit,     # 구성한 주제모형\n  texts = vac2_df$text,  # 문서 본문 문자 벡터\n  topics = c(1, 2),     # 찾고자 하는 주제의 값. 기본값은 모든 주제\n  n = 3                 # 찾고자 하는 문서의 수\n)## \n##  Topic 1: \n##       770만명 맞을 아스트라, 유럽선 중단 EU(유럽연합) 4대 회원국인 독일 프랑스 이탈리아 스페인이 15일(현지 시각) 아스트라제네카(AZ) 코로나 백신의 접종을 일시 중단하겠다고 선언했다. 접종 후 혈전(血栓 핏덩이)이 생기는 부작용이 발생했다는 보고가 잇따르자 추가 조사 결과가 나올 때까지 접종을 멈추겠다는 것이다. 이날 옌스 슈판 독일 보건부 장관은 “부작용이 백신 접종의 효과를 ..\n##      아스트라 백신 1차 검증서 \"예방 효과 62% 고령층 접종 가능\" 아스트라제네카 백신이 국내 접종을 앞두고 허가 심사 첫 관문을 통과했다. 식품의약품안전처 외부 자문단이 아스트라제네카 백신에 대해 조건부 허가할 수 있다는 결론을 내렸다. 해외에서 논란이 일고 있는 65세 이상 고령자 접종에 대해서도 자문단 다수가 “투여할 수 있다”고 의견을 냈다. 식약처 허가를 통과하면 아스트라제네카 백신은 이달 말께 공급돼 요양병원..\n##      65세 이상, 아스트라 백신 접종 연기 아스트라제네카 코로나 백신의 최우선 접종 대상에서 ‘65세 이상’ 고령층이 제외됐다. 코로나19 예방접종 대응 추진단(단장 정은경 질병관리청장)은 당초 2월 접종 예정이었던 요양병원 요양시설의 65세 이상 고령자 접종을 2분기(4~6월) 이후로 미룬다고 15일 밝혔다. 최근 유럽에서 ‘효과 검증이 부족하다’는 이유로 65세 이상에 대한 아스트라제네.. \n##  Topic 2: \n##       매출 근로자 수 기준 재검토 실질 피해 따져 ‘사각’ 없앤다 기재부, 선별 방식 연구용역 발주 업종 간 형평성도 보완 ‘직격탄’ 업종 더 주고, 특고 프리랜서 제한 기준 개선할 듯 4차 재난지원금 ‘선별지원’을 주장해온 정부는 ‘선별 속 선별’을 강조하고 있다. 지급 대상을 더 정교하게 구분해서 피해에 걸맞은 지원액을 최대한 많이 주자는 것이다. 앞선 지급과정에서 문제로 드러난 매출 근로자 수 기준, 업종 구분..\n##      분배 악화일로 근로 사업소득 사상 첫 3분기째 동반 감소 코로나19 3차 유행이 시작된 지난해 4분기 상 하위 계층 간 소득 격차가 벌어지며 두 분기 연속 분배 상황이 악화했다. 코로나19로 인한 경기 부진과 고용 충격이 저소득층에 집중되면서 소득 불균형을 키웠다. 코로나19 사태 장기화로 일해서 버는 돈인 근로소득과 사업소득이 사상 처음으로 세 분기 연속 동반 감소했다. 18일 통계청이 발표한 ‘2020년..\n##      소상공인 자영업자 코로나 피해지원금 최대 500만원 검토 정부가 4차 재난지원금에 포함할 소상공인 자영업자 피해지원금을 매출 감소에 따라 차등 지원하는 방안을 추진 중이다. 1인당 지원금은 최대 500만원까지 지급하는 방안을 검토 중이다. 21일 더불어민주당과 정부 설명을 종합하면, 기획재정부는 다음달 초 국회에 제출할 예정인 추가경정예산(추경)안에 이런 내용을 담은 소상공인 피해지원대책을 논의 중이다. ..\n\ntd_gamma <- meta_fit %>% tidy(matrix = \"gamma\")\ntd_gamma$document <- as.integer(td_gamma$document)\ncombined_df$ID <- as.integer(combined_df$ID) \ntext_gamma <- \ncombined_df %>% \n  select(ID, text2, text, 키워드) %>% \n  left_join(td_gamma, by = c(\"ID\" = \"document\")) %>% \n  pivot_wider(\n    names_from = topic,\n    values_from = gamma,\n    names_prefix = \"tGamma\",\n    values_fill = 0\n    ) \n\ntext_gamma %>% glimpse()  ## Rows: 14,090\n## Columns: 13\n## $ ID      <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,…\n## $ text2   <chr> \"국채 금리 상승 출발 뉴욕 증시 소폭 하락 …\n## $ text    <chr> \"美 국채 금리 상승에 뉴욕 증시 소폭 하락 …\n## $ 키워드  <chr> \"국채,금리,상승,출발,뉴욕,증시,소폭,하락,…\n## $ tGamma1 <dbl> 0.006021, 0.944370, 0.058890, 0.362034, 0.…\n## $ tGamma2 <dbl> 0.011376, 0.001862, 0.009169, 0.063926, 0.…\n## $ tGamma3 <dbl> 0.936021, 0.003908, 0.018304, 0.007786, 0.…\n## $ tGamma4 <dbl> 0.000581, 0.004401, 0.003925, 0.004451, 0.…\n## $ tGamma5 <dbl> 0.007778, 0.009563, 0.012495, 0.001949, 0.…\n## $ tGamma6 <dbl> 0.003105, 0.002289, 0.369640, 0.516974, 0.…\n## $ tGamma7 <dbl> 0.033440, 0.011748, 0.210739, 0.025828, 0.…\n## $ tGamma8 <dbl> 0.000197, 0.007103, 0.003683, 0.012100, 0.…\n## $ tGamma9 <dbl> 0.001481, 0.014757, 0.313156, 0.004952, 0.…\n\ntext_gamma %>% \n  arrange(-tGamma7) %>% \n  pull(text) %>% head(9)## [1] \"이낙연의 사면론 묘수일까 자충수일까 당대표 임기 두달 앞두고 전격 제기 하필이면 왜, 지금일까 이적수. 바둑용어다. 바둑에서 이적수는 둘이다. 이적수(利敵手)와 이적수(耳赤手). 한글발음은 같지만, 뜻은 정반대다. 이적수(利敵手)는 상대방에게 유리한 결과를 두는 수다. 자충수가 대표적이다. 이적수(耳赤手)는 상대방의 귀가 빨갛게 변하는 수다. 형세가 불리할 때 역전의 발판이 되..\"                                         \n## [2] \"2021 서울시장 선거, 단일화 전쟁이 시작됐다 오는 4월7일 서울시장과 부산시장을 다시 뽑는다. 내년 3월 대선을 앞둔 전초전이어서 여야 모두 물러설 수 없는 승부다. 대한민국 ‘민심의 풍향계’인 수도 서울에선 더더욱 그렇다. 여당은 수성을, 야당은 수복을 노리고 있다. 선거 한 달을 앞두고 주요 정당의 서울시장 후보가 모습을 드러내고 있지만 아직 대진표는 확정되지 않았다. 후보 단일화 과정이 남..\"                        \n## [3] \"금태섭 \\\"1대1 경선하자\\\" 안철수 \\\"국민의힘 논의 먼저\\\" 금태섭 전 더불어민주당 의원이 31일 서울시장 보궐선거 출마선언을 하면서 안철수 국민의당 대표를 향해 “제3지대 경선을 하자”고 제안했다. 이날 조정훈 시대전환 대표도 보선 출사표를 던지면서 범야권 단일화 시계가 본격적으로 움직이기 시작했다. 이날 오전 서울 마포구의 한 공연장에서 출마선언을 한 금 전 의원은 “서울 시민의 삶을 바꾸고 변화의 ..\"              \n## [4] \"사면론 이틀만에 막히자...이낙연 \\\"오랜 충정에서 말한 것뿐\\\" 이낙연 더불어민주당 대표가 꺼낸 이명박 박근혜 전 대통령에 대한 사면(赦免) 구상이 당원과 지지층의 반발에 부딪히면서 제동이 걸렸다. 이 대표가 새해 첫날 언론 인터뷰에서 ‘사면론’을 꺼낸 지 이틀 만이다. 이 대표는 3일 오후 국회 의원회관 사무실에서 긴급 최고위원 간담회를 소집해 의견 수렴에 나섰다. 당 최고위원회는 회의 직후 입장문(최인호 ..\"          \n## [5] \"서울시장 선거 여권 재역전 가능할까 오세훈 강세 지속 선거전 1주일 유권자 선택 중요 “오해하면 안 되는 것이 LH 사태라는 돌발변수 때문에 뒤집힌 것이 아니라는 점이다.” 민주당 측 당 전략전문가의 말이다. “교육부 서기관 나향욱의 개돼지 발언을 보라. 그게 박근혜가 시켜 한 발언인가. 박근혜나 당시 새누리당과 아무 관련 없이 터져나온 것이다. 그렇지 않아도 불만이 쌓여..\"                                              \n## [6] \"文 교감 속 작품? 반전 노린 3위 이낙연의 '사면' 승부수 이낙연 더불어민주당 대표가 “이명박 박근혜 전 대통령 사면 건의” 뜻을 1일 언론 인터뷰에서 밝히면서 정국을 흔들었다. 평소 신중한 성격의 이 대표가 대통령 고유 권한인 사면 문제를 먼저 꺼낸 걸 두고 “사면에 대한 대통령의 정치적 부담을 줄이고, 한 편으론 대선 후보 지지율 3위로 주저앉은 자신을 위한 반전 계기를 만들려 했다”는 분석이 나온다. 이날..\"          \n## [7] \"\\\"대선 포기\\\" 안철수가 치고나간 서울시장 선거 여야 딜레마 4 7 재보선의 하이라이트로 꼽히는 서울시장 보궐선거전은 지난달 20일 출렁댔다. 주요 예상 후보들이 출마 선언을 미루는 가운데 안철수 국민의당 대표가 “대권 포기, 야권 단일화”를 외치며 치고 나갔다. 여야는 과거 중도층 지지를 받았던 안 대표의 등장에 촉각을 곤두세웠다. ━ 경선에 주목하는 與 민주당에선 우상호 의원이 처음 출마를..\"                         \n## [8] \"금 “새정치만 10년째, 성과 뭔가?” 안 “정치개혁 초심 여전히 굳다” “10년 전 ‘새정치’라는 기치를 들고 나오셨다. 그런데 10년 동안 어떤 성과가 있었나?”(금태섭 후보) “금 후보나 저나 정치를 같은 시기에 시작했다. 정치를 개혁하겠단 초심, 의지는 여전히 굳고 똑같다.”(안철수 후보) 4월 서울시장 재보궐 선거에 출마한 안철수 국민의당 예비후보와 무소속 금태섭 예비후보가 18일 ‘문재인 정부의 4년 평가와..\"        \n## [9] \"범야권 ‘단일화 시계’ 빨라지나 금태섭, 안철수에 “1:1 경선하자” 금태섭 전 더불어민주당 의원이 31일 서울시장 보궐선거 출마선언을 하면서 안철수 국민의당 대표를 향해 “제3지대 경선을 하자”고 제안했다. 금 전 의원은 이날 오전 서울 마포구 공연장에서 출마선언을 하면서 “서울시민의 삶을 바꾸고 변화의 새판을 열어야 하는 선거지만 정치권은 오래된 싸움만 하고 있다”며 “엄중한 시기를 오래되고 낡은 정치에 맡길 ..\"\n\ntext_gamma %>% \n  arrange(-tGamma7) %>% \n  pull(키워드) %>% .[6]## [1] \"교감,작품,반전,이낙연,사면,승부수,대표,이낙연,더불어민주당,이명박,박근혜,대통령,건의,인터뷰,언론,정국,평소,신중,성격,대표,권한,대통령,고유,사면,사면,대통령,정치,부담,대선,후보,지지율,자신,반전,계기,이날,민주당,사면,주장,모습,추락,승부수,관계자,민주당,핵심,중앙일보,메시지,대표,모습,통합,원래,이낙연,본모습,국가,좌고우면,설명,취임,문파,文派,강성,지지,비판,대표,목소리,신호,의미,이날,여론조사,각종,신년,여론,조사,대표,이재명,윤석열,지지,대선,주자,3위,대표,혼자,주장,여권,대통령,대표,사이,임기,마지막,화두,국민,통합,의기투합,4,모종,그림,발언,기획,대표,26일,차례,대통령,차례,독대,대표,당내,의견,청와대,국민,소통,수석,윤영찬,의원,참여,의원,중앙일보,통화,조언,대표,의견,교환,생각,임기,총대,멨나,대표,임기,마지막,대통령,구속,전직,대통령,정치,부담,총대,가능성,거론,친문,분류,의원,대통령,고민,대표,이슈,당직,의원,대표,신중,성격,대통령,영역,사람,혼자,독단,대통령,권한,이야기,임기,대표,지지,반발,각오,결단,추론,당내,부정,여론,김종민,최고,위원,통화,결정,국민,수용,대통령,사면권,국민,위임,권한,여야,국정농단,정치,상황,고민,극복,개선,방안,모색,논의,우상호,의원,페이스북,사람,반성,사과,박근혜,심판,사법,반대,공개적,가시화,내부,반발,가시,대표,명분,신년사,언급,사회,갈등,완화,국민,통합,비판,분출,의원,친문,재선,통합,이명박,박근혜,대통령,사면,국민,논의,당대표,대통령,압박,비판,상의,초선,최고,위원,본인,결단,전략통,의원,사면,수도,재선,당내,반발,확산,양상,게시판,민주당,권리당원,대통령,도전,생각,사퇴,촛불민심,뒤통수,비난,도배,대표,건의,해피엔딩,대통령,수용,여부,대통령,통합,이미지,대선후보,유력,대선,후보,청와대,반대,좌초,대표,오점,대표,모두발언,신년인,사회,발언,김대중,외환위기,노무현,안보,위기,문재인,코로나19,현직,대통령,위기,극복,거론,전진,통합,동시,통합,강조,주변,대표,윤석열,탄핵,배제,친문세력들,문자,폭탄,와중,반응,삼척동자,친문,초선,대표,비난,승부수,의미\"\n\ntext_gamma %>% \n  arrange(-tGamma2) %>% \n  filter(str_detect(text, \"지원금\")) %>% \n  mutate(text = str_replace_all(text, \"지원금\", \"**지원금**\")) %>% \n  pull(text) %>% \n  head(5)## [1] \"매출 근로자 수 기준 재검토 실질 피해 따져 ‘사각’ 없앤다 기재부, 선별 방식 연구용역 발주 업종 간 형평성도 보완 ‘직격탄’ 업종 더 주고, 특고 프리랜서 제한 기준 개선할 듯 4차 재난**지원금** ‘선별지원’을 주장해온 정부는 ‘선별 속 선별’을 강조하고 있다. 지급 대상을 더 정교하게 구분해서 피해에 걸맞은 지원액을 최대한 많이 주자는 것이다. 앞선 지급과정에서 문제로 드러난 매출 근로자 수 기준, 업종 구분..\"       \n## [2] \"소상공인 자영업자 코로나 피해**지원금** 최대 500만원 검토 정부가 4차 재난**지원금**에 포함할 소상공인 자영업자 피해**지원금**을 매출 감소에 따라 차등 지원하는 방안을 추진 중이다. 1인당 **지원금**은 최대 500만원까지 지급하는 방안을 검토 중이다. 21일 더불어민주당과 정부 설명을 종합하면, 기획재정부는 다음달 초 국회에 제출할 예정인 추가경정예산(추경)안에 이런 내용을 담은 소상공인 피해지원대책을 논의 중이다. ..\"\n## [3] \"피해지원? 경기부양? “재난**지원금** 목표따라 규모 정해야” 지난해 전국민에게 지급한 14조3천억원 규모의 긴급재난**지원금**이 일으킨 소비 효과를 두고 학계가 뜨겁게 논쟁하고 있다. 소비 진작 효과가 투입 금액의 24%였다는 연구에서 최대 78%에 이른다는 분석까지 나온다. 연구 결과들은 공통으로 전체적인 소비 진작 효과뿐만 아니라 지원방식, 지급대상 등을 더욱 정교하게 설계해 **지원금** 지급 효율성을 높여야 한다고 강..\"\n## [4] \"코로나 고용 한파에 소득 불평등 커졌다 ㆍ임시 일용직 많은 저소득층 타격 ㆍ하위 20% 근로소득 급감 적자살림 ㆍ4분기 ‘5분위 배율’ 4.72로 더 악화 ㆍ정부 지원 효과로 양극화 폭 줄여 코로나19에 따른 경제충격으로 지난해 4분기에 소득양극화가 심화된 것으로 나타났다. 정부가 2차 긴급재난**지원금**으로 취약계층을 집중 지원했지만 물리적(사회적) 거리 두기에 따른 일자리 쇼크 해소..\"                                        \n## [5] \"29일 소상공인, 30일 특고 프리랜서 4차 재난**지원금** 지급 29일부터 소상공인 특수형태근로종사자 등 코로나19 피해계층을 대상으로 한 4차 재난**지원금** 지급이 시작된다. 28일 기획재정부 등에 따르면 정부는 6조7천억원 규모의 소상공인 버팀목자금 플러스와 1조원 규모의 고용 취약계층 피해**지원금**을 29일부터 순차 지급한다. 소상공인 버팀목자금 플러스는 집합금지 제한업종 및 국세청 자료에서 매출 감소가 확인되는..\"\n"},{"path":"anal4topic.html","id":"주제-이름-목록","chapter":"9 .  주제모형(공변인)","heading":"9.4.2 주제 이름 목록","text":"각 주제별로 주요 주제어와 해당 문서의 본문을 비교해 주제별로 주요 문서를 살펴보고 주제에 대한 이름을 짓는다. 각 주제별 주요 단어는 labelTopics()함수를 통해 주요단어를 찾을 수 있다. 주제별 이름은 목록을 만들어 데이터프레임에 저장한다.주제별 이름 목록을 데이터프레임에 저장한다.주제별 상위 7개 단어목록을 데이터프레임에 저장한 다음, 이름 목록과 결합한다.","code":"\nlabelTopics(meta_fit)## Topic 1 Top Words:\n##       Highest Prob: 백신, 접종, 코로나, 아스트라제네카, 바이러스, 예방, 영국 \n##       FREX: 접종, 아스트라제네카, 임상, az, 면역, 화이자, 혈전 \n##       Lift: aah, abv, acip, acut, adapt, adcov, ade \n##       Score: 접종, 백신, 아스트라제네카, 접종자, az, 임상, 혈전 \n## Topic 2 Top Words:\n##       Highest Prob: 코로나, 지원, 지급, 정부, 지원금, 재난, 매출 \n##       FREX: 추경안, 가액, 예비비, 지원대상, 부가세, 본예산, 하이트진로 \n##       Lift: bgf, blt, carrier, cj오쇼핑, covideigokr, db손해보험, e영업제한 \n##       Score: 지원금, 매출, 지급, 소상공인, 소득, 대출, 자영업자 \n## Topic 3 Top Words:\n##       Highest Prob: 달러, 투자, 기업, 코로나, 시장, 미국, 경제 \n##       FREX: 주식, 반도체, 주가, 증시, 투자자, 코스피, 공매도 \n##       Lift: a기업, bev, carbon, cbi, ceo스코어, cpng, c쇼크 \n##       Score: 금리, 증시, 달러, 주식, 반도체, 코스피, 공매도 \n## Topic 4 Top Words:\n##       Highest Prob: 코로나, 병원, 경찰, 환자, a씨, 의료, 법무부 \n##       FREX: 교도소, 서울동부구치소, 의료법, 재판부, 재소자, 서울구치소, 정인이 \n##       Lift: 교도소, 서울구치소, 총회장, abus, admit, aids감염인연합회, aids인권활동가네트워크 \n##       Score: 수용자, 구치소, 경찰, 병원, 법무부, 동부구치소, 혐의 \n## Topic 5 Top Words:\n##       Highest Prob: 미국, 중국, 일본, 한국, 코로나, 북한, 정부 \n##       FREX: 올림픽, 도쿄, 미얀마, 스가, 쿼드, 군부, 도쿄올림픽 \n##       Lift: 관영, 도쿄, aaaj, aaip, aapi, aapp, aaron \n##       Score: 북한, 중국, 바이든, 외교, 트럼프, 이란, 미국 \n## Topic 6 Top Words:\n##       Highest Prob: 온라인, 교육, 코로나, 서비스, 지원, 사업, 학교 \n##       FREX: 클럽하우스, 학년도, 학급, 메타버스, vr, 특강, 명예의 \n##       Lift: 학급, aadhaar, aapex, abet, abf, abf제도, abl생명보험 \n##       Score: 브랜드, 수업, 학생, 등교, 학년, 고객, 학교 \n## Topic 7 Top Words:\n##       Highest Prob: 대통령, 국민, 의원, 후보, 대표, 정부, 민주당 \n##       FREX: 선거, 지지율, 출마, 탄핵, 사면, 보궐, 경선 \n##       Lift: sica, 고전역학, 공정사단, 기록관, 기초의원, 김해신공항, 난타전 \n##       Score: 대통령, 민주당, 후보, 선거, 대선, 출마, 의원 \n## Topic 8 Top Words:\n##       Highest Prob: 확진자, 코로나, 감염, 검사, 방역, 확진, 발생 \n##       FREX: 진단검사, 비수도권, 입국자, 열방, btj, 선교회, 다중이용시설 \n##       Lift: abrimosomorimo, accept, agejspnurl, antibodi, anvisa, a관세법, a관세법인 \n##       Score: 확진자, 확진, 판정, 검사, 감염, 방대본, 집단감염 \n## Topic 9 Top Words:\n##       Highest Prob: 코로나, 여성, 사람, 사회, 생각, 사람들, 서울 \n##       FREX: 출생아, 층간소음, 천사, 빙어, 칫솔, 장미, 빨대 \n##       Lift: a무리, bj파이, brt, cesd, cmip, connectus, contact \n##       Score: 플라스틱, 쓰레기, 결혼, 축제, 일자리, 가구, 여성\n\ntopic_name <- tibble(topic = 1:9,\n                     name = c(\"1. 백신 접종자\",\n                              \"2. 코로나 지원금\",\n                              \"3. 경제 영향\",\n                              \"4. 집단 수용자\",\n                              \"5. 국제 관계\",\n                              \"6. 교육 온라인\",\n                              \"7. 정치권 동향\",\n                              \"8. 확진자 검진\",\n                              \"9. 사회 영향\") )\ntd_beta <- meta_fit %>% tidy(matrix = 'beta') \n\nterm_topic_name <- \ntd_beta %>% \n  group_by(topic) %>% \n  slice_max(beta, n = 7) %>% \n  left_join(topic_name, by = \"topic\")\n\nterm_topic_name## # A tibble: 63 × 4\n## # Groups:   topic [9]\n##   topic term              beta name          \n##   <int> <chr>            <dbl> <chr>         \n## 1     1 백신           0.0842  1. 백신 접종자\n## 2     1 접종           0.0612  1. 백신 접종자\n## 3     1 코로나         0.0249  1. 백신 접종자\n## 4     1 아스트라제네카 0.0115  1. 백신 접종자\n## 5     1 바이러스       0.00924 1. 백신 접종자\n## 6     1 예방           0.00745 1. 백신 접종자\n## # … with 57 more rows\n"},{"path":"anal4topic.html","id":"주제별-단어-분포도","chapter":"9 .  주제모형(공변인)","heading":"9.4.3 주제별 단어 분포도","text":"단어별로 부여된 베타 값을 이용해 주제별 단어 분포도를 각 주제의 이름과 함께 시각화한다.","code":"\nterm_topic_name %>% \n  \n  ggplot(aes(x = beta, \n             y = reorder_within(term, beta, name),  # 각 주제별로 재정렬\n             fill = name)) +\n  geom_col(show.legend = F) +\n  facet_wrap(~name, scales = \"free\") +\n  scale_y_reordered() +                             # 재정렬한 y축의 값 설정\n  labs(x = expression(\"단어 확률분포: \"~beta), y = NULL,\n       title = \"주제별 단어 확률 분포\",\n       subtitle = \"주제별로 다른 단어들로 군집\") +\n  theme(plot.title = element_text(size = 20))"},{"path":"anal4topic.html","id":"주제별-문서-분포도","chapter":"9 .  주제모형(공변인)","heading":"9.4.4 주제별 문서 분포도","text":"문서별로 부여된 감마 값을 이용한 주제별로 문서의 분포도를 각 주제의 이름과 함께 시각화한다.","code":"\ntd_gamma <- meta_fit %>% tidy(matrix = 'gamma') \n\ndoc_topic_name <- \ntd_gamma %>% \n  group_by(topic) %>% \n  left_join(topic_name, by = \"topic\")\n\ndoc_topic_name## # A tibble: 126,810 × 4\n## # Groups:   topic [9]\n##   document topic    gamma name          \n##      <int> <int>    <dbl> <chr>         \n## 1        1     1 0.00602  1. 백신 접종자\n## 2        2     1 0.944    1. 백신 접종자\n## 3        3     1 0.0589   1. 백신 접종자\n## 4        4     1 0.362    1. 백신 접종자\n## 5        5     1 0.000923 1. 백신 접종자\n## 6        6     1 0.00141  1. 백신 접종자\n## # … with 126,804 more rows\n\ndoc_topic_name %>% \n  ggplot(aes(x = gamma, fill = name)) +\n  geom_histogram(bins = 50, show.legend = F) +\n  facet_wrap(~name) + \n  labs(title = \"주제별 문서 확률 분포\",\n       y = \"문서(기사)의 수\", x = expression(\"문서 확률분포\"~(gamma))) +\n  theme(plot.title = element_text(size = 20))"},{"path":"anal4topic.html","id":"주제별-단어-문서-분포-1","chapter":"9 .  주제모형(공변인)","heading":"9.4.5 주제별 단어-문서 분포","text":"결합한 데이터프레임을 막대도표로 시각화.","code":"\n# 주제별 상위 7개 단어 추출\ntop_terms <- \ntd_beta %>% \n  group_by(topic) %>% \n  slice_max(beta, n = 7) %>% \n  select(topic, term) %>% \n  summarise(terms = str_flatten(term, collapse = \", \")) \n\n# 주제별 감마 평균 계산  \ngamma_terms <- \ntd_gamma %>% \n  group_by(topic) %>% \n  summarise(gamma = mean(gamma)) %>% \n  left_join(top_terms, by = 'topic') %>%  # 주제별 단어 데이터프레임과 결합\n  left_join(topic_name, by = 'topic')     # 주제 이름 데이터프레임과 결합\n\ngamma_terms## # A tibble: 9 × 4\n##   topic  gamma terms                                   name \n##   <int>  <dbl> <chr>                                   <chr>\n## 1     1 0.152  백신, 접종, 코로나, 아스트라제네카, 바… 1. … \n## 2     2 0.106  코로나, 지원, 지급, 정부, 지원금, 재난… 2. … \n## 3     3 0.100  달러, 투자, 기업, 코로나, 시장, 미국, … 3. … \n## 4     4 0.0838 코로나, 병원, 경찰, 환자, a씨, 의료, …  4. … \n## 5     5 0.0996 미국, 중국, 일본, 한국, 코로나, 북한, … 5. … \n## 6     6 0.111  온라인, 교육, 코로나, 서비스, 지원, 사… 6. … \n## # … with 3 more rows\n\ngamma_terms %>% \n  \n  ggplot(aes(x = gamma, y = reorder(name, gamma), fill = name)) +\n  geom_col(show.legend = F) +\n  geom_text(aes(label = round(gamma, 2)), # 소수점 2자리 \n            hjust = 1.15) +                # 라벨을 막대도표 안쪽으로 이동\n  geom_text(aes(label = terms), \n            hjust = -0.05) +              # 단어를 막대도표 바깥으로 이동\n  scale_x_continuous(expand = c(0, 0),    # x축 막대 위치를 Y축쪽으로 조정\n                     limit = c(0, .8)) +   # x축 범위 설정\n  labs(x = expression(\"문서 확률분포\"~(gamma)), y = NULL,\n       title = \"코로나19와 백신 관련보도 상위 주제어\",\n       subtitle = \"주제별로 기여도가 높은 단어 중심\") +\n  theme(plot.title = element_text(size = 20))"},{"path":"anal4topic.html","id":"공변인-분석","chapter":"9 .  주제모형(공변인)","heading":"9.5 공변인 분석","text":"stm패키지는 메타데이터와 주제 사이의 관계 탐색을 위해 estimateEffect()함수를 제공한다. 각 주제를 종속변수(산출요소)로 설정하고, 메타데이터를 독립변수(투입요소)로 설정해 회귀분석으로 수행한다. 분석결과는 회귀계수(estimate), 표준오차, t값으로 요약해 제시한다.여기서 사용한 자료에서 메타데이터로 투입한 독립변수는 press(언론사의 정치성향)와 시간(week)이다. 즉, 언론사 정치성향과 시간은 독립변수로서 종속변수인 추촐한 주제를 예측하는 변인이 된다.회귀계수는 독립변수가 종속변수를 설명하는 정도다. 예를 들어, press(언론사 정치성향)가 각 주제를 예측하는 정도가 회귀계수이므로, 이 회귀계수가 야당지에 비해 여당지에서 해당 주제에 대해 평균적으로 더 많이 언급한 정도를 나타낸다. 회귀계수가 음수면 야당지에서 언급이 더 많은 주제이고, 양수면 여당지에서 언급이 더 많은 주제다. t값과 p값은 그 효과(회귀계수)가 0과 유의하게 다른지를 나타낸다.분석 결과는 plot.estimateEffect()함수로 시각화한다. 주제 분포의 불확실성은 uncertainty =인자를 통해 다양한 방법으로 계산하는데, 기본값은 모든 경우를 고려하는 ’Global’이다. 만일 계산속도를 높이고 싶으면 추가적인 불확실성 계산을 생략하는 ’None’으로 투입한다.언론사의 정치성향(press)과 보도시점을 주 단위를 6개로 구분해 독립변수로 투입하고, 추출한 9개 주제를 종속변수로 투입해 분석해보자.주제4와 주제7을 제외하고 모두 언론사의 정치성향에 따라 기사의 주제가 다른 경향을 보이고 있다. 주제3과 주제5를 제외하고 모두 3월에 기사의 주제가 바뀌었다.\n언론사의 정치성향을 ’여당지’와 ’야당지’로 구분했는데, 분석결과에 ’여당지’만 나오는 이유는 야당지를 기준으로 여당지의 분포를 계산했기 때문이다. 즉, 계수가 음수면 야당지를 기준으로 여당지에 해당 주제를 구성하는 단어의 빈도가 상대적으로 적게 나타났다는 의미이다. 반대로 계수가 양수면 여당지에 해당 주제를 구성하는 단어의 빈도가 더 크다는 의미다.","code":"\nout$meta$rating <- as.factor(out$meta$press)\nprep <- estimateEffect(formula = 1:9 ~ press + s(week, 6), \n                       stmobj = meta_fit,\n                       metadata = out$meta,\n                       uncertainty = \"Global\")\n\nsummary(prep, topics= 1:9)## \n## Call:\n## estimateEffect(formula = 1:9 ~ press + s(week, 6), stmobj = meta_fit, \n##     metadata = out$meta, uncertainty = \"Global\")\n## \n## \n## Topic 1:\n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  0.13292    0.00836   15.90  < 2e-16 ***\n## press여당지 -0.06927    0.00518  -13.36  < 2e-16 ***\n## s(week, 6)1 -0.00787    0.01797   -0.44   0.6615    \n## s(week, 6)2  0.01004    0.01744    0.58   0.5648    \n## s(week, 6)3  0.02105    0.01823    1.15   0.2483    \n## s(week, 6)4  0.13472    0.01943    6.93  4.2e-12 ***\n## s(week, 6)5  0.08363    0.02002    4.18  3.0e-05 ***\n## s(week, 6)6  0.04645    0.01492    3.11   0.0019 ** \n## ---\n## Signif. codes:  \n## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## \n## Topic 2:\n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  0.075411   0.006971   10.82   <2e-16 ***\n## press여당지  0.033215   0.004549    7.30    3e-13 ***\n## s(week, 6)1  0.026761   0.015302    1.75   0.0803 .  \n## s(week, 6)2  0.041134   0.014805    2.78   0.0055 ** \n## s(week, 6)3  0.027896   0.015051    1.85   0.0638 .  \n## s(week, 6)4  0.000922   0.016436    0.06   0.9553    \n## s(week, 6)5 -0.008214   0.017042   -0.48   0.6298    \n## s(week, 6)6  0.016163   0.011890    1.36   0.1741    \n## ---\n## Signif. codes:  \n## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## \n## Topic 3:\n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  0.12245    0.00709   17.27   <2e-16 ***\n## press여당지 -0.01048    0.00471   -2.22    0.026 *  \n## s(week, 6)1 -0.01928    0.01529   -1.26    0.207    \n## s(week, 6)2 -0.01191    0.01551   -0.77    0.442    \n## s(week, 6)3 -0.03370    0.01586   -2.13    0.034 *  \n## s(week, 6)4 -0.02218    0.01644   -1.35    0.177    \n## s(week, 6)5  0.00103    0.01695    0.06    0.951    \n## s(week, 6)6 -0.00978    0.01217   -0.80    0.422    \n## ---\n## Signif. codes:  \n## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## \n## Topic 4:\n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  0.11652    0.00659   17.67  < 2e-16 ***\n## press여당지  0.00411    0.00388    1.06    0.289    \n## s(week, 6)1  0.02654    0.01381    1.92    0.055 .  \n## s(week, 6)2 -0.07887    0.01353   -5.83  5.7e-09 ***\n## s(week, 6)3 -0.01425    0.01263   -1.13    0.259    \n## s(week, 6)4 -0.03679    0.01461   -2.52    0.012 *  \n## s(week, 6)5 -0.05692    0.01367   -4.16  3.2e-05 ***\n## s(week, 6)6 -0.04658    0.01139   -4.09  4.3e-05 ***\n## ---\n## Signif. codes:  \n## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## \n## Topic 5:\n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  0.09544    0.00674   14.16  < 2e-16 ***\n## press여당지 -0.01034    0.00449   -2.30    0.021 *  \n## s(week, 6)1 -0.00683    0.01457   -0.47    0.639    \n## s(week, 6)2  0.03850    0.01517    2.54    0.011 *  \n## s(week, 6)3 -0.00535    0.01422   -0.38    0.707    \n## s(week, 6)4 -0.03021    0.01592   -1.90    0.058 .  \n## s(week, 6)5  0.07686    0.01555    4.94  7.8e-07 ***\n## s(week, 6)6 -0.01430    0.01203   -1.19    0.235    \n## ---\n## Signif. codes:  \n## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## \n## Topic 6:\n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  0.087814   0.007130   12.32  < 2e-16 ***\n## press여당지 -0.011102   0.004793   -2.32   0.0206 *  \n## s(week, 6)1  0.000218   0.015118    0.01   0.9885    \n## s(week, 6)2  0.042055   0.015102    2.78   0.0054 ** \n## s(week, 6)3  0.005794   0.015508    0.37   0.7087    \n## s(week, 6)4  0.051091   0.016810    3.04   0.0024 ** \n## s(week, 6)5  0.020497   0.018240    1.12   0.2611    \n## s(week, 6)6  0.069777   0.012326    5.66  1.5e-08 ***\n## ---\n## Signif. codes:  \n## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## \n## Topic 7:\n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  0.12688    0.00791   16.04  < 2e-16 ***\n## press여당지  0.00781    0.00439    1.78    0.075 .  \n## s(week, 6)1 -0.01200    0.01661   -0.72    0.470    \n## s(week, 6)2  0.02591    0.01587    1.63    0.102    \n## s(week, 6)3 -0.06810    0.01597   -4.27  2.0e-05 ***\n## s(week, 6)4  0.01741    0.01689    1.03    0.303    \n## s(week, 6)5 -0.10097    0.01746   -5.78  7.4e-09 ***\n## s(week, 6)6 -0.02197    0.01282   -1.71    0.087 .  \n## ---\n## Signif. codes:  \n## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## \n## Topic 8:\n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  0.17429    0.00849   20.53  < 2e-16 ***\n## press여당지  0.02968    0.00521    5.69  1.3e-08 ***\n## s(week, 6)1 -0.01232    0.01928   -0.64  0.52275    \n## s(week, 6)2 -0.05192    0.01936   -2.68  0.00734 ** \n## s(week, 6)3  0.03165    0.01875    1.69  0.09145 .  \n## s(week, 6)4 -0.09281    0.02112   -4.39  1.1e-05 ***\n## s(week, 6)5 -0.05503    0.01992   -2.76  0.00574 ** \n## s(week, 6)6 -0.05245    0.01520   -3.45  0.00056 ***\n## ---\n## Signif. codes:  \n## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## \n## Topic 9:\n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  0.06851    0.00589   11.63  < 2e-16 ***\n## press여당지  0.02653    0.00375    7.07  1.6e-12 ***\n## s(week, 6)1  0.00463    0.01325    0.35   0.7269    \n## s(week, 6)2 -0.01549    0.01295   -1.20   0.2317    \n## s(week, 6)3  0.03523    0.01274    2.76   0.0057 ** \n## s(week, 6)4 -0.02312    0.01444   -1.60   0.1094    \n## s(week, 6)5  0.03956    0.01470    2.69   0.0071 ** \n## s(week, 6)6  0.01209    0.01101    1.10   0.2721    \n## ---\n## Signif. codes:  \n## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n"},{"path":"anal4topic.html","id":"문서-내용-확인","chapter":"9 .  주제모형(공변인)","heading":"9.5.1 문서 내용 확인","text":"주제1은 ’백신접종’에 관련된 내용이다. 언론사 별로 각 주제에 대해 전형적인 기사가 무엇인지 확인해보자. 원문과 언론사 정보가 포함된 데이터프레임과 감마 계수 데이터프레임을 결합해 전형적인 기사를 찾을 수 있다.’백신 접종’을 주제로 보도한 기사에서 주제1의 감마 계수 상위 10대 기사 중 9건이 야당지 보도다. 기사의 제목과 본문을 살펴보자. 4번째가 여당지 기사이고, 나머지는 모두 야당지 기사다.","code":"\ncombined_df %>% names()##  [1] \"ID\"     \"text2\"  \"일자\"   \"text\"   \"언론사\" \"cat\"   \n##  [7] \"키워드\" \"week\"   \"press\"  \"catSoc\"\n\ncombined_df %>% \n  left_join(td_gamma, by = c(\"ID\" = \"document\")) %>% \n  pivot_wider(\n    names_from = topic,\n    values_from = gamma,\n    names_prefix = \"tGamma\",\n    values_fill = 0\n    ) %>% \n\n  arrange(-tGamma1) %>% \n  filter(str_detect(text, \"백신\")) %>% \n  mutate(text = str_replace_all(text, \"백신\", \"**백신**\")) %>% \n  head(30)## # A tibble: 30 × 19\n##      ID text2    일자  text  언론사 cat   키워드  week press\n##   <int> <chr>    <chr> <chr> <chr>  <chr> <chr>  <dbl> <chr>\n## 1  1847 770만 …  2021… \"770… 조선…  국제  770만…    11 야당…\n## 2  8183 62 아스… 2021… \"아…  중앙…  사회  62%,…      5 야당…\n## 3  6092 65세 아… 2021… \"65…  조선…  사회  65세,…     7 야당…\n## 4  3822 유럽 접… 2021… \"유…  중앙…  사회  유럽,…     9 야당…\n## 5  7362 az 고령… 2021… \"AZ … 중앙…  사회  AZ,고…     6 야당…\n## 6  1852 백신 혈… 2021… \"**…  조선…  국제  백신,…    11 야당…\n## # … with 24 more rows, and 10 more variables: catSoc <chr>,\n## #   tGamma1 <dbl>, tGamma2 <dbl>, tGamma3 <dbl>,\n## #   tGamma4 <dbl>, tGamma5 <dbl>, tGamma6 <dbl>,\n## #   tGamma7 <dbl>, tGamma8 <dbl>, tGamma9 <dbl>\n\ncombined_df %>% names()##  [1] \"ID\"     \"text2\"  \"일자\"   \"text\"   \"언론사\" \"cat\"   \n##  [7] \"키워드\" \"week\"   \"press\"  \"catSoc\"\n\ncombined_df %>% \n  left_join(td_gamma, by = c(\"ID\" = \"document\")) %>% \n  pivot_wider(\n    names_from = topic,\n    values_from = gamma,\n    names_prefix = \"tGamma\",\n    values_fill = 0\n    ) %>% \n\n  arrange(-tGamma1) %>% \n  filter(str_detect(text, \"백신\")) %>% \n  mutate(text = str_replace_all(text, \"백신\", \"**백신**\")) %>% \n  pull(text) %>% .[1:10]##  [1] \"770만명 맞을 아스트라, 유럽선 중단 EU(유럽연합) 4대 회원국인 독일 프랑스 이탈리아 스페인이 15일(현지 시각) 아스트라제네카(AZ) 코로나 **백신**의 접종을 일시 중단하겠다고 선언했다. 접종 후 혈전(血栓 핏덩이)이 생기는 부작용이 발생했다는 보고가 잇따르자 추가 조사 결과가 나올 때까지 접종을 멈추겠다는 것이다. 이날 옌스 슈판 독일 보건부 장관은 “부작용이 **백신** 접종의 효과를 ..\"                                                \n##  [2] \"아스트라 **백신** 1차 검증서 \\\"예방 효과 62% 고령층 접종 가능\\\" 아스트라제네카 **백신**이 국내 접종을 앞두고 허가 심사 첫 관문을 통과했다. 식품의약품안전처 외부 자문단이 아스트라제네카 **백신**에 대해 조건부 허가할 수 있다는 결론을 내렸다. 해외에서 논란이 일고 있는 65세 이상 고령자 접종에 대해서도 자문단 다수가 “투여할 수 있다”고 의견을 냈다. 식약처 허가를 통과하면 아스트라제네카 **백신**은 이달 말께 공급돼 요양병원..\"\n##  [3] \"65세 이상, 아스트라 **백신** 접종 연기 아스트라제네카 코로나 **백신**의 최우선 접종 대상에서 ‘65세 이상’ 고령층이 제외됐다. 코로나19 예방접종 대응 추진단(단장 정은경 질병관리청장)은 당초 2월 접종 예정이었던 요양병원 요양시설의 65세 이상 고령자 접종을 2분기(4~6월) 이후로 미룬다고 15일 밝혔다. 최근 유럽에서 ‘효과 검증이 부족하다’는 이유로 65세 이상에 대한 아스트라제네..\"                                                 \n##  [4] \"유럽도 AZ 고령층 접종한다는데 국내선 결론나도 2분기에나 아스트라제네카(AZ) **백신**의 고령층 접종을 제한했던 유럽 국가 일부가 접종을 다시 허용하는 쪽으로 입장을 바꾸는 분위기다. 국내에서도 고령층 접종을 재검토해야 한다는 목소리가 나오고 있다. 2일 영국 BBC 등 외신에 따르면 프랑스 정부는 1일(현지시간) 아스트라제네카 **백신**의 접종 권고 연령을 기존 65세 미만에서 74세까지로 확대했다. 올리비에..\"                             \n##  [5] \"AZ 고령층 접종 신중하게...질병청 예방접종위로 공 넘겨 아스트라제네카사(社)의 **백신** 허가를 위한 식품의약품안전처의 중앙약사심의위원회 회의에서 ‘18세 이상에게 접종할 수 있다’는 조건부 허가 권고가 나왔다. 현재 진행 중인 3상 임상결과를 제출하는 조건이다. 다만 중앙약심은 핵심 쟁점인 ‘65세 이상 고령자’에 대해서는 애매한 판단을 내렸다. 그간 나온 임상시험 자료가 충분하지 않다는 이유에서다. 허용은 하..\"                    \n##  [6] \"**백신**이 혈전 원인? 아스트라에 대한 3가지 의문 유럽 국가들이 줄줄이 아스트라제네카(AZ) **백신** 접종을 잠정 중단하면서 국내서도 불안감이 커지고 있다. 전문가들은 대부분 “현재까지 AZ **백신**과 혈전(핏덩이) 등과의 인과성은 확인된 바 없다”며 “고령층과 기저 질환자도 코로나 감염 위험을 감안하면 접종을 하는 게 더 이득”이란 입장이다. 하지만 정부가 국민 불안에 지금처럼 소극적 수동적으로 대처하면 백..\"                      \n##  [7] \"아스트라 효과 62% 자문단 다수 “고령자 배제 이유 없다” 식품의약품안전처의 전문가 자문기구가 아스트라제네카의 코로나19 **백신**을 고령층에게 접종해도 된다는 판단을 내렸다. 해당 **백신**은 유럽에서 고령층 접종 효과를 둘러싼 논란이 불거지면서 일부 국가는 고령층을 제외하고 접종하도록 권고했다. 자문기구는 현재 진행 중인 임상시험 결과를 제출하는 것을 전제로 아스트라제네카 **백신**에 대해 ‘조건부 허가’를 권고했다. ..\"       \n##  [8] \"화이자 **백신**, 국내 두 번째 허가 \\\"16세 이상 접종 가능\\\" 화이자 **백신**이 아스트라제네카에 이어 국내에서 두 번째 코로나19 **백신**으로 정식 허가됐다. 허가 접종 연령이 만 16세 이상으로 결론 난 데 따라 원칙적으로는 미성년자인 고등학생도 **백신**을 접종할 수 있게 됐다. 식품의약품안전처는 5일 최종점검위원회를 열어 한국화이자제약의 코로나19 **백신**인 ‘코미나티주’를 허가했다고 밝혔다. 식약처는 “앞서 실시된 ..\"          \n##  [9] \"AZ **백신**, 고령층 중증 위험 84% 낮춰...이상반응 0.4%는? 26일 아스트라제네카(AZ) **백신** 접종이 시작된 가운데 접종 후 나타날 수 있는 '이상 반응'에 관심이 높다. 우리나라보다 먼저 AZ **백신**을 맞은 영국에서는 접종 인구 가운데 약 0.4%가 이상 반응을 신고한 것으로 나타났다. 질별관리청 중앙방역대책본부(방대본)는 지난 24일 열린 코로나19 예방접종 특집브리핑에서 “\\\"각 국가에서 보고되는 코..\"                                   \n## [10] \"대상자 54만명 줄고, 접종시기 밀리고 스텝 꼬인 **백신**플랜 방역 당국이 15일 발표한 ‘1분기(2~3월) **백신** 접종 계획’은 요양병원에 입원한 65세 환자 등의 접종을 2월 말에서 4월 이후로 미루는 내용이 골자다. 각국이 아스트라제네카 **백신**의 ‘고령자 예방 효과’에 대한 근거 부족을 이유로 잇따라 ‘고령자 접종 제한’ 권고를 내렸던 게 고려됐다. 또 국내 1분기 도입 물량 100만명분 중 94만명분이 이 **백신**..\"\n"},{"path":"anal4topic.html","id":"공변인-분석-시각화","chapter":"9 .  주제모형(공변인)","heading":"9.5.2 공변인 분석 시각화","text":"공변인이 주제를 어떻게 예측하는지에 대해 도표로 시각화할 수 있다. 간단한 방법은 stm패키지에서 제공하는 plot.estimate()함수를 이용하는 방식이다. ggplot2패키지를 이용하면 보다 다양한 방식으로 독립변수와 종속변수의 관계를 시각화할 수 있다.","code":""},{"path":"anal4topic.html","id":"정치성향에-따른-주제분포","chapter":"9 .  주제모형(공변인)","heading":"9.5.2.1 정치성향에 따른 주제분포","text":"ggplot2패키지를 이용하면 모다 다양한 방식으로 시각화할 수 있다. 앞서 명명한 주제의 이름이 막대도표에 표시되도록 먼저 데이터프레임을 결합한다.데이터셋이 마련됐으면 막대도표에 시각화한다.","code":"\nplot.estimateEffect(\n  prep,\n  covariate = \"press\",\n  topics = c(1, 2, 4),\n  model = meta_fit,\n  method = \"difference\",\n  cov.value1 = \"여당지\",\n  cov.value2 = \"야당지\",\n  xlab = \"문서당 주제 분포 비율(야당지 대 여당지)\",\n  main = \"언론사 정치성향에 따른 문서별 주제 분포\",\n  xlim = c(-.1, .1),\n  labeltype = \"custom\",\n  custom.labels = c(\"주제1\", \"주제2\", \"주제4\")\n)\n# 주제 이름\ntopic_name## # A tibble: 9 × 2\n##   topic name            \n##   <int> <chr>           \n## 1     1 1. 백신 접종자  \n## 2     2 2. 코로나 지원금\n## 3     3 3. 경제 영향    \n## 4     4 4. 집단 수용자  \n## 5     5 5. 국제 관계    \n## 6     6 6. 교육 온라인  \n## # … with 3 more rows\n\n# 공변인 계수\ncoef_df <- \nprep %>% tidy() %>% \n  filter(term == \"press여당지\")\ncoef_df## # A tibble: 9 × 6\n##   topic term        estimate std.error statistic  p.value\n##   <int> <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1     1 press여당지 -0.0693    0.00521    -13.3  4.30e-40\n## 2     2 press여당지  0.0332    0.00454      7.31 2.90e-13\n## 3     3 press여당지 -0.0105    0.00467     -2.25 2.47e- 2\n## 4     4 press여당지  0.00413   0.00393      1.05 2.93e- 1\n## 5     5 press여당지 -0.0103    0.00448     -2.31 2.08e- 2\n## 6     6 press여당지 -0.0111    0.00481     -2.31 2.10e- 2\n## # … with 3 more rows\n\n# 주제별 상위 10개 단어 추출\ntop_terms <- \nmeta_fit %>% tidy(matrix = \"beta\")  %>% \n  group_by(topic) %>% \n  slice_max(beta, n = 7) %>% \n  select(topic, term) %>% \n  summarise(terms = str_flatten(term, \" \"))\n\ntop_terms## # A tibble: 9 × 2\n##   topic terms                                             \n##   <int> <chr>                                             \n## 1     1 백신 접종 코로나 아스트라제네카 바이러스 예방 영국\n## 2     2 코로나 지원 지급 정부 지원금 재난 매출            \n## 3     3 달러 투자 기업 코로나 시장 미국 경제              \n## 4     4 코로나 병원 경찰 환자 a씨 의료 법무부             \n## 5     5 미국 중국 일본 한국 코로나 북한 정부              \n## 6     6 온라인 교육 코로나 서비스 지원 사업 학교          \n## # … with 3 more rows\n\n# 데이터프레임 결합\nterm_coef_name <- \ntop_terms %>% \n  left_join(topic_name, by = \"topic\") %>% \n  left_join(coef_df, by = \"topic\") \n  \nterm_coef_name %>% glimpse()## Rows: 9\n## Columns: 8\n## $ topic     <int> 1, 2, 3, 4, 5, 6, 7, 8, 9\n## $ terms     <chr> \"백신 접종 코로나 아스트라제네카 바이러…\n## $ name      <chr> \"1. 백신 접종자\", \"2. 코로나 지원금\", \"3…\n## $ term      <chr> \"press여당지\", \"press여당지\", \"press여당…\n## $ estimate  <dbl> -0.06931, 0.03317, -0.01049, 0.00413, -0…\n## $ std.error <dbl> 0.00521, 0.00454, 0.00467, 0.00393, 0.00…\n## $ statistic <dbl> -13.30, 7.31, -2.25, 1.05, -2.31, -2.31,…\n## $ p.value   <dbl> 4.30e-40, 2.90e-13, 2.47e-02, 2.93e-01, …\n\nterm_coef_name %>% \n  \n  ggplot(aes(x = estimate,\n             y = reorder(name, estimate),\n             fill = name)) +\n  geom_col(show.legend = F) +\n  geom_errorbar(aes(xmin = estimate - std.error,\n                    xmax = estimate + std.error), \n                width = .9, size = .4, color = \"grey10\",\n                show.legend = F) +\n  scale_x_continuous(expand = c(0, 0),\n                     limits = c(-.75, .15),\n                     breaks = 0) +\n  geom_text(aes(x =-.4, label = terms), show.legend = F) +\n  geom_text(aes(label = round(estimate, 3)),\n            hjust = -.2) +\n  \n  labs(x = \"문서당 주제 분포 비율(야당지 대 여당지)\",\n       y = NULL,\n       title = \"언론사 정치성향에 따른 문서별 주제 분포\") +\n  theme(plot.title = element_text(size = 20))"},{"path":"anal4topic.html","id":"시간대별-주제-변화","chapter":"9 .  주제모형(공변인)","heading":"9.5.2.2 시간대별 주제 변화","text":"ggplot2패키지로 시각화하기 위해 먼저 데이터프레임을 결합한다.","code":"\nplot.estimateEffect(\n  prep,\n  covariate = \"week\",    \n  topics = c(1, 8),\n  model = meta_fit,\n  method = \"continuous\", # 시간대 연속적으로 표시\n  xlab = \"기간 (1월 ~ 3월)\",\n  main = \"시간대별 주제 분포\"\n)\n# 주제 이름\ntopic_name## # A tibble: 9 × 2\n##   topic name            \n##   <int> <chr>           \n## 1     1 1. 백신 접종자  \n## 2     2 2. 코로나 지원금\n## 3     3 3. 경제 영향    \n## 4     4 4. 집단 수용자  \n## 5     5 5. 국제 관계    \n## 6     6 6. 교육 온라인  \n## # … with 3 more rows\n\n# 공변인 계수\ncoef_time <- \nprep %>% tidy() %>% \n  filter(str_detect(term, \"^s\"))\ncoef_time## # A tibble: 54 × 6\n##   topic term        estimate std.error statistic  p.value\n##   <int> <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1     1 s(week, 6)1 -0.00808    0.0180    -0.449 6.54e- 1\n## 2     1 s(week, 6)2  0.00992    0.0174     0.571 5.68e- 1\n## 3     1 s(week, 6)3  0.0206     0.0181     1.14  2.54e- 1\n## 4     1 s(week, 6)4  0.135      0.0194     6.96  3.54e-12\n## 5     1 s(week, 6)5  0.0830     0.0201     4.12  3.74e- 5\n## 6     1 s(week, 6)6  0.0462     0.0147     3.16  1.60e- 3\n## # … with 48 more rows\n\n# 데이터프레임 결합\nterm_coef_time <- \ncoef_time %>% \n  left_join(topic_name, by = \"topic\") \n  \nterm_coef_time %>% glimpse()## Rows: 54\n## Columns: 7\n## $ topic     <int> 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3…\n## $ term      <chr> \"s(week, 6)1\", \"s(week, 6)2\", \"s(week, 6…\n## $ estimate  <dbl> -0.00808, 0.00992, 0.02061, 0.13494, 0.0…\n## $ std.error <dbl> 0.0180, 0.0174, 0.0181, 0.0194, 0.0201, …\n## $ statistic <dbl> -0.4485, 0.5706, 1.1401, 6.9606, 4.1245,…\n## $ p.value   <dbl> 6.54e-01, 5.68e-01, 2.54e-01, 3.54e-12, …\n## $ name      <chr> \"1. 백신 접종자\", \"1. 백신 접종자\", \"1. …\n\nterm_coef_time %>% \n  mutate(term = str_extract(term, \"\\\\d$\")) %>% \n  mutate(term = as.integer(term)) %>% \n  mutate(term = term * 2 - 1) %>% \n  mutate(term = as.factor(term)) %>% \n           \n  filter(str_detect(name, \"^1|^2|^8\")) %>% \n  \n  ggplot(aes(x = term,\n             y = estimate,\n             color = name)) +\n  geom_line(aes(group = name), size = 1.2) +\n  geom_point(aes(shape = name), size = 3,) +\n  geom_errorbar(aes(ymin = estimate - std.error, \n                    ymax = estimate + std.error), \n                width = .4, size = 1,\n                position = position_dodge(.01)) +\n  labs(x = \"기간(1월 ~ 3월)\",\n       y = \"문서당 주제 분포 비율\",\n       title = \"시간대별 주제 분포\") +\n  theme(plot.title = element_text(size = 20))"},{"path":"anal4topic.html","id":"주제-사이-상관성","chapter":"9 .  주제모형(공변인)","heading":"9.5.3 주제 사이 상관성","text":"주제사이의 상관성을 표시할 수 있다.","code":"\nlibrary(reshape2)\n\nget_lower_tri <- function(x){\n  x[upper.tri(x)] <- NA\n  return(x)\n}\n\ntopicCorr(meta_fit) %>% .$cor %>% \n  get_lower_tri() %>% \n  melt(na.rm = T) %>% \n  \n  ggplot(aes(x = factor(Var1), \n             y = factor(Var2), \n             fill = value)) +\n  geom_tile(color = \"white\") + \n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\",\n                       midpoint = 0,\n                       limit = c(-1, 1), space = \"Lab\") +\n  geom_text(aes(Var1, Var2, label = round(value, 3)), color = \"black\", size = 3) +\n  theme_minimal()"},{"path":"anal4topic.html","id":"과제-4","chapter":"9 .  주제모형(공변인)","heading":"9.5.4 과제","text":"관심있는 검색어를 이용해 빅카인즈에서 기사를 검색해 수집한 기사의 주제모형을 구축한다.주요 주제어에 대하여 시각화한다.공변인(메타데이터)를 설정해, 공변인과 주제와의 관계를 탐색한다. 공변인은 시간, 언론사, 기사분류 등 선택.","code":""}]
